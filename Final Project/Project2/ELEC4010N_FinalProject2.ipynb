{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jLPJjp76lZMY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "\n",
        "Baseline code: https://github.com/emma-sjwang/Dofe"
      ],
      "metadata": {
        "id": "z7rWTqmmi34W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm Implementation\n",
        "\n",
        "1. Implement the naive baseline model for comparison. This is typically a simple model with basic feature extraction and classification steps.\n",
        "\n",
        "2. Choose at least one Domain Generalization (DG) method to implement and compare against the naive baseline. This could be FACT, Dofe, or another method of your choice.\n",
        "\n",
        "3. Report the segmentation performance in terms of dice coefficient and average surface distance."
      ],
      "metadata": {
        "id": "ikoXfMF_lOJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2\"\n",
        "!unzip \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2/Fundus-doFE.zip\" -d \"/content/\"\n",
        "\n",
        "## pip install external lib\n",
        "!pip install -U albumentations\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "IBA8GHAbJ-Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset info\n",
        "[cup]Domian1: Drishti-GS dataset [101] including training[50] and testing[51]\n",
        "\n",
        "[cup]Domain2: RIM-ONE_r3 dataset [159] including training and[99] testing[60]. \n",
        "\n",
        "[cup]Domain3: REFUGE training [400]  MICCAI 2018 workshop including training and[320] testing[80]. \n",
        "\n",
        "[cup]Domian4: REFUGE val [400]  including training and[320] testing[80]. \n",
        "Domain5: ISBI [81]  IDRID chanllenge\n",
        "\n"
      ],
      "metadata": {
        "id": "KB_yBwBhTu45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Baseline Model"
      ],
      "metadata": {
        "id": "jLPJjp76lZMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just for reference\n",
        "# UNet implementation\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv_down1 = double_conv(3, 64)      # Number of channel\n",
        "        self.conv_down2 = double_conv(64, 128)\n",
        "        self.conv_down3 = double_conv(128, 256)\n",
        "        self.conv_down4 = double_conv(256, 512)        \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        self.conv_up3 = double_conv(256 + 512, 256)\n",
        "        self.conv_up2 = double_conv(128 + 256, 128)\n",
        "        self.conv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.last_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        conv2 = self.conv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        conv3 = self.conv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "        x = self.conv_down4(x)\n",
        "        x = self.upsample(x)\n",
        "        \n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "        \n",
        "        x = self.conv_up1(x)\n",
        "        out = self.last_conv(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "2cHXXW2wj-Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model = UNet(num_classes=1).to(device)\n",
        "output = model(torch.randn(1,3,256,256).to(device))\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "1LQxOa7tkAA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FACT\n",
        "\n",
        "https://github.com/MediaBrain-SJTU/FACT"
      ],
      "metadata": {
        "id": "gEwWV9bcle8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "\n",
        "# PyTorch imports for deep learning\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50 as _resnet50\n",
        "from torchsummary import summary\n",
        "\n",
        "# Other utilities\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFilter\n",
        "from google.colab.patches import cv2_imshow\n",
        "import albumentations as Augm\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "w_qdq2stHBS4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1.1 Data Reader"
      ],
      "metadata": {
        "id": "dBgSpAIZIh-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.data_utils import *\n",
        "\n",
        "\n",
        "D1_Img_dir = [\"/content/Fundus/Domain1/test/image\", \"/content/Fundus/Domain1/train/image\"]\n",
        "D2_Img_dir = [\"/content/Fundus/Domain2/test/image\", \"/content/Fundus/Domain2/train/image\"]\n",
        "D3_Img_dir = [\"/content/Fundus/Domain3/test/image\", \"/content/Fundus/Domain3/train/image\"]\n",
        "D4_Img_dir = [\"/content/Fundus/Domain4/test/image\", \"/content/Fundus/Domain4/train/image\"]\n",
        "\n",
        "D1_Mask_dir = [\"/content/Fundus/Domain1/test/mask\", \"/content/Fundus/Domain1/train/mask\"]\n",
        "D2_Mask_dir = [\"/content/Fundus/Domain2/test/mask\", \"/content/Fundus/Domain2/train/mask\"]\n",
        "D3_Mask_dir = [\"/content/Fundus/Domain3/test/mask\", \"/content/Fundus/Domain3/train/mask\"]\n",
        "D4_Mask_dir = [\"/content/Fundus/Domain4/test/mask\", \"/content/Fundus/Domain4/train/mask\"]\n",
        "\n",
        "IS_TUMOR_VALUE = 0.5\n",
        "class Fundus_Dataset(Dataset):\n",
        "    def __init__(self, path, transform, transform_mask):\n",
        "        self.PATH = path\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "        dirsImg, images, dirsMask, masks = [], [], [], []\n",
        "        for PATH in self.PATH:\n",
        "            for img_path in PATH:\n",
        "                for root, folders, files in os.walk(img_path):\n",
        "                    for file in files:\n",
        "                        check_mask = cv2.imread(img_path+'/'+file, cv2.IMREAD_GRAYSCALE)\n",
        "                        if check_mask.sum() > 0:\n",
        "                            match (img_path[16:23]):  ## Domain idx\n",
        "                                case 'Domain1':\n",
        "                                    images.append(file)\n",
        "                                case 'Domain2':\n",
        "                                    images.append(file.replace('png','jpg'))\n",
        "                                case 'Domain3':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "                                case 'Domain4':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "                            masks.append(file)\n",
        "                            dirsMask.append(img_path+'/')\n",
        "                            dirsImg.append(img_path.replace('mask','image')+'/')\n",
        "        path_df = pd.DataFrame({'direcImg':dirsImg, 'images':images, 'direcMask':dirsMask, 'masks':masks})\n",
        "        self.path_df = path_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path_df)\n",
        "  \n",
        "    def __getitem__(self):\n",
        "        FundusImg_list = []\n",
        "        Mask_list = []\n",
        "        for idx in range(len(self.path_df)):\n",
        "              ## load MRI\n",
        "              folder = self.path_df.loc[idx]['direcImg'][:-1]+'/'+self.path_df.loc[idx]['images']\n",
        "              fundus_img = cv2.imread(folder)\n",
        "              fundus_img = self.transform(fundus_img)\n",
        "              FundusImg_list.append(fundus_img)\n",
        "\n",
        "              ## load mask\n",
        "              folder = self.path_df.loc[idx]['direcMask'][:-1]+'/'+self.path_df.loc[idx]['masks']\n",
        "              mask = cv2.imread(folder, cv2.IMREAD_GRAYSCALE)\n",
        "              mask = self.transform_mask(mask)\n",
        "              mask[mask > IS_TUMOR_VALUE] = 1\n",
        "              mask[mask < IS_TUMOR_VALUE] = 0\n",
        "              Mask_list.append(mask)\n",
        "        return FundusImg_list, Mask_list\n",
        "\n",
        "    def direc(self, batch_size=8):\n",
        "        FundusImg_filepath = []\n",
        "        Mask_filepath = []\n",
        "        for idx in range(len(self.path_df)):\n",
        "            img_path = self.path_df.loc[idx]['direcImg']+'/'+self.path_df.loc[idx]['images']\n",
        "            FundusImg_filepath.append(img_path)\n",
        "            mask_path = self.path_df.loc[idx]['direcMask']+'/'+self.path_df.loc[idx]['masks']\n",
        "            Mask_filepath.append(mask_path)\n",
        "\n",
        "        return FundusImg_filepath, Mask_filepath\n",
        "\n",
        "    def stack_dataset(self, batch_size=8):\n",
        "        Fundus, MASK = self.__getitem__()\n",
        "        inputs = (Fundus[0])\n",
        "        labels = (MASK[0])\n",
        "        inputs = torch.stack(Fundus)\n",
        "        labels = torch.stack(MASK)\n",
        "        print(inputs.shape)\n",
        "        print(labels.shape)\n",
        "        dataset = TensorDataset(inputs,labels)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "class AugmDataSet():\n",
        "    def __init__(self, direcs=[], transform_img=None, transform_mask=None):\n",
        "        self.img_direcs = direcs[0]\n",
        "        self.mask_direcs = direcs[1]\n",
        "        self.transform_mask = transform_mask\n",
        "        self.transform_img = transform_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_direcs)\n",
        "\n",
        "    def stack_dataset(self):\n",
        "        MRI = []\n",
        "        MASK = []\n",
        "        for img_path, mask_path in zip(self.img_direcs, self.mask_direcs):\n",
        "            image = cv2.imread(img_path)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            mask = np.expand_dims(mask, axis=2)\n",
        "\n",
        "            if self.transform_img is not None:\n",
        "                transform = self.transform_img(image=image, mask=mask)\n",
        "                image = transform['image']/255\n",
        "                mask = transform['mask']\n",
        "                mask = torch.permute(mask, (2,0,1))\n",
        "                mask[mask > IS_TUMOR_VALUE] = 1\n",
        "                MRI.append(image)\n",
        "                MASK.append(mask)\n",
        "                \n",
        "        IMAGE = torch.stack(MRI)\n",
        "        LABEL = torch.stack(MASK)\n",
        "        IMAGE = IMAGE.type(torch.float32)\n",
        "        LABEL = LABEL.type(torch.float32)\n",
        "        self.dataset = TensorDataset(IMAGE,LABEL)\n",
        "        return self.dataset\n",
        "\n",
        "\n",
        "def get_AugmDataset(batch_size=16, MRImg_dataset=None):\n",
        "    TrainImage_subset = []\n",
        "    n_mri_split = int(1*len(MRImg_dataset[0]))\n",
        "    for i in range(len(MRImg_dataset)):\n",
        "        TrainImage_subset.append(torch.utils.data.Subset(MRImg_dataset[i], range(n_mri_split)))\n",
        "    MRI_dataset = torch.utils.data.ConcatDataset([TrainImage_subset[0], TrainImage_subset[1],\n",
        "                                                  TrainImage_subset[2], TrainImage_subset[3]])\n",
        "    return MRI_dataset\n"
      ],
      "metadata": {
        "id": "d9xTyMmkHiT6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalization = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(64),\n",
        "    transforms.RandomCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Flipping\n",
        "GEO = Augm.Compose([\n",
        "    Augm.augmentations.geometric.resize.Resize(64, 64),\n",
        "    Augm.RandomCrop(width=64, height=64),\n",
        "    Augm.HorizontalFlip(p=1),\n",
        "    Augm.RandomBrightnessContrast(p=0.1),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Color distortion\n",
        "COL = Augm.Compose([\n",
        "    Augm.augmentations.geometric.resize.Resize(64, 64),\n",
        "    Augm.RandomCrop(width=64, height=64),\n",
        "    Augm.augmentations.transforms.ColorJitter(),\n",
        "    Augm.RandomBrightnessContrast(p=0.1),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "## PCA\n",
        "PCA = Augm.Compose([\n",
        "    Augm.augmentations.geometric.resize.Resize(64, 64),\n",
        "    Augm.RandomCrop(width=64, height=64),\n",
        "    Augm.augmentations.transforms.FancyPCA(),\n",
        "    Augm.RandomBrightnessContrast(p=0.1),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "\n",
        "## Raw Dataset\n",
        "Train_class = Fundus_Dataset([D1_Mask_dir,D2_Mask_dir,D3_Mask_dir], transform=normalization, transform_mask=normalization)\n",
        "Test_class = Fundus_Dataset([D4_Mask_dir], transform=normalization, transform_mask=normalization)\n",
        "train_dataset = Train_class.stack_dataset()\n",
        "test_dataset = Test_class.stack_dataset()\n",
        "\n",
        "## Train & Validation DataLoader\n",
        "train_size = int(0.8*len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "## Augmen Dataset\n",
        "PCA_dataset = AugmDataSet(direcs=Train_class.direc(), transform_img=PCA).stack_dataset()\n",
        "GEO_dataset = AugmDataSet(direcs=Train_class.direc(), transform_img=GEO).stack_dataset()\n",
        "COL_dataset = AugmDataSet(direcs=Train_class.direc(), transform_img=COL).stack_dataset()\n",
        "train_augm_dataset = get_AugmDataset(MRImg_dataset=[train_dataset, PCA_dataset, GEO_dataset, COL_dataset])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
        "\n",
        "## Test DataLoader\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "print(f'Shape of image: {train_loader.dataset[0][0].shape}')        #[3, 224, 224]\n",
        "print(f'Number of training batches: {len(train_loader)}')           #23\n",
        "print(f'Number of validation batches: {len(val_loader)}')           #8\n",
        "print(f'Number of test batches: {len(test_loader)}')                #32"
      ],
      "metadata": {
        "id": "4p8xLZdGxAqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################ BELOW ARE ORIGINAL CODE ############################\n",
        "\n",
        "class DGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.names[index]\n",
        "        img = Image.open(img_name).convert('RGB')\n",
        "        if self.transformer is not None:\n",
        "            img = self.transformer(img)\n",
        "        label = self.labels[index]\n",
        "        return img, label\n",
        "\n",
        "\n",
        "class FourierDGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, from_domain=None, alpha=1.0):\n",
        "        \n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.post_transform = get_post_transform()\n",
        "        self.from_domain = from_domain\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.flat_names = []\n",
        "        self.flat_labels = []\n",
        "        self.flat_domains = []\n",
        "        for i in range(len(names)):\n",
        "            self.flat_names += names[i]\n",
        "            self.flat_labels += labels[i]\n",
        "            self.flat_domains += [i] * len(names[i])\n",
        "        assert len(self.flat_names) == len(self.flat_labels)\n",
        "        assert len(self.flat_names) == len(self.flat_domains)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.flat_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.flat_names[index]\n",
        "        label = self.flat_labels[index]\n",
        "        domain = self.flat_domains[index]\n",
        "        img = Image.open(img_name).convert('RGB')\n",
        "        img_o = self.transformer(img)\n",
        "\n",
        "        img_s, label_s, domain_s = self.sample_image(domain)\n",
        "        img_s2o, img_o2s = colorful_spectrum_mix(img_o, img_s, alpha=self.alpha)\n",
        "        img_o, img_s = self.post_transform(img_o), self.post_transform(img_s)\n",
        "        img_s2o, img_o2s = self.post_transform(img_s2o), self.post_transform(img_o2s)\n",
        "        img = [img_o, img_s, img_s2o, img_o2s]\n",
        "        label = [label, label_s, label, label_s]\n",
        "        domain = [domain, domain_s, domain, domain_s]\n",
        "        return img, label, domain\n",
        "\n",
        "    def sample_image(self, domain):\n",
        "        if self.from_domain == 'all':\n",
        "            domain_idx = random.randint(0, len(self.names)-1)\n",
        "        elif self.from_domain == 'inter':\n",
        "            domains = list(range(len(self.names)))\n",
        "            domains.remove(domain)\n",
        "            domain_idx = random.sample(domains, 1)[0]\n",
        "        elif self.from_domain == 'intra':\n",
        "            domain_idx = domain\n",
        "        else:\n",
        "            raise ValueError(\"Not implemented\")\n",
        "        img_idx = random.randint(0, len(self.names[domain_idx])-1)\n",
        "        imgn_ame_sampled = self.names[domain_idx][img_idx]\n",
        "        img_sampled = Image.open(imgn_ame_sampled).convert('RGB')\n",
        "        label_sampled = self.labels[domain_idx][img_idx]\n",
        "        return self.transformer(img_sampled), label_sampled, domain_idx\n",
        "\n",
        "\n",
        "def get_dataset(path, train=False, image_size=224, crop=False, jitter=0, config=None):\n",
        "    names, labels = dataset_info(path)\n",
        "    if config:\n",
        "        image_size = config[\"image_size\"]\n",
        "        crop = config[\"use_crop\"]\n",
        "        jitter = config[\"jitter\"]\n",
        "    img_transform = get_img_transform(train, image_size, crop, jitter)\n",
        "    return DGDataset(names, labels, img_transform)\n",
        "\n",
        "\n",
        "def get_fourier_dataset(path, image_size=224, crop=False, jitter=0, from_domain='all', alpha=1.0, config=None):\n",
        "    assert isinstance(path, list)\n",
        "    names = []\n",
        "    labels = []\n",
        "    for p in path:\n",
        "        name, label = dataset_info(p)\n",
        "        names.append(name)\n",
        "        labels.append(label)\n",
        "\n",
        "    if config:\n",
        "        image_size = config[\"image_size\"]\n",
        "        crop = config[\"use_crop\"]\n",
        "        jitter = config[\"jitter\"]\n",
        "        from_domain = config[\"from_domain\"]\n",
        "        alpha = config[\"alpha\"]\n",
        "\n",
        "    img_transform = get_pre_transform(image_size, crop, jitter)\n",
        "    return FourierDGDataset(names, labels, img_transform, from_domain, alpha)\n",
        "\n",
        "\n",
        "## fix tmr & load dataset from Google drive\n",
        "import bisect\n",
        "from data.ConcatDataset import ConcatDataset\n",
        "from utils.tools import *\n",
        "\n",
        "\n",
        "default_input_dir = '/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2/data/datalists'\n",
        "\n",
        "digits_datset = [\"mnist\", \"mnist_m\", \"svhn\", \"syn\"]\n",
        "pacs_dataset = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
        "officehome_dataset = ['Art', 'Clipart', 'Product', 'Real_World']\n",
        "available_datasets = pacs_dataset + officehome_dataset + digits_datset\n",
        "\n",
        "\n",
        "def get_datalists_folder(args=None):\n",
        "    datalists_folder = default_input_dir\n",
        "    if args is not None:\n",
        "        if args.input_dir is not None:\n",
        "            datalists_folder = args.input_dir\n",
        "    return datalists_folder\n",
        "\n",
        "\n",
        "def get_train_dataloader(source_list=None, batch_size=64, image_size=224, crop=False, jitter=0, args=None, config=None):\n",
        "    if args is not None:\n",
        "        source_list = args.source\n",
        "    if config is not None:\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        data_config = config[\"data_opt\"]\n",
        "    else:\n",
        "        data_config = None\n",
        "    assert isinstance(source_list, list)\n",
        "    datasets = []\n",
        "    for dname in source_list:\n",
        "        datalists_folder = get_datalists_folder(args)\n",
        "        path = os.path.join(datalists_folder, '%s_train.txt' % dname)\n",
        "        train_dataset = get_dataset(path=path,\n",
        "                                    train=True,\n",
        "                                    image_size=image_size,\n",
        "                                    crop=crop,\n",
        "                                    jitter=jitter,\n",
        "                                    config=data_config)\n",
        "        datasets.append(train_dataset)\n",
        "    dataset = ConcatDataset(datasets)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False, drop_last=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def get_fourier_train_dataloader(\n",
        "        source_list=None,\n",
        "        batch_size=64,\n",
        "        image_size=224,\n",
        "        crop=False,\n",
        "        jitter=0,\n",
        "        args=None,\n",
        "        from_domain='all',\n",
        "        alpha=1.0,\n",
        "        config=None\n",
        "):\n",
        "    if args is not None:\n",
        "        source_list = args.source\n",
        "    if config is not None:\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        data_config = config[\"data_opt\"]\n",
        "    else:\n",
        "        data_config = None\n",
        "    assert isinstance(source_list, list)\n",
        "\n",
        "    paths = []\n",
        "    for dname in source_list:\n",
        "        datalists_folder = get_datalists_folder(args)\n",
        "        path = os.path.join(datalists_folder, '%s_train.txt' % dname)\n",
        "        paths.append(path)\n",
        "    dataset = get_fourier_dataset(path=paths,\n",
        "                                  image_size=image_size,\n",
        "                                  crop=crop,\n",
        "                                  jitter=jitter,\n",
        "                                  from_domain=from_domain,\n",
        "                                  alpha=alpha,\n",
        "                                  config=data_config)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False, drop_last=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def get_val_dataloader(source_list=None, batch_size=64, image_size=224, args=None, config=None):\n",
        "    if args is not None:\n",
        "        source_list = args.source\n",
        "    if config is not None:\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        data_config = config[\"data_opt\"]\n",
        "    else:\n",
        "        data_config = None\n",
        "    assert isinstance(source_list, list)\n",
        "    datasets = []\n",
        "    for dname in source_list:\n",
        "        datalists_folder = get_datalists_folder(args)\n",
        "        path = os.path.join(datalists_folder, '%s_val.txt' % dname)\n",
        "        val_dataset = get_dataset(path=path, train=False, image_size=image_size, config=data_config)\n",
        "        datasets.append(val_dataset)\n",
        "    dataset = ConcatDataset(datasets)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def get_test_loader(target=None, batch_size=64, image_size=224, args=None, config=None):\n",
        "    if args is not None:\n",
        "        target = args.target\n",
        "    if config is not None:\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        data_config = config[\"data_opt\"]\n",
        "    else:\n",
        "        data_config = None\n",
        "    data_folder = get_datalists_folder(args)\n",
        "    path = os.path.join(data_folder, '%s_test.txt' % target)\n",
        "    test_dataset = get_dataset(path=path, train=False, image_size=image_size, config=data_config)\n",
        "    dataset = ConcatDataset([test_dataset])\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
        "    return loader\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # print(\"yes this main\")\n",
        "    batch_size=16\n",
        "    source = [\"art_painting\", \"cartoon\", \"photo\"]\n",
        "    loader = get_fourier_train_dataloader(source, batch_size, image_size=224, from_domain='all', alpha=1.0)\n",
        "\n",
        "    it = iter(loader)\n",
        "    batch = next(it)\n",
        "    images = torch.cat(batch[0], dim=0)\n",
        "    # images = batch[0][0]\n",
        "    save_image_from_tensor_batch(images, batch_size, path='batch.jpg', device='cpu')"
      ],
      "metadata": {
        "id": "lpwjv5xjlfPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}