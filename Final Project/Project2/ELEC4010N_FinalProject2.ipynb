{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jLPJjp76lZMY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "\n",
        "Baseline code: https://github.com/emma-sjwang/Dofe"
      ],
      "metadata": {
        "id": "z7rWTqmmi34W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm Implementation\n",
        "\n",
        "1. Implement the naive baseline model for comparison. This is typically a simple model with basic feature extraction and classification steps.\n",
        "\n",
        "2. Choose at least one Domain Generalization (DG) method to implement and compare against the naive baseline. This could be FACT, Dofe, or another method of your choice.\n",
        "\n",
        "3. Report the segmentation performance in terms of dice coefficient and average surface distance."
      ],
      "metadata": {
        "id": "ikoXfMF_lOJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2\"\n",
        "!unzip \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2/Fundus-doFE.zip\" -d \"/content/\"\n",
        "\n",
        "## pip install external lib\n",
        "!pip install -U albumentations\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "IBA8GHAbJ-Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset info\n",
        "[cup]Domian1: Drishti-GS dataset [101] including training[50] and testing[51]\n",
        "\n",
        "[cup]Domain2: RIM-ONE_r3 dataset [159] including training and[99] testing[60]. \n",
        "\n",
        "[cup]Domain3: REFUGE training [400]  MICCAI 2018 workshop including training and[320] testing[80]. \n",
        "\n",
        "[cup]Domian4: REFUGE val [400]  including training and[320] testing[80]. \n",
        "Domain5: ISBI [81]  IDRID chanllenge\n",
        "\n"
      ],
      "metadata": {
        "id": "KB_yBwBhTu45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Baseline Model"
      ],
      "metadata": {
        "id": "jLPJjp76lZMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just for reference\n",
        "# UNet implementation\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv_down1 = double_conv(3, 64)      # Number of channel\n",
        "        self.conv_down2 = double_conv(64, 128)\n",
        "        self.conv_down3 = double_conv(128, 256)\n",
        "        self.conv_down4 = double_conv(256, 512)        \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        self.conv_up3 = double_conv(256 + 512, 256)\n",
        "        self.conv_up2 = double_conv(128 + 256, 128)\n",
        "        self.conv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.last_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        conv2 = self.conv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        conv3 = self.conv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "        x = self.conv_down4(x)\n",
        "        x = self.upsample(x)\n",
        "        \n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "        \n",
        "        x = self.conv_up1(x)\n",
        "        out = self.last_conv(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "2cHXXW2wj-Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model = UNet(num_classes=1).to(device)\n",
        "output = model(torch.randn(1,3,256,256).to(device))\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "1LQxOa7tkAA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FACT\n",
        "\n",
        "https://github.com/MediaBrain-SJTU/FACT"
      ],
      "metadata": {
        "id": "gEwWV9bcle8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "\n",
        "# PyTorch imports for deep learning\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50 as _resnet50\n",
        "from torchsummary import summary\n",
        "\n",
        "# Other utilities\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFilter\n",
        "from google.colab.patches import cv2_imshow\n",
        "import albumentations as Augm\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "w_qdq2stHBS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmXdlqNFhHCN",
        "outputId": "f064146d-f036-4105-e4e3-193a78944f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "D1_Mask_dir = [\"/content/Fundus/Domain1/test/mask\", \"/content/Fundus/Domain1/train/mask\"]\n",
        "D2_Mask_dir = [\"/content/Fundus/Domain2/test/mask\", \"/content/Fundus/Domain2/train/mask\"]\n",
        "D3_Mask_dir = [\"/content/Fundus/Domain3/test/mask\", \"/content/Fundus/Domain3/train/mask\"]\n",
        "D4_Mask_dir = [\"/content/Fundus/Domain4/test/mask\", \"/content/Fundus/Domain4/train/mask\"]\n",
        "\n",
        "\n",
        "class Fundus_Dataset(Dataset):\n",
        "    def __init__(self, path, transform, transform_mask, split_idx):\n",
        "        self.PATH = path\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "        self.split_idx = split_idx\n",
        "        dirsImg, images, dirsMask, masks = [], [], [], []\n",
        "        for PATH in self.PATH:\n",
        "            for img_path in PATH:\n",
        "                for root, folders, files in os.walk(img_path):\n",
        "                    for file in files:\n",
        "                        check_mask = cv2.imread(img_path+'/'+file, cv2.IMREAD_GRAYSCALE)\n",
        "                        if check_mask.sum() > 0:\n",
        "                            match (img_path[16:23]):  ## Domain idx\n",
        "                                case 'Domain1':\n",
        "                                    images.append(file)\n",
        "                                case 'Domain2':\n",
        "                                    images.append(file.replace('png','jpg'))\n",
        "                                case 'Domain3':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "                                case 'Domain4':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "                            masks.append(file)\n",
        "                            dirsMask.append(img_path+'/')\n",
        "                            dirsImg.append(img_path.replace('mask','image')+'/')\n",
        "        path_df = pd.DataFrame({'direcImg':dirsImg, 'images':images, 'direcMask':dirsMask, 'masks':masks})\n",
        "        self.path_df = path_df\n",
        "    \n",
        "    def split_traindataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1]+'/'+self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1]+'/'+self.path_df.loc[i]['masks']\n",
        "            match(self.split_idx):\n",
        "                case 1:\n",
        "                    if \"Domain4\" not in image_direc:\n",
        "                        if i < int(0.8*len(self.path_df)):\n",
        "                            shutil.copy(image_direc, train_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, train_path+\"/mask/\")\n",
        "                        else:\n",
        "                            shutil.copy(image_direc, val_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, val_path+\"/mask/\")\n",
        "                case 2:\n",
        "                    if \"Domain3\" not in image_direc:\n",
        "                        if i < int(0.8*len(self.path_df)):\n",
        "                            shutil.copy(image_direc, train_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, train_path+\"/mask/\")\n",
        "                        else:\n",
        "                            shutil.copy(image_direc, val_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, val_path+\"/mask/\")\n",
        "                case 3:\n",
        "                    if \"Domain2\" not in image_direc:\n",
        "                        if i < int(0.8*len(self.path_df)):\n",
        "                            shutil.copy(image_direc, train_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, train_path+\"/mask/\")\n",
        "                        else:\n",
        "                            shutil.copy(image_direc, val_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, val_path+\"/mask/\")\n",
        "                case 4:\n",
        "                    if \"Domain1\" not in image_direc:\n",
        "                        if i < int(0.8*len(self.path_df)):\n",
        "                            shutil.copy(image_direc, train_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, train_path+\"/mask/\")\n",
        "                        else:\n",
        "                            shutil.copy(image_direc, val_path+\"/image/\")\n",
        "                            shutil.copy(mask_direc, val_path+\"/mask/\")\n",
        "\n",
        "        return self.trainGetitem()\n",
        "\n",
        "    def split_testdataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1]+'/'+self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1]+'/'+self.path_df.loc[i]['masks']\n",
        "            shutil.copy(image_direc, test_path+\"/image/\")\n",
        "            shutil.copy(mask_direc, test_path+\"/mask/\")\n",
        "        return self.testGetitem()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path_df)\n",
        "  \n",
        "    def trainGetitem(self):\n",
        "        trainImg, trainMask = [], []\n",
        "        valImg, valMask = [], []\n",
        "        train_folders = os.listdir(train_path)\n",
        "        for folder in train_folders:\n",
        "            files = os.listdir(train_path+'/'+folder)\n",
        "            for file in files:\n",
        "                direc = train_path+'/'+folder+'/'+file\n",
        "                if 'mask' in folder:\n",
        "                    mask = cv2.imread(direc, cv2.IMREAD_GRAYSCALE)\n",
        "                    mask = self.transform(mask)\n",
        "                    mask = torch.round(mask)\n",
        "                    trainMask.append(mask)\n",
        "                else:\n",
        "                    fundus_img = cv2.imread(direc)\n",
        "                    fundus_img = self.transform(fundus_img)\n",
        "                    trainImg.append(fundus_img)\n",
        "\n",
        "        val_folders = os.listdir(val_path)\n",
        "        for folder in val_folders:\n",
        "            files = os.listdir(val_path+'/'+folder)\n",
        "            for file in files:\n",
        "                direc = val_path+'/'+folder+'/'+file\n",
        "                if 'mask' in folder:\n",
        "                    mask = cv2.imread(direc, cv2.IMREAD_GRAYSCALE)\n",
        "                    mask = self.transform(mask)\n",
        "                    mask = torch.round(mask)\n",
        "                    valMask.append(mask)\n",
        "                else:\n",
        "                    fundus_img = cv2.imread(direc)\n",
        "                    fundus_img = self.transform(fundus_img)\n",
        "                    valImg.append(fundus_img)\n",
        "        return [[trainImg,trainMask], [valImg,valMask]]\n",
        "\n",
        "    def testGetitem(self):\n",
        "        testImg, testMask = [], []\n",
        "        test_folders = os.listdir(test_path)\n",
        "        for folder in test_folders:\n",
        "            files = os.listdir(test_path+'/'+folder)\n",
        "            for file in files:\n",
        "                direc = test_path+'/'+folder+'/'+file\n",
        "                if 'mask' in folder:\n",
        "                    mask = cv2.imread(direc, cv2.IMREAD_GRAYSCALE)\n",
        "                    mask = self.transform(mask)\n",
        "                    mask = torch.round(mask)\n",
        "                    testMask.append(mask)\n",
        "                else:\n",
        "                    fundus_img = cv2.imread(direc)\n",
        "                    fundus_img = self.transform(fundus_img)\n",
        "                    testImg.append(fundus_img)\n",
        "        return [[testImg,testMask]]\n",
        "\n",
        "    def direc(self):\n",
        "        FundusImg_direc = []\n",
        "        Mask_direc = []\n",
        "        train_folders = os.listdir(train_path)\n",
        "        for folder in train_folders:\n",
        "            files = os.listdir(train_path+'/'+folder)\n",
        "            for file in files:\n",
        "                direc = train_path+'/'+folder+'/'+file\n",
        "                if 'mask' in folder:\n",
        "                    Mask_direc.append(direc)\n",
        "                else:\n",
        "                    FundusImg_direc.append(direc)\n",
        "        return FundusImg_direc, Mask_direc\n",
        "\n",
        "    def stack_train(self, batch_size=8):\n",
        "        All_dataset = []\n",
        "        DATASET = self.trainGetitem()\n",
        "        for dataset in DATASET:\n",
        "            fundus = dataset[0]\n",
        "            mask = dataset[1]\n",
        "            inputs = (fundus[0])\n",
        "            labels = (mask[0])\n",
        "            inputs = torch.stack(fundus)\n",
        "            labels = torch.stack(mask)\n",
        "            All_dataset.append(TensorDataset(inputs,labels))\n",
        "        return All_dataset[0], All_dataset[1]\n",
        "\n",
        "    def stack_test(self, batch_size=8):\n",
        "        All_dataset = []\n",
        "        DATASET = self.testGetitem()\n",
        "        for dataset in DATASET:\n",
        "            fundus = dataset[0]\n",
        "            mask = dataset[1]\n",
        "            inputs = (fundus[0])\n",
        "            labels = (mask[0])\n",
        "            inputs = torch.stack(fundus)\n",
        "            labels = torch.stack(mask)\n",
        "            All_dataset.append(TensorDataset(inputs,labels))\n",
        "        return All_dataset[0]"
      ],
      "metadata": {
        "id": "d9xTyMmkHiT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for path in [train_path, val_path, test_path]:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path)\n",
        "\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    os.makedirs(os.path.join(path, \"image\"))\n",
        "    os.makedirs(os.path.join(path, \"mask\"))\n",
        "\n",
        "normalization = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(64),\n",
        "    transforms.RandomCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "## Raw Dataset\n",
        "Train_class = Fundus_Dataset([D1_Mask_dir,D2_Mask_dir,D3_Mask_dir], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "Test_class = Fundus_Dataset([D4_Mask_dir], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "Train_class.split_traindataset()\n",
        "Test_class.split_testdataset()\n",
        "train_dataset, val_dataset = Train_class.stack_train()\n",
        "test_dataset = Test_class.stack_test()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "print(f'Shape of image: {train_loader.dataset[0][0].shape}')       \n",
        "print(f'Number of training batches: {len(train_loader)}')          \n",
        "print(f'Number of validation batches: {len(val_loader)}')          \n",
        "print(f'Number of test batches: {len(test_loader)}')               "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p8xLZdGxAqZ",
        "outputId": "7d6ca68f-19a0-4023-e80f-9a6e8e547e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image: torch.Size([3, 64, 64])\n",
            "Number of training batches: 50\n",
            "Number of validation batches: 17\n",
            "Number of test batches: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data_utils.py"
      ],
      "metadata": {
        "id": "ASCCtcZzsZbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/\""
      ],
      "metadata": {
        "id": "R-xsxxEzYJAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_info(filepath):\n",
        "    img_direcs = []\n",
        "    mask_direcs = []\n",
        "    for folder in os.listdir(filepath):\n",
        "        files = os.listdir(filepath+'/'+folder)\n",
        "        for file in files:\n",
        "            direc = filepath+'/'+folder+'/'+file\n",
        "            if 'mask' in direc: mask_direcs.append(direc)\n",
        "            else: img_direcs.append(direc)\n",
        "    return img_direcs, mask_direcs\n",
        "\n",
        "\n",
        "def get_img_transform(train=False, image_size=224, crop=False, jitter=0):\n",
        "    mean = [0.5]\n",
        "    std = [0.5]\n",
        "    if train:\n",
        "        if crop:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "        else:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.Resize((image_size, image_size))]\n",
        "        if jitter > 0:\n",
        "            img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                        contrast=jitter,\n",
        "                                                        saturation=jitter,\n",
        "                                                        hue=min(0.5, jitter)))\n",
        "        img_transform += [transforms.RandomHorizontalFlip(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize(mean, std)]\n",
        "        img_transform = transforms.Compose(img_transform)\n",
        "    else:\n",
        "        img_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "    return img_transform\n",
        "    \n",
        "\n",
        "def get_pre_transform(image_size=224, crop=False, jitter=0):\n",
        "    if crop:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "    else:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.Resize((image_size, image_size))]\n",
        "    if jitter > 0:\n",
        "        img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                    contrast=jitter,\n",
        "                                                    saturation=jitter,\n",
        "                                                    hue=min(0.5, jitter)))\n",
        "    img_transform += [transforms.RandomHorizontalFlip(), transforms.ToTensor(), lambda x: np.asarray(x)]\n",
        "    img_transform = transforms.Compose(img_transform)\n",
        "    return img_transform\n",
        "\n",
        "def get_post_transform(mean=[0.5], std=[0.5]):\n",
        "    img_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "    return img_transform\n",
        "\n",
        "def colorful_spectrum_mix(img1, img2, alpha, ratio=1.0):\n",
        "    \"\"\"Input image size: ndarray of [H, W, C]\"\"\"\n",
        "    lam = np.random.uniform(0, alpha)\n",
        "\n",
        "    assert img1.shape == img2.shape\n",
        "    h, w, c = img1.shape\n",
        "    h_crop = int(h * sqrt(ratio))\n",
        "    w_crop = int(w * sqrt(ratio))\n",
        "    h_start = h // 2 - h_crop // 2\n",
        "    w_start = w // 2 - w_crop // 2\n",
        "\n",
        "    img1_fft = np.fft.fft2(img1, axes=(0, 1))\n",
        "    img2_fft = np.fft.fft2(img2, axes=(0, 1))\n",
        "    img1_abs, img1_pha = np.abs(img1_fft), np.angle(img1_fft)\n",
        "    img2_abs, img2_pha = np.abs(img2_fft), np.angle(img2_fft)\n",
        "\n",
        "    img1_abs = np.fft.fftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.fftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img1_abs_ = np.copy(img1_abs)\n",
        "    img2_abs_ = np.copy(img2_abs)\n",
        "    img1_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img2_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img1_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "    img2_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img1_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img2_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "\n",
        "    img1_abs = np.fft.ifftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.ifftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img21 = img1_abs * (np.e ** (1j * img1_pha))\n",
        "    img12 = img2_abs * (np.e ** (1j * img2_pha))\n",
        "    img21 = np.real(np.fft.ifft2(img21, axes=(0, 1)))\n",
        "    img12 = np.real(np.fft.ifft2(img12, axes=(0, 1)))\n",
        "    img21 = np.uint8(np.clip(img21, 0, 255))\n",
        "    img12 = np.uint8(np.clip(img12, 0, 255))\n",
        "\n",
        "    return img21, img12"
      ],
      "metadata": {
        "id": "Kyu2yCvnsAnj"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1.1 Data Reader"
      ],
      "metadata": {
        "id": "dBgSpAIZIh-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self):\n",
        "        Img, Mask = [], []\n",
        "        for idx in range(len(self.names)):\n",
        "            img_name = self.names[idx]\n",
        "            mask_name = self.labels[idx]\n",
        "            img = cv2.imread(img_name)\n",
        "            mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
        "            if self.transformer is not None:\n",
        "                img = self.transformer(img)\n",
        "                mask = self.transformer(mask)\n",
        "            Img.append(img)\n",
        "            Mask.append(mask)\n",
        "        return Img, Mask\n",
        "\n",
        "    def stack_dataset(self, batch_size=8):\n",
        "        IMAGE, MASK = self.__getitem__()\n",
        "        images = IMAGE[0]\n",
        "        masks = MASK[0]\n",
        "        inputs = (images[0])\n",
        "        labels = (masks[0])\n",
        "        inputs = torch.stack(IMAGE)\n",
        "        labels = torch.stack(MASK)\n",
        "        dataset = TensorDataset(inputs,labels)\n",
        "        return dataset\n",
        "\n",
        "class FourierDGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, from_domain=None, alpha=1.0):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.post_transform = get_post_transform()\n",
        "        self.from_domain = from_domain\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.flat_names = []\n",
        "        self.flat_labels = []\n",
        "        self.flat_domains = []\n",
        "        for i in range(len(names)):\n",
        "            self.flat_domains += [i] * len(names[i])\n",
        "            self.flat_names += names[i]\n",
        "            self.flat_labels += labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.flat_names)\n",
        "\n",
        "    def __getitem__(self):\n",
        "        Img, Mask = [], []\n",
        "        for idx in range(len(self.names)):\n",
        "            img_name = self.names[idx]\n",
        "            mask_name = self.labels[idx]\n",
        "            img = cv2.imread(img_name)\n",
        "            mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
        "            img_o = self.transformer(img)\n",
        "            mask_o = self.transformer(mask)\n",
        "\n",
        "            img_s, mask_s, _ = self.sample_image()  ## random pick image\n",
        "            img_s2o, img_o2s = colorful_spectrum_mix(img_o, img_s, alpha=self.alpha)  ## mix their amplitude\n",
        "            img_o, img_s = self.post_transform(img_o), self.post_transform(img_s)\n",
        "            img_s2o, img_o2s = self.post_transform(img_s2o), self.post_transform(img_o2s)\n",
        "\n",
        "            img = [img_o, img_s, img_s2o, img_o2s]\n",
        "            mask = [mask, mask_s, mask, mask_s]\n",
        "            Img.append(img)\n",
        "            Mask.append(mask)\n",
        "            # domain = [domain, domain_s, domain, domain_s]\n",
        "        return Img, Mask\n",
        "\n",
        "    def sample_image(self, domain=None):\n",
        "        if self.from_domain == 'all':\n",
        "            domain_idx = random.randint(0, len(self.names)-1)\n",
        "        elif self.from_domain == 'inter':\n",
        "            domains = list(range(len(self.names)))\n",
        "            # domains.remove(domain)\n",
        "            domain_idx = random.sample(domains, 1)[0]\n",
        "        elif self.from_domain == 'intra':\n",
        "            domain_idx = domain\n",
        "        else:\n",
        "            raise ValueError(\"Not implemented\")\n",
        "        img_idx = random.randint(0, len(self.names[domain_idx])-1)\n",
        "        imgn_ame_sampled = self.names[img_idx]\n",
        "        img_sampled = cv2.imread(imgn_ame_sampled)\n",
        "        label_ame_sampled = self.labels[img_idx]\n",
        "        label_sampled = cv2.imread(label_ame_sampled)\n",
        "        return self.transformer(img_sampled), self.transformer(label_sampled), domain_idx\n",
        "\n",
        "    def stack_dataset(self, batch_size=8):\n",
        "        IMAGE, MASK = self.__getitem__()\n",
        "        images = IMAGE[0]\n",
        "        masks = MASK[0]\n",
        "        inputs = (images[0])\n",
        "        labels = (masks[0])\n",
        "        inputs = torch.stack(IMAGE)\n",
        "        labels = torch.stack(MASK)\n",
        "        dataset = TensorDataset(inputs,labels)\n",
        "        return dataset\n",
        "\n",
        "\n",
        "def get_dataset(path, train=False, image_size=64, crop=False, jitter=0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_img_transform(train, image_size, crop, jitter)\n",
        "    return DGDataset(names, labels, img_transform)\n",
        "\n",
        "def get_fourier_dataset(path, image_size=64, crop=False, jitter=0, from_domain='all', alpha=1.0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_pre_transform(image_size, crop, jitter)\n",
        "    return FourierDGDataset(names, labels, img_transform, from_domain, alpha)"
      ],
      "metadata": {
        "id": "lpwjv5xjlfPj"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trian_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "\n",
        "# train_dataset = get_dataset(path=train_path, train=True,crop=True,jitter=0.1).stack_dataset()\n",
        "train_fourier_dataset = get_fourier_dataset(path=train_path,crop=True,jitter=0.1).stack_dataset()\n",
        "\n",
        "\n",
        "# print(f'Shape of image: {train_loader.dataset[0][0].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "YKxmdpWZbMjd",
        "outputId": "f68d7564-9dc5-4f96-97e5-ef8f3a680672"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-226-057c2c313382>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train_dataset = get_dataset(path=train_path, train=True,crop=True,jitter=0.1).stack_dataset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_fourier_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fourier_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-225-e8314fa51eb5>\u001b[0m in \u001b[0;36mstack_dataset\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
          ]
        }
      ]
    }
  ]
}