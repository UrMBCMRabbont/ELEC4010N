{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jLPJjp76lZMY"
      ],
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "\n",
        "Baseline code: https://github.com/emma-sjwang/Dofe"
      ],
      "metadata": {
        "id": "z7rWTqmmi34W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm Implementation\n",
        "\n",
        "1. Implement the naive baseline model for comparison. This is typically a simple model with basic feature extraction and classification steps.\n",
        "\n",
        "2. Choose at least one Domain Generalization (DG) method to implement and compare against the naive baseline. This could be FACT, Dofe, or another method of your choice.\n",
        "\n",
        "3. Report the segmentation performance in terms of dice coefficient and average surface distance."
      ],
      "metadata": {
        "id": "ikoXfMF_lOJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and unzip data\n",
        "# Download from https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2\"\n",
        "!unzip \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2/Fundus-doFE.zip\" -d \"/content/\"\n",
        "\n",
        "# Install additional libraries\n",
        "!pip install -U albumentations\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install torchmetrics\n",
        "!git clone https://github.com/deepmind/surface-distance.git\n",
        "!pip install surface-distance/\n",
        "!pip install seg-metrics\n",
        "!pip install MedPy"
      ],
      "metadata": {
        "id": "IBA8GHAbJ-Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Baseline Model"
      ],
      "metadata": {
        "id": "jLPJjp76lZMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just for reference\n",
        "# UNet implementation\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv_down1 = double_conv(3, 64)      # Number of channel\n",
        "        self.conv_down2 = double_conv(64, 128)\n",
        "        self.conv_down3 = double_conv(128, 256)\n",
        "        self.conv_down4 = double_conv(256, 512)        \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        self.conv_up3 = double_conv(256 + 512, 256)\n",
        "        self.conv_up2 = double_conv(128 + 256, 128)\n",
        "        self.conv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.last_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        conv2 = self.conv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        conv3 = self.conv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "        x = self.conv_down4(x)\n",
        "        x = self.upsample(x)\n",
        "        \n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "        \n",
        "        x = self.conv_up1(x)\n",
        "        out = self.last_conv(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "2cHXXW2wj-Cj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "da2cf772-f401-4e18-c1e7-432036a0e30e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6ff8d284c3c9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         nn.ReLU(inplace=True))\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model = UNet(num_classes=1).to(device)\n",
        "output = model(torch.randn(1,3,256,256).to(device))\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "1LQxOa7tkAA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FACT\n",
        "\n",
        "https://github.com/MediaBrain-SJTU/FACT"
      ],
      "metadata": {
        "id": "gEwWV9bcle8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import math\n",
        "import copy\n",
        "import gc\n",
        "import random as randpy\n",
        "from copy import deepcopy\n",
        "from math import sqrt\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageFilter\n",
        "from pylab import *\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from ast import In\n",
        "import albumentations as Augm\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# PyTorch imports for deep learning\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50 as _resnet50\n",
        "from torchsummary import summary\n",
        "\n",
        "# Utilities for image data handling\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# For segmentation models\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Metrics\n",
        "from medpy import metric\n",
        "from segmentation_models_pytorch.losses import DiceLoss\n",
        "import surface_distance as surfdist\n",
        "import seg_metrics.seg_metrics as sg\n",
        "from torchmetrics.classification import BinaryJaccardIndex\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "w_qdq2stHBS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e64a55-f6ba-40a0-e619-e47bc28a698d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset info\n",
        "[cup]Domian1: Drishti-GS dataset [101] including training[50] and testing[51]\n",
        "\n",
        "[cup]Domain2: RIM-ONE_r3 dataset [159] including training and[99] testing[60]. \n",
        "\n",
        "[cup]Domain3: REFUGE training [400]  MICCAI 2018 workshop including training and[320] testing[80]. \n",
        "\n",
        "[cup]Domian4: REFUGE val [400]  including training and[320] testing[80]. \n",
        "Domain5: ISBI [81]  IDRID chanllenge\n",
        "\n"
      ],
      "metadata": {
        "id": "KB_yBwBhTu45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 3 classes\n",
        "label = cv2.imread(\"/content/val/mask/g0033.bmp\")\n",
        "np.unique(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlDSxZpb1uBo",
        "outputId": "0f3a9ef5-2e0e-4eaf-e7c5-c5b6d1be41ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([None], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmXdlqNFhHCN",
        "outputId": "ba988c55-3518-471c-bf4a-c29a9dfa1098"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to move the data\n",
        "train_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "\n",
        "# Define the path to load the data\n",
        "D_mask_dir = [[\"./Fundus/Domain1/train/mask\", \"./Fundus/Domain1/test/mask\"],\n",
        "              [\"./Fundus/Domain2/train/mask\", \"./Fundus/Domain2/test/mask\"],\n",
        "              [\"./Fundus/Domain3/train/mask\", \"./Fundus/Domain3/test/mask\"],\n",
        "              [\"./Fundus/Domain4/train/mask\", \"./Fundus/Domain4/test/mask\"]]\n",
        "\n",
        "# Define the fundus dataset class to load the data by functions\n",
        "class FundusDataset(Dataset):\n",
        "    # Define the constructor\n",
        "    def __init__(self, path, transform, transform_mask, split_idx):\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "        self.split_idx = split_idx\n",
        "        self.path_df = self.create_path_df()\n",
        "    \n",
        "    # Creates a DataFrame with image and mask paths\n",
        "    def create_path_df(self):\n",
        "        dirsImg, images, dirsMask, masks = [], [], [], []\n",
        "        \n",
        "        for mask_dir in self.path:\n",
        "            for img_path in mask_dir:\n",
        "                for _, _, files in os.walk(img_path):\n",
        "                    for file in files:\n",
        "                        check_mask = cv2.imread(os.path.join(img_path, file))\n",
        "                        \n",
        "                        if check_mask.sum() > 0:\n",
        "                            domain = re.search(r'Domain\\d', img_path).group() # Extract domain\n",
        "                            match (domain):\n",
        "                                case 'Domain1':\n",
        "                                    images.append(file)\n",
        "                                case 'Domain2':\n",
        "                                    images.append(file.replace('png','jpg'))\n",
        "                                case 'Domain3':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "                                case 'Domain4':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "\n",
        "                            masks.append(file)\n",
        "                            dirsMask.append(img_path+'/')\n",
        "                            dirsImg.append(img_path.replace('mask','image')+'/')\n",
        "\n",
        "        return pd.DataFrame({'direcImg':dirsImg, 'images':images, 'direcMask':dirsMask, 'masks':masks})\n",
        "    \n",
        "    # Splits the train dataset into train and validation based on the split index\n",
        "    def split_traindataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1] + '/' + self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1] + '/' + self.path_df.loc[i]['masks']\n",
        "            domain = re.search(r'Domain\\d', image_direc).group() if image_direc else None\n",
        "            \n",
        "            # Split the dataset based on the project description\n",
        "            if domain:\n",
        "                if domain != \"Domain4\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain3\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain2\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain1\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "\n",
        "    # Moves the test dataset to the test path\n",
        "    def split_testdataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1]+'/'+self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1]+'/'+self.path_df.loc[i]['masks']\n",
        "            shutil.copy(image_direc, test_path+\"/image/\")\n",
        "            shutil.copy(mask_direc, test_path+\"/mask/\")\n",
        "\n",
        "    # Returns length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.path_df)\n",
        "\n",
        "    #  Creates a list of image and mask paths\n",
        "    def direc(self):\n",
        "        FundusImg_direc = []\n",
        "        Mask_direc = []\n",
        "        train_folders = os.listdir(train_path)\n",
        "        \n",
        "        for folder in train_folders:\n",
        "            files = os.listdir(os.path.join(train_path, folder))\n",
        "            \n",
        "            for file in files:\n",
        "                direc = os.path.join(train_path, folder, file)\n",
        "                \n",
        "                if 'mask' in folder:\n",
        "                    Mask_direc.append(direc)\n",
        "                else:\n",
        "                    FundusImg_direc.append(direc)\n",
        "        \n",
        "        return FundusImg_direc, Mask_direc"
      ],
      "metadata": {
        "id": "d9xTyMmkHiT6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the train, validation, and test folders\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path)\n",
        "    os.makedirs(os.path.join(path, \"image\"))\n",
        "    os.makedirs(os.path.join(path, \"mask\"))\n",
        "\n",
        "# Normalization\n",
        "normalization = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(64),\n",
        "    transforms.RandomCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "# train = 1,2,3 test= 4\n",
        "train_class = FundusDataset(D_mask_dir[:3], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "test_class = FundusDataset(D_mask_dir[3:], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "\n",
        "# Split files of system\n",
        "train_class.split_traindataset()\n",
        "test_class.split_testdataset()"
      ],
      "metadata": {
        "id": "4p8xLZdGxAqZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data_utils.py"
      ],
      "metadata": {
        "id": "ASCCtcZzsZbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/\""
      ],
      "metadata": {
        "id": "R-xsxxEzYJAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2e9e24-d8c3-46a1-9ed7-2afb31619f7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_info(filepath):\n",
        "    img_direcs = []\n",
        "    mask_direcs = []\n",
        "    for folder in os.listdir(filepath):\n",
        "        files = os.listdir(filepath+'/'+folder)\n",
        "        for file in files:\n",
        "            direc = filepath+'/'+folder+'/'+file\n",
        "            if 'mask' in direc: mask_direcs.append(direc)\n",
        "            else: img_direcs.append(direc)\n",
        "    return img_direcs, mask_direcs\n",
        "\n",
        "\n",
        "def get_img_transform(train=False, image_size=224, crop=False, jitter=0):\n",
        "    mean = [0.5]\n",
        "    std = [0.5]\n",
        "    if train:\n",
        "        if crop:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "        else:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.Resize((image_size, image_size))]\n",
        "        if jitter > 0:\n",
        "            img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                        contrast=jitter,\n",
        "                                                        saturation=jitter,\n",
        "                                                        hue=min(0.5, jitter)))\n",
        "        img_transform += [transforms.RandomHorizontalFlip(),\n",
        "                          transforms.Normalize(mean, std)]\n",
        "        img_transform = transforms.Compose(img_transform)\n",
        "    else:\n",
        "        img_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "    return img_transform\n",
        "\n",
        "def get_label_transform(train=False, image_size=224, crop=False, jitter=0):\n",
        "    mean = [0.5]\n",
        "    std = [0.5]\n",
        "    if train:\n",
        "        if crop:\n",
        "            label_transform = [transforms.ToPILImage(),transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0], interpolation=Image.NEAREST)]\n",
        "        else:\n",
        "            label_transform = [transforms.ToPILImage(),transforms.Resize((image_size, image_size), interpolation=Image.NEAREST)]\n",
        "        if jitter > 0:\n",
        "            label_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                        contrast=jitter,\n",
        "                                                        saturation=jitter,\n",
        "                                                        hue=min(0.5, jitter)))\n",
        "        label_transform += [transforms.RandomHorizontalFlip(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize(mean, std)]\n",
        "        label_transform = transforms.Compose(label_transform)\n",
        "    else:\n",
        "        label_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((image_size, image_size), interpolation=Image.NEAREST),\n",
        "        ])\n",
        "    return label_transform\n",
        "    \n",
        "\n",
        "def get_pre_transform(image_size=224, crop=False, jitter=0):\n",
        "    if crop:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "    else:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.Resize((image_size, image_size))]\n",
        "    if jitter > 0:\n",
        "        img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                    contrast=jitter,\n",
        "                                                    saturation=jitter,\n",
        "                                                    hue=min(0.5, jitter)))\n",
        "    img_transform += [transforms.RandomHorizontalFlip(), lambda x: np.asarray(x)]\n",
        "    img_transform = transforms.Compose(img_transform)\n",
        "    return img_transform\n",
        "\n",
        "def get_post_transform(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    img_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    return img_transform\n",
        "\n",
        "def colorful_spectrum_mix(img1, img2, alpha, ratio=1.0):\n",
        "    \"\"\"Input image size: ndarray of [H, W, C]\"\"\"\n",
        "    lam = np.random.uniform(0, alpha)\n",
        "\n",
        "    assert img1.shape == img2.shape\n",
        "    h, w, c = img1.shape\n",
        "    h_crop = int(h * sqrt(ratio))\n",
        "    w_crop = int(w * sqrt(ratio))\n",
        "    h_start = h // 2 - h_crop // 2\n",
        "    w_start = w // 2 - w_crop // 2\n",
        "\n",
        "    img1_fft = np.fft.fft2(img1, axes=(0, 1))\n",
        "    img2_fft = np.fft.fft2(img2, axes=(0, 1))\n",
        "    img1_abs, img1_pha = np.abs(img1_fft), np.angle(img1_fft)\n",
        "    img2_abs, img2_pha = np.abs(img2_fft), np.angle(img2_fft)\n",
        "\n",
        "    img1_abs = np.fft.fftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.fftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img1_abs_ = np.copy(img1_abs)\n",
        "    img2_abs_ = np.copy(img2_abs)\n",
        "    img1_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img2_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img1_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "    img2_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img1_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img2_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "\n",
        "    img1_abs = np.fft.ifftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.ifftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img21 = img1_abs * (np.e ** (1j * img1_pha))\n",
        "    img12 = img2_abs * (np.e ** (1j * img2_pha))\n",
        "    img21 = np.real(np.fft.ifft2(img21, axes=(0, 1)))\n",
        "    img12 = np.real(np.fft.ifft2(img12, axes=(0, 1)))\n",
        "    img21 = np.uint8(np.clip(img21, 0, 255))\n",
        "    img12 = np.uint8(np.clip(img12, 0, 255))\n",
        "\n",
        "    return img21, img12"
      ],
      "metadata": {
        "id": "Kyu2yCvnsAnj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1.1 Data Read(Fourier) & Load "
      ],
      "metadata": {
        "id": "dBgSpAIZIh-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, lbl_transformer=None):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.lbl_transformer = lbl_transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.names[index]\n",
        "        mask_name = self.labels[index]\n",
        "        img = cv2.imread(img_name).copy()\n",
        "        mask = cv2.imread(mask_name).copy()\n",
        "        if self.transformer is not None:\n",
        "            img = self.transformer(img)\n",
        "            mask = self.lbl_transformer(mask)\n",
        "        return img, mask\n",
        "\n",
        "class FourierDGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, lbl_transformer=None, from_domain=None, alpha=1.0):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.lbl_transformer = lbl_transformer\n",
        "        self.post_transform = get_post_transform()\n",
        "        self.from_domain = from_domain\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.flat_names = []\n",
        "        self.flat_labels = []\n",
        "        self.flat_domains = []\n",
        "        for i in range(len(names)):\n",
        "            self.flat_domains += [i] * len(names[i])\n",
        "            self.flat_names += names[i]\n",
        "            self.flat_labels += labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.names[index]\n",
        "        mask_name = self.labels[index]\n",
        "        img = cv2.imread(img_name).copy()\n",
        "        mask = cv2.imread(mask_name).copy()\n",
        "        img_o = self.transformer(img)\n",
        "        mask_o = self.lbl_transformer(mask)\n",
        "\n",
        "        img_s, mask_s, _ = self.sample_image()  ## random pick image\n",
        "        img_s2o, img_o2s = colorful_spectrum_mix(img_o, img_s, alpha=self.alpha)  ## mix their amplitude\n",
        "        img_o, img_s = self.post_transform(img_o), self.post_transform(img_s)\n",
        "        img_s2o, img_o2s = self.post_transform(img_s2o), self.post_transform(img_o2s)\n",
        "        img = [img_o, img_s, img_s2o, img_o2s]  ## [original, img2, img1 x img2, img2 x img1] \n",
        "        mask = [mask_o, mask_s, mask_o, mask_s]\n",
        "        # domain = [domain, domain_s, domain, domain_s]\n",
        "        return img, mask\n",
        "\n",
        "    def sample_image(self, domain=None):\n",
        "        if self.from_domain == 'all':\n",
        "            domain_idx = randpy.randint(0, len(self.names)-1)\n",
        "        elif self.from_domain == 'inter':\n",
        "            domains = list(range(len(self.names)))\n",
        "            # domains.remove(domain)\n",
        "            domain_idx = randpy.sample(domains, 1)[0]\n",
        "        elif self.from_domain == 'intra':\n",
        "            domain_idx = domain\n",
        "        else:\n",
        "            raise ValueError(\"Not implemented\")\n",
        "        img_idx = randpy.randint(0, len(self.names[domain_idx])-1)\n",
        "        imgn_ame_sampled = self.names[img_idx]\n",
        "        img_sampled = cv2.imread(imgn_ame_sampled)\n",
        "        label_ame_sampled = self.labels[img_idx]\n",
        "        label_sampled = cv2.imread(label_ame_sampled)\n",
        "        label_sampled = self.lbl_transformer(label_sampled)\n",
        "        return self.transformer(img_sampled), label_sampled, domain_idx\n",
        "\n",
        "\n",
        "def get_dataset(path, train=False, image_size=64, crop=False, jitter=0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_img_transform(train, image_size, crop, jitter)\n",
        "    lbl_transform = get_label_transform(train, image_size=image_size, crop=crop, jitter=jitter)\n",
        "    return DGDataset(names, labels, img_transform, lbl_transform)\n",
        "\n",
        "def get_fourier_dataset(path, image_size=64, crop=False, jitter=0, from_domain='all', alpha=1.0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_pre_transform(image_size, crop, jitter)\n",
        "    lbl_transform = get_label_transform(train=True, image_size=image_size, crop=crop, jitter=jitter)\n",
        "    return FourierDGDataset(names, labels, img_transform, lbl_transform, from_domain, alpha)\n",
        "\n",
        "\n",
        "\n",
        "trian_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "\n",
        "train_fourier_dataset = get_fourier_dataset(path=train_path,crop=True,jitter=0.1)\n",
        "val_dataset = get_dataset(path=val_path, train=False)\n",
        "test_dataset = get_dataset(path=test_path, train=False)\n",
        "\n",
        "train_fourier_loader = torch.utils.data.DataLoader(train_fourier_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "print(f'Shape of image: {train_fourier_loader.dataset[0][0][0].numpy().shape}')\n",
        "print(f'Number of training batches: {len(train_fourier_loader)}')\n",
        "print(f'Number of validation batches: {len(val_loader)}')\n",
        "print(f'Number of test batches: {len(test_loader)}')"
      ],
      "metadata": {
        "id": "lpwjv5xjlfPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a05dfc0-87c0-4834-fbf5-ce7bdc60f716"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image: (3, 64, 64)\n",
            "Number of training batches: 66\n",
            "Number of validation batches: 17\n",
            "Number of test batches: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Build Unet2D Model"
      ],
      "metadata": {
        "id": "3SL8XcjmAnQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Teacher Model"
      ],
      "metadata": {
        "id": "O38A5I8dBnrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Teacher Model\n",
        "# Student model would be ResNet50 model\n",
        "class MeanTeacherModel(nn.Module):\n",
        "    # Core\n",
        "    def __init__(self, student_model, ema_decay):\n",
        "        super().__init__()\n",
        "        self.student_model = student_model\n",
        "        self.teacher_model = deepcopy(student_model)\n",
        "        self.ema_decay = ema_decay\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.student_model(x)\n",
        "\n",
        "    def update_teacher_model(self, current_epoch, momentum=0.9995):\n",
        "        # The momentum increases from 0 to ema_decay\n",
        "        # Useful for improving quickly at the beginning\n",
        "        momentum = min(1 - 1 / (current_epoch + 1), self.ema_decay)\n",
        "        with torch.no_grad():\n",
        "            for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):\n",
        "                teacher_params.data.mul_(momentum).add_((1 - momentum) * student_params.data)\n",
        "\n",
        "    # Adjust the weight of the consistency loss to rely on teacher's prediction\n",
        "    # The weight factor decreases from 1 to 0 during the first 5 epochs\n",
        "    def sigmoid_rampup(self, current_epoch):\n",
        "        current_epoch = np.clip(current_epoch, 0.0, 7.0)\n",
        "        phase = 1.0 - current_epoch / 7.0\n",
        "        return np.exp(-5.0 * phase * phase).astype(np.float32)\n",
        "\n",
        "    # The weight decreases from 2\n",
        "    def get_consistency_weight(self, epoch):\n",
        "        return 2.0 * self.sigmoid_rampup(epoch)"
      ],
      "metadata": {
        "id": "zkXkYCFGBdJj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medical Image Metrics"
      ],
      "metadata": {
        "id": "wPIglb1nI4d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice Loss\n",
        "class SoftDiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(SoftDiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, targets, smooth=1):\n",
        "        num = targets.size(0)\n",
        "        probs = logits\n",
        "        print(probs.shape)\n",
        "        print(targets.shape)\n",
        "        m1 = probs.view(num, -1)\n",
        "        m2 = targets.view(num, -1)\n",
        "        print(m1.shape)\n",
        "        print(m2.shape)\n",
        "        intersection = (m1 * m2)\n",
        " \n",
        "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
        "        return 1 - score.sum() / num\n",
        "\n",
        "    def dice_coeff(self, logits, targets):\n",
        "        return 1-(self.forward(logits, targets))\n",
        "\n",
        "# Implement ASD\n",
        "def compute_asd(outputs, labels):\n",
        "  OC, OD, BG = torch.unique(labels)[0].item(), torch.unique(labels)[1].item(), torch.unique(labels)[2].item()\n",
        "  outputs = F.softmax(outputs,dim=1)\n",
        "  outputs_cpy1, outputs_cpy2 = outputs.clone(), outputs.clone()\n",
        "  labels_cpy1, labels_cpy2 = labels.clone(), labels.clone()\n",
        "  ## [-1, 0.xx..., 1]\n",
        "  mean, std, max, min = outputs.mean(), outputs.std(), torch.max(outputs), torch.min(outputs)\n",
        "\n",
        "  ## OC asd\n",
        "  labels_cpy1[labels_cpy1 == OC] = 1.0\n",
        "  labels_cpy1[labels_cpy1 != OC] = 0.0\n",
        "  outputs_cpy1[outputs_cpy1 > (mean-std)] = 0.0\n",
        "  labels_np = labels.cpu().detach().numpy().astype(np.bool_)\n",
        "  outputs_np = outputs_cpy1.cpu().detach().numpy().astype(np.bool_)\n",
        "  surface_dist = surfdist.compute_surface_distances(labels_np.reshape((1,-1,labels_np.shape[3])),\n",
        "                                                    outputs_np.reshape((1,-1,outputs_np.shape[3])),spacing_mm=(1.0,1.0,1.0))\n",
        "  oc_avg_asd = surfdist.compute_average_surface_distance(surface_dist)[1]\n",
        "\n",
        "  ## OD asd\n",
        "  labels_cpy2[labels_cpy2 == OD] = 1.0\n",
        "  labels_cpy2[labels_cpy2 != OD] = 0.0\n",
        "  outputs_cpy2[outputs_cpy2 < (mean-std)] = 0.0\n",
        "  outputs_cpy2[outputs_cpy2 > (mean+std)] = 0.0\n",
        "  labels_np = labels.cpu().detach().numpy().astype(np.bool_)\n",
        "  outputs_np = outputs_cpy2.cpu().detach().numpy().astype(np.bool_)\n",
        "  surface_dist = surfdist.compute_surface_distances(labels_np.reshape((1,-1,labels_np.shape[3])),\n",
        "                                                    outputs_np.reshape((1,-1,outputs_np.shape[3])),spacing_mm=(1.0,1.0,1.0))\n",
        "  od_avg_asd = surfdist.compute_average_surface_distance(surface_dist)[1]\n",
        "\n",
        "  return oc_avg_asd, od_avg_asd"
      ],
      "metadata": {
        "id": "TmR0BSOXI26V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(8,3,64,64)\n",
        "print(torch.max(a), torch.min(a))\n",
        "print(a.mean()+a.std())\n",
        "print(a.mean()-a.std())\n",
        "\n",
        "a = np.array([[0.1,0.8689,1.12], [1.1,0.8689,1.12]])\n",
        "# print(a.astype(np.bool_))\n",
        "b = np.array([[0,0.89,1.12], [1.1,0.8689,1.12]])\n",
        "metric.binary.asd(a, b)"
      ],
      "metadata": {
        "id": "qYxtLwBvjCCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Training"
      ],
      "metadata": {
        "id": "j3_18EFUCceP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load model\n",
        "Unet2D_model = smp.Unet(\n",
        "    encoder_name=\"resnet50\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=3,\n",
        ")\n",
        "\n",
        "\n",
        "def train_mean_teacher(model, train_loader, val_loader, optimizer, scheduler,\n",
        "                       consistency_criterion, supervised_criterion, device, epochs):\n",
        "    # Clear GPU cache\n",
        "    if torch.cuda.is_available():\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    loss_total_list = []\n",
        "    dice_train_list = []    \n",
        "\n",
        "    model.student_model.train()\n",
        "    model.teacher_model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        loss_total_train, dice_total_train, super_loss_train, const_loss_train = 0,0,0,0\n",
        "        for it, (batch, label) in enumerate(train_loader):\n",
        "            batch = torch.cat(batch, dim=0).to(device)\n",
        "            label = torch.cat(label, dim=0).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get predictions\n",
        "            scores = model.student_model(batch)\n",
        "            with torch.no_grad():\n",
        "                scores_teacher = model.teacher_model(batch)\n",
        "\n",
        "            # Split data and predictions into original and augmented parts\n",
        "            assert batch.size(0) % 2 == 0\n",
        "            split_idx = int(batch.size(0) / 2)\n",
        "            scores_ori, scores_aug = torch.split(scores, split_idx)\n",
        "            scores_ori_tea, scores_aug_tea = torch.split(scores_teacher, split_idx)\n",
        "            scores_ori_tea, scores_aug_tea = scores_ori_tea.detach(), scores_aug_tea.detach()\n",
        "            labels_ori, labels_aug = torch.split(label, split_idx)\n",
        "            assert scores_ori.size(0) == scores_aug.size(0)\n",
        "\n",
        "            # Compute supervised losses for original and augmented data\n",
        "            # Use KL Divergence for consistency loss\n",
        "            loss_cls = supervised_criterion(scores_ori, labels_ori)\n",
        "            loss_aug = supervised_criterion(scores_aug, labels_aug)\n",
        "\n",
        "            # Calculate probability\n",
        "            p_ori, p_aug = F.softmax(scores_ori / 10.0, dim=1), F.softmax(scores_aug / 10.0, dim=1)\n",
        "            p_ori_tea, p_aug_tea = F.softmax(scores_ori_tea / 10.0, dim=1), F.softmax(scores_aug_tea / 10.0, dim=1)\n",
        "\n",
        "            # Compute consistency losses\n",
        "            loss_ori_tea = consistency_criterion(p_aug.log(), p_ori_tea, reduction='batchmean')\n",
        "            loss_aug_tea = consistency_criterion(p_ori.log(), p_aug_tea, reduction='batchmean')\n",
        "\n",
        "            const_weight = model.get_consistency_weight(epoch)\n",
        "\n",
        "            # Compute total loss\n",
        "            train_dice_coeff = (-0.5) * (loss_cls-1) + (-0.5) * (loss_aug-1)\n",
        "            total_loss = 0.5 * loss_cls + 0.5 * loss_aug + const_weight * loss_ori_tea + const_weight * loss_aug_tea\n",
        "            print(f\"loss_cls: {loss_cls:.4f},   \")\n",
        "            print(f\"loss_aug: {loss_aug:.4f},   \")\n",
        "            print(f\"loss_ori_tea: {loss_ori_tea:.4f},   \")\n",
        "            print(f\"loss_aug_tea: {loss_aug_tea:.4f},   \")\n",
        "\n",
        "            # Backward pass and update weights\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update teacher model parameters\n",
        "            model.update_teacher_model(current_epoch=epoch)\n",
        "            loss_total_train += total_loss.item()\n",
        "            dice_total_train += train_dice_coeff.item()\n",
        "        loss_total_list.append(loss_total_train)\n",
        "        dice_train_list.append(dice_total_train)\n",
        "\n",
        "        ## Validation\n",
        "        model.student_model.eval()\n",
        "        dice_val_list = []\n",
        "        asd_val_list = []\n",
        "        dice_total_val, asd_total_val = 0,0\n",
        "        for it, (batch) in enumerate(val_loader):\n",
        "            data, labels = batch[0].to(device), batch[1].to(device)\n",
        "            scores = model.student_model(data)\n",
        "\n",
        "            loss = supervised_criterion(scores, labels)\n",
        "            oc_asd, od_asd = compute_asd(scores, labels)\n",
        "            asd_ori = 0.5*oc_asd + 0.5*od_asd\n",
        "            # Compute dice\n",
        "            val_dice_coeff = (-1) * (loss-1)\n",
        "            dice_total_val += val_dice_coeff.item()\n",
        "            asd_total_val += asd_ori\n",
        "        dice_val_list.append(dice_total_val)\n",
        "        asd_val_list.append(asd_total_val)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Epoch summary\n",
        "        print(f\"Epoch [{epoch+1:02}/{epochs}],   \"\n",
        "              f\"Mean Train Loss: {loss_total_train/len(train_loader):.4f},   \"\n",
        "              f\"Mean Train Dice: {dice_total_train/len(train_loader):.4f},   \"\n",
        "              f\"Mean Val Dice: {dice_total_val/len(val_loader):.4f},   \"\n",
        "              f\"Mean Val ASD: {asd_total_val/len(val_loader):.4f},   \")\n",
        "\n",
        "        # Clear GPU cache\n",
        "        if torch.cuda.is_available():\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return model, loss_total_list, dice_train_list, dice_val_list, asd_val_list\n",
        "\n",
        "# Load Unet2D as Student model and Mean Teacher model\n",
        "base_model = Unet2D_model.to(device)\n",
        "mean_teacher_model = MeanTeacherModel(base_model, ema_decay=0.99).to(device)\n",
        "\n",
        "# Optimizer, loss functions and scheduler\n",
        "optimizer = Adam(mean_teacher_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "consistency_criterion = F.kl_div\n",
        "supervised_criterion = DiceLoss(mode='multilabel', classes=3)\n",
        "\n",
        "epochs = 15\n",
        "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=epochs)\n",
        "\n",
        "# Run training\n",
        "mean_teacher_model, loss_total_list, dice_train_list, dice_val_list, asd_val_list = train_mean_teacher(\n",
        "    mean_teacher_model, train_fourier_loader, val_loader, optimizer, scheduler,\n",
        "    consistency_criterion, supervised_criterion, device, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "0f1zLnoeCWYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5bcd8d-19d7-466d-c52c-6f460982bb67"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_cls: 0.4183,   \n",
            "loss_aug: 0.4196,   \n",
            "loss_ori_tea: 10.4485,   \n",
            "loss_aug_tea: 10.4570,   \n",
            "loss_cls: 0.4145,   \n",
            "loss_aug: 0.4178,   \n",
            "loss_ori_tea: 9.1970,   \n",
            "loss_aug_tea: 9.1979,   \n",
            "loss_cls: 0.4235,   \n",
            "loss_aug: 0.4162,   \n",
            "loss_ori_tea: 8.7201,   \n",
            "loss_aug_tea: 8.7242,   \n",
            "loss_cls: 0.4175,   \n",
            "loss_aug: 0.4142,   \n",
            "loss_ori_tea: 7.9394,   \n",
            "loss_aug_tea: 7.9422,   \n",
            "loss_cls: 0.4208,   \n",
            "loss_aug: 0.4143,   \n",
            "loss_ori_tea: 7.5690,   \n",
            "loss_aug_tea: 7.5898,   \n",
            "loss_cls: 0.4109,   \n",
            "loss_aug: 0.4077,   \n",
            "loss_ori_tea: 7.2184,   \n",
            "loss_aug_tea: 7.2348,   \n",
            "loss_cls: 0.4095,   \n",
            "loss_aug: 0.4091,   \n",
            "loss_ori_tea: 4.8180,   \n",
            "loss_aug_tea: 4.8009,   \n",
            "loss_cls: 0.4103,   \n",
            "loss_aug: 0.4079,   \n",
            "loss_ori_tea: 4.8186,   \n",
            "loss_aug_tea: 4.8183,   \n",
            "loss_cls: 0.4108,   \n",
            "loss_aug: 0.4047,   \n",
            "loss_ori_tea: 5.5828,   \n",
            "loss_aug_tea: 5.6029,   \n",
            "loss_cls: 0.4143,   \n",
            "loss_aug: 0.4044,   \n",
            "loss_ori_tea: 4.9533,   \n",
            "loss_aug_tea: 4.9682,   \n",
            "loss_cls: 0.3862,   \n",
            "loss_aug: 0.3854,   \n",
            "loss_ori_tea: 3.2349,   \n",
            "loss_aug_tea: 3.2358,   \n",
            "loss_cls: 0.3963,   \n",
            "loss_aug: 0.3859,   \n",
            "loss_ori_tea: 4.1860,   \n",
            "loss_aug_tea: 4.2175,   \n",
            "loss_cls: 0.3897,   \n",
            "loss_aug: 0.3862,   \n",
            "loss_ori_tea: 1.8893,   \n",
            "loss_aug_tea: 1.8900,   \n",
            "loss_cls: 0.3931,   \n",
            "loss_aug: 0.3935,   \n",
            "loss_ori_tea: 1.2732,   \n",
            "loss_aug_tea: 1.2731,   \n",
            "loss_cls: 0.4016,   \n",
            "loss_aug: 0.3919,   \n",
            "loss_ori_tea: 2.4901,   \n",
            "loss_aug_tea: 2.4936,   \n",
            "loss_cls: 0.3870,   \n",
            "loss_aug: 0.3758,   \n",
            "loss_ori_tea: 2.4354,   \n",
            "loss_aug_tea: 2.4473,   \n",
            "loss_cls: 0.3890,   \n",
            "loss_aug: 0.3758,   \n",
            "loss_ori_tea: 2.5233,   \n",
            "loss_aug_tea: 2.5394,   \n",
            "loss_cls: 0.4016,   \n",
            "loss_aug: 0.3952,   \n",
            "loss_ori_tea: 1.3735,   \n",
            "loss_aug_tea: 1.3758,   \n",
            "loss_cls: 0.3868,   \n",
            "loss_aug: 0.3837,   \n",
            "loss_ori_tea: 1.2990,   \n",
            "loss_aug_tea: 1.3014,   \n",
            "loss_cls: 0.3916,   \n",
            "loss_aug: 0.3843,   \n",
            "loss_ori_tea: 1.1258,   \n",
            "loss_aug_tea: 1.1267,   \n",
            "loss_cls: 0.3764,   \n",
            "loss_aug: 0.3731,   \n",
            "loss_ori_tea: 0.8623,   \n",
            "loss_aug_tea: 0.8624,   \n",
            "loss_cls: 0.3718,   \n",
            "loss_aug: 0.3681,   \n",
            "loss_ori_tea: 0.9599,   \n",
            "loss_aug_tea: 0.9607,   \n",
            "loss_cls: 0.3757,   \n",
            "loss_aug: 0.3666,   \n",
            "loss_ori_tea: 1.4702,   \n",
            "loss_aug_tea: 1.4743,   \n",
            "loss_cls: 0.3801,   \n",
            "loss_aug: 0.3609,   \n",
            "loss_ori_tea: 1.6027,   \n",
            "loss_aug_tea: 1.6056,   \n",
            "loss_cls: 0.3774,   \n",
            "loss_aug: 0.3692,   \n",
            "loss_ori_tea: 1.2363,   \n",
            "loss_aug_tea: 1.2390,   \n",
            "loss_cls: 0.3658,   \n",
            "loss_aug: 0.3620,   \n",
            "loss_ori_tea: 0.4858,   \n",
            "loss_aug_tea: 0.4859,   \n",
            "loss_cls: 0.3637,   \n",
            "loss_aug: 0.3703,   \n",
            "loss_ori_tea: 0.8197,   \n",
            "loss_aug_tea: 0.8193,   \n",
            "loss_cls: 0.3748,   \n",
            "loss_aug: 0.3742,   \n",
            "loss_ori_tea: 0.3481,   \n",
            "loss_aug_tea: 0.3480,   \n",
            "loss_cls: 0.3606,   \n",
            "loss_aug: 0.3510,   \n",
            "loss_ori_tea: 1.1341,   \n",
            "loss_aug_tea: 1.1370,   \n",
            "loss_cls: 0.3590,   \n",
            "loss_aug: 0.3592,   \n",
            "loss_ori_tea: 0.4046,   \n",
            "loss_aug_tea: 0.4031,   \n",
            "loss_cls: 0.3653,   \n",
            "loss_aug: 0.3540,   \n",
            "loss_ori_tea: 0.9826,   \n",
            "loss_aug_tea: 0.9849,   \n",
            "loss_cls: 0.3575,   \n",
            "loss_aug: 0.3498,   \n",
            "loss_ori_tea: 0.5979,   \n",
            "loss_aug_tea: 0.5988,   \n",
            "loss_cls: 0.3598,   \n",
            "loss_aug: 0.3550,   \n",
            "loss_ori_tea: 0.6068,   \n",
            "loss_aug_tea: 0.6086,   \n",
            "loss_cls: 0.3560,   \n",
            "loss_aug: 0.3488,   \n",
            "loss_ori_tea: 0.8517,   \n",
            "loss_aug_tea: 0.8564,   \n",
            "loss_cls: 0.3585,   \n",
            "loss_aug: 0.3524,   \n",
            "loss_ori_tea: 0.5445,   \n",
            "loss_aug_tea: 0.5445,   \n",
            "loss_cls: 0.3568,   \n",
            "loss_aug: 0.3507,   \n",
            "loss_ori_tea: 0.3486,   \n",
            "loss_aug_tea: 0.3484,   \n",
            "loss_cls: 0.3553,   \n",
            "loss_aug: 0.3499,   \n",
            "loss_ori_tea: 0.5235,   \n",
            "loss_aug_tea: 0.5240,   \n",
            "loss_cls: 0.3547,   \n",
            "loss_aug: 0.3506,   \n",
            "loss_ori_tea: 0.3424,   \n",
            "loss_aug_tea: 0.3426,   \n",
            "loss_cls: 0.3558,   \n",
            "loss_aug: 0.3551,   \n",
            "loss_ori_tea: 0.2312,   \n",
            "loss_aug_tea: 0.2312,   \n",
            "loss_cls: 0.3646,   \n",
            "loss_aug: 0.3623,   \n",
            "loss_ori_tea: 0.2090,   \n",
            "loss_aug_tea: 0.2090,   \n",
            "loss_cls: 0.3515,   \n",
            "loss_aug: 0.3490,   \n",
            "loss_ori_tea: 0.2600,   \n",
            "loss_aug_tea: 0.2600,   \n",
            "loss_cls: 0.3516,   \n",
            "loss_aug: 0.3503,   \n",
            "loss_ori_tea: 0.3852,   \n",
            "loss_aug_tea: 0.3860,   \n",
            "loss_cls: 0.3460,   \n",
            "loss_aug: 0.3364,   \n",
            "loss_ori_tea: 0.6556,   \n",
            "loss_aug_tea: 0.6569,   \n",
            "loss_cls: 0.3530,   \n",
            "loss_aug: 0.3410,   \n",
            "loss_ori_tea: 0.7548,   \n",
            "loss_aug_tea: 0.7552,   \n",
            "loss_cls: 0.3383,   \n",
            "loss_aug: 0.3387,   \n",
            "loss_ori_tea: 0.1579,   \n",
            "loss_aug_tea: 0.1579,   \n",
            "loss_cls: 0.3408,   \n",
            "loss_aug: 0.3330,   \n",
            "loss_ori_tea: 0.6465,   \n",
            "loss_aug_tea: 0.6492,   \n",
            "loss_cls: 0.3444,   \n",
            "loss_aug: 0.3437,   \n",
            "loss_ori_tea: 0.1665,   \n",
            "loss_aug_tea: 0.1665,   \n",
            "loss_cls: 0.3470,   \n",
            "loss_aug: 0.3382,   \n",
            "loss_ori_tea: 0.5438,   \n",
            "loss_aug_tea: 0.5444,   \n",
            "loss_cls: 0.3490,   \n",
            "loss_aug: 0.3471,   \n",
            "loss_ori_tea: 0.2325,   \n",
            "loss_aug_tea: 0.2325,   \n",
            "loss_cls: 0.3414,   \n",
            "loss_aug: 0.3337,   \n",
            "loss_ori_tea: 0.7294,   \n",
            "loss_aug_tea: 0.7300,   \n",
            "loss_cls: 0.3480,   \n",
            "loss_aug: 0.3417,   \n",
            "loss_ori_tea: 0.5162,   \n",
            "loss_aug_tea: 0.5164,   \n",
            "loss_cls: 0.3346,   \n",
            "loss_aug: 0.3337,   \n",
            "loss_ori_tea: 0.1584,   \n",
            "loss_aug_tea: 0.1583,   \n",
            "loss_cls: 0.3379,   \n",
            "loss_aug: 0.3275,   \n",
            "loss_ori_tea: 0.6398,   \n",
            "loss_aug_tea: 0.6406,   \n",
            "loss_cls: 0.3363,   \n",
            "loss_aug: 0.3386,   \n",
            "loss_ori_tea: 0.2276,   \n",
            "loss_aug_tea: 0.2276,   \n",
            "loss_cls: 0.3369,   \n",
            "loss_aug: 0.3322,   \n",
            "loss_ori_tea: 0.6745,   \n",
            "loss_aug_tea: 0.6741,   \n",
            "loss_cls: 0.3395,   \n",
            "loss_aug: 0.3287,   \n",
            "loss_ori_tea: 0.4826,   \n",
            "loss_aug_tea: 0.4824,   \n",
            "loss_cls: 0.3297,   \n",
            "loss_aug: 0.3268,   \n",
            "loss_ori_tea: 0.1743,   \n",
            "loss_aug_tea: 0.1743,   \n",
            "loss_cls: 0.3346,   \n",
            "loss_aug: 0.3359,   \n",
            "loss_ori_tea: 0.6212,   \n",
            "loss_aug_tea: 0.6210,   \n",
            "loss_cls: 0.3408,   \n",
            "loss_aug: 0.3394,   \n",
            "loss_ori_tea: 0.2007,   \n",
            "loss_aug_tea: 0.2008,   \n",
            "loss_cls: 0.3324,   \n",
            "loss_aug: 0.3249,   \n",
            "loss_ori_tea: 0.4625,   \n",
            "loss_aug_tea: 0.4627,   \n",
            "loss_cls: 0.3307,   \n",
            "loss_aug: 0.3304,   \n",
            "loss_ori_tea: 0.1562,   \n",
            "loss_aug_tea: 0.1561,   \n",
            "loss_cls: 0.3380,   \n",
            "loss_aug: 0.3326,   \n",
            "loss_ori_tea: 0.3503,   \n",
            "loss_aug_tea: 0.3506,   \n",
            "loss_cls: 0.3251,   \n",
            "loss_aug: 0.3259,   \n",
            "loss_ori_tea: 0.1551,   \n",
            "loss_aug_tea: 0.1551,   \n",
            "loss_cls: 0.3296,   \n",
            "loss_aug: 0.3325,   \n",
            "loss_ori_tea: 0.7800,   \n",
            "loss_aug_tea: 0.7787,   \n",
            "loss_cls: 0.3348,   \n",
            "loss_aug: 0.3279,   \n",
            "loss_ori_tea: 0.3824,   \n",
            "loss_aug_tea: 0.3832,   \n",
            "loss_cls: 0.3255,   \n",
            "loss_aug: 0.3210,   \n",
            "loss_ori_tea: 0.2160,   \n",
            "loss_aug_tea: 0.2161,   \n",
            "Epoch [01/15],   Mean Train Loss: 0.4124,   Mean Train Dice: 0.6360,   Mean Val Dice: 0.6820,   Mean Val ASD: 0.0020,   \n",
            "loss_cls: 0.3154,   \n",
            "loss_aug: 0.3155,   \n",
            "loss_ori_tea: 0.3336,   \n",
            "loss_aug_tea: 0.4351,   \n",
            "loss_cls: 0.3242,   \n",
            "loss_aug: 0.3205,   \n",
            "loss_ori_tea: 0.5785,   \n",
            "loss_aug_tea: 1.6291,   \n",
            "loss_cls: 0.3169,   \n",
            "loss_aug: 0.3100,   \n",
            "loss_ori_tea: 0.5030,   \n",
            "loss_aug_tea: 0.3713,   \n",
            "loss_cls: 0.2979,   \n",
            "loss_aug: 0.2941,   \n",
            "loss_ori_tea: 0.4473,   \n",
            "loss_aug_tea: 0.3509,   \n",
            "loss_cls: 0.2968,   \n",
            "loss_aug: 0.2950,   \n",
            "loss_ori_tea: 0.5098,   \n",
            "loss_aug_tea: 0.4435,   \n",
            "loss_cls: 0.2823,   \n",
            "loss_aug: 0.2807,   \n",
            "loss_ori_tea: 0.6926,   \n",
            "loss_aug_tea: 0.5788,   \n",
            "loss_cls: 0.2749,   \n",
            "loss_aug: 0.2747,   \n",
            "loss_ori_tea: 0.6119,   \n",
            "loss_aug_tea: 0.5960,   \n",
            "loss_cls: 0.2698,   \n",
            "loss_aug: 0.2694,   \n",
            "loss_ori_tea: 0.6386,   \n",
            "loss_aug_tea: 0.6805,   \n",
            "loss_cls: 0.2596,   \n",
            "loss_aug: 0.2587,   \n",
            "loss_ori_tea: 0.8027,   \n",
            "loss_aug_tea: 0.8188,   \n",
            "loss_cls: 0.2513,   \n",
            "loss_aug: 0.2510,   \n",
            "loss_ori_tea: 1.1495,   \n",
            "loss_aug_tea: 0.9439,   \n",
            "loss_cls: 0.2501,   \n",
            "loss_aug: 0.2498,   \n",
            "loss_ori_tea: 0.8642,   \n",
            "loss_aug_tea: 0.7884,   \n",
            "loss_cls: 0.2619,   \n",
            "loss_aug: 0.2625,   \n",
            "loss_ori_tea: 0.8560,   \n",
            "loss_aug_tea: 0.9934,   \n",
            "loss_cls: 0.2351,   \n",
            "loss_aug: 0.2359,   \n",
            "loss_ori_tea: 1.0907,   \n",
            "loss_aug_tea: 0.8274,   \n",
            "loss_cls: 0.2376,   \n",
            "loss_aug: 0.2376,   \n",
            "loss_ori_tea: 1.3479,   \n",
            "loss_aug_tea: 0.5739,   \n",
            "loss_cls: 0.2264,   \n",
            "loss_aug: 0.2259,   \n",
            "loss_ori_tea: 1.1373,   \n",
            "loss_aug_tea: 1.4541,   \n",
            "loss_cls: 0.2051,   \n",
            "loss_aug: 0.2032,   \n",
            "loss_ori_tea: 1.2952,   \n",
            "loss_aug_tea: 0.7657,   \n",
            "loss_cls: 0.2081,   \n",
            "loss_aug: 0.2082,   \n",
            "loss_ori_tea: 0.9575,   \n",
            "loss_aug_tea: 1.7109,   \n",
            "loss_cls: 0.2039,   \n",
            "loss_aug: 0.2033,   \n",
            "loss_ori_tea: 0.6373,   \n",
            "loss_aug_tea: 1.5343,   \n",
            "loss_cls: 0.1807,   \n",
            "loss_aug: 0.1820,   \n",
            "loss_ori_tea: 1.2358,   \n",
            "loss_aug_tea: 0.8158,   \n",
            "loss_cls: 0.1830,   \n",
            "loss_aug: 0.1831,   \n",
            "loss_ori_tea: 0.8850,   \n",
            "loss_aug_tea: 1.4266,   \n",
            "loss_cls: 0.1745,   \n",
            "loss_aug: 0.1753,   \n",
            "loss_ori_tea: 1.0450,   \n",
            "loss_aug_tea: 1.3410,   \n",
            "loss_cls: 0.1412,   \n",
            "loss_aug: 0.1417,   \n",
            "loss_ori_tea: 1.0159,   \n",
            "loss_aug_tea: 1.4187,   \n",
            "loss_cls: 0.1574,   \n",
            "loss_aug: 0.1594,   \n",
            "loss_ori_tea: 1.4229,   \n",
            "loss_aug_tea: 1.0304,   \n",
            "loss_cls: 0.1180,   \n",
            "loss_aug: 0.1191,   \n",
            "loss_ori_tea: 1.2884,   \n",
            "loss_aug_tea: 1.1729,   \n",
            "loss_cls: 0.1178,   \n",
            "loss_aug: 0.1184,   \n",
            "loss_ori_tea: 0.9626,   \n",
            "loss_aug_tea: 1.4057,   \n",
            "loss_cls: 0.1076,   \n",
            "loss_aug: 0.1081,   \n",
            "loss_ori_tea: 1.0175,   \n",
            "loss_aug_tea: 1.4274,   \n",
            "loss_cls: 0.1098,   \n",
            "loss_aug: 0.1103,   \n",
            "loss_ori_tea: 1.0061,   \n",
            "loss_aug_tea: 1.4953,   \n",
            "loss_cls: 0.1005,   \n",
            "loss_aug: 0.1007,   \n",
            "loss_ori_tea: 1.1876,   \n",
            "loss_aug_tea: 1.2332,   \n",
            "loss_cls: 0.0902,   \n",
            "loss_aug: 0.0904,   \n",
            "loss_ori_tea: 0.9351,   \n",
            "loss_aug_tea: 1.7161,   \n",
            "loss_cls: 0.0825,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 0.8847,   \n",
            "loss_aug_tea: 1.4921,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.7664,   \n",
            "loss_aug_tea: 1.5464,   \n",
            "loss_cls: 0.0836,   \n",
            "loss_aug: 0.0844,   \n",
            "loss_ori_tea: 1.4954,   \n",
            "loss_aug_tea: 1.3193,   \n",
            "loss_cls: 0.0889,   \n",
            "loss_aug: 0.0887,   \n",
            "loss_ori_tea: 1.3735,   \n",
            "loss_aug_tea: 1.1457,   \n",
            "loss_cls: 0.0842,   \n",
            "loss_aug: 0.0843,   \n",
            "loss_ori_tea: 0.9581,   \n",
            "loss_aug_tea: 1.4883,   \n",
            "loss_cls: 0.0760,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 1.1629,   \n",
            "loss_aug_tea: 1.3119,   \n",
            "loss_cls: 0.0829,   \n",
            "loss_aug: 0.0834,   \n",
            "loss_ori_tea: 1.0476,   \n",
            "loss_aug_tea: 1.3153,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0865,   \n",
            "loss_ori_tea: 1.2727,   \n",
            "loss_aug_tea: 1.4769,   \n",
            "loss_cls: 0.0841,   \n",
            "loss_aug: 0.0844,   \n",
            "loss_ori_tea: 1.0880,   \n",
            "loss_aug_tea: 1.2244,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0789,   \n",
            "loss_ori_tea: 1.3567,   \n",
            "loss_aug_tea: 1.0945,   \n",
            "loss_cls: 0.0912,   \n",
            "loss_aug: 0.0915,   \n",
            "loss_ori_tea: 1.5945,   \n",
            "loss_aug_tea: 1.1353,   \n",
            "loss_cls: 0.0757,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 0.9323,   \n",
            "loss_aug_tea: 1.5302,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0867,   \n",
            "loss_ori_tea: 1.5094,   \n",
            "loss_aug_tea: 1.0508,   \n",
            "loss_cls: 0.0826,   \n",
            "loss_aug: 0.0833,   \n",
            "loss_ori_tea: 0.8615,   \n",
            "loss_aug_tea: 1.4241,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 1.0480,   \n",
            "loss_aug_tea: 1.0590,   \n",
            "loss_cls: 0.0931,   \n",
            "loss_aug: 0.0934,   \n",
            "loss_ori_tea: 1.2100,   \n",
            "loss_aug_tea: 1.0229,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 0.9074,   \n",
            "loss_aug_tea: 1.4475,   \n",
            "loss_cls: 0.0678,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 0.9532,   \n",
            "loss_aug_tea: 1.0906,   \n",
            "loss_cls: 0.0650,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 1.3258,   \n",
            "loss_aug_tea: 1.4977,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 1.0672,   \n",
            "loss_aug_tea: 1.0497,   \n",
            "loss_cls: 0.0884,   \n",
            "loss_aug: 0.0889,   \n",
            "loss_ori_tea: 1.1167,   \n",
            "loss_aug_tea: 1.1240,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 0.8845,   \n",
            "loss_aug_tea: 1.2715,   \n",
            "loss_cls: 0.0764,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 0.9160,   \n",
            "loss_aug_tea: 1.5941,   \n",
            "loss_cls: 0.0651,   \n",
            "loss_aug: 0.0654,   \n",
            "loss_ori_tea: 1.0824,   \n",
            "loss_aug_tea: 0.9358,   \n",
            "loss_cls: 0.0928,   \n",
            "loss_aug: 0.0931,   \n",
            "loss_ori_tea: 1.0207,   \n",
            "loss_aug_tea: 1.0921,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0716,   \n",
            "loss_ori_tea: 0.6029,   \n",
            "loss_aug_tea: 1.5006,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.8104,   \n",
            "loss_aug_tea: 1.3160,   \n",
            "loss_cls: 0.0833,   \n",
            "loss_aug: 0.0835,   \n",
            "loss_ori_tea: 0.7047,   \n",
            "loss_aug_tea: 1.5089,   \n",
            "loss_cls: 0.0863,   \n",
            "loss_aug: 0.0867,   \n",
            "loss_ori_tea: 1.0652,   \n",
            "loss_aug_tea: 1.5237,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 1.1428,   \n",
            "loss_aug_tea: 1.2815,   \n",
            "loss_cls: 0.0754,   \n",
            "loss_aug: 0.0756,   \n",
            "loss_ori_tea: 0.7805,   \n",
            "loss_aug_tea: 1.3176,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.9269,   \n",
            "loss_aug_tea: 1.4453,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0665,   \n",
            "loss_ori_tea: 0.7678,   \n",
            "loss_aug_tea: 1.3093,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0590,   \n",
            "loss_ori_tea: 0.7762,   \n",
            "loss_aug_tea: 1.3806,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0656,   \n",
            "loss_ori_tea: 0.9280,   \n",
            "loss_aug_tea: 1.3314,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 1.0051,   \n",
            "loss_aug_tea: 1.1504,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 1.2908,   \n",
            "loss_aug_tea: 0.9689,   \n",
            "Epoch [02/15],   Mean Train Loss: 0.2469,   Mean Train Dice: 0.8630,   Mean Val Dice: 0.9663,   Mean Val ASD: 0.0015,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 1.0363,   \n",
            "loss_aug_tea: 0.8550,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.8779,   \n",
            "loss_aug_tea: 1.4929,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.9397,   \n",
            "loss_aug_tea: 0.9920,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0721,   \n",
            "loss_ori_tea: 0.8796,   \n",
            "loss_aug_tea: 1.0231,   \n",
            "loss_cls: 0.0718,   \n",
            "loss_aug: 0.0719,   \n",
            "loss_ori_tea: 0.8804,   \n",
            "loss_aug_tea: 0.7900,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.6530,   \n",
            "loss_aug_tea: 1.4126,   \n",
            "loss_cls: 0.0792,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 0.7212,   \n",
            "loss_aug_tea: 1.4122,   \n",
            "loss_cls: 0.0569,   \n",
            "loss_aug: 0.0574,   \n",
            "loss_ori_tea: 1.4515,   \n",
            "loss_aug_tea: 0.8054,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.8037,   \n",
            "loss_aug_tea: 1.2379,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.6372,   \n",
            "loss_aug_tea: 1.2041,   \n",
            "loss_cls: 0.0911,   \n",
            "loss_aug: 0.0914,   \n",
            "loss_ori_tea: 0.9286,   \n",
            "loss_aug_tea: 0.9359,   \n",
            "loss_cls: 0.0643,   \n",
            "loss_aug: 0.0644,   \n",
            "loss_ori_tea: 0.7500,   \n",
            "loss_aug_tea: 0.9159,   \n",
            "loss_cls: 0.0672,   \n",
            "loss_aug: 0.0675,   \n",
            "loss_ori_tea: 1.1533,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0575,   \n",
            "loss_aug: 0.0579,   \n",
            "loss_ori_tea: 0.9524,   \n",
            "loss_aug_tea: 0.9526,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 1.3190,   \n",
            "loss_aug_tea: 1.3574,   \n",
            "loss_cls: 0.0512,   \n",
            "loss_aug: 0.0514,   \n",
            "loss_ori_tea: 0.9925,   \n",
            "loss_aug_tea: 0.9823,   \n",
            "loss_cls: 0.0845,   \n",
            "loss_aug: 0.0846,   \n",
            "loss_ori_tea: 0.9930,   \n",
            "loss_aug_tea: 0.9232,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 1.4387,   \n",
            "loss_aug_tea: 0.6314,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.6395,   \n",
            "loss_aug_tea: 1.3081,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 0.9254,   \n",
            "loss_aug_tea: 0.9744,   \n",
            "loss_cls: 0.0690,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 0.7495,   \n",
            "loss_aug_tea: 1.2152,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0769,   \n",
            "loss_ori_tea: 1.1188,   \n",
            "loss_aug_tea: 0.7306,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0642,   \n",
            "loss_ori_tea: 0.5678,   \n",
            "loss_aug_tea: 1.4054,   \n",
            "loss_cls: 0.0548,   \n",
            "loss_aug: 0.0548,   \n",
            "loss_ori_tea: 0.9616,   \n",
            "loss_aug_tea: 1.3817,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0745,   \n",
            "loss_ori_tea: 0.6722,   \n",
            "loss_aug_tea: 1.1905,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0672,   \n",
            "loss_ori_tea: 0.6860,   \n",
            "loss_aug_tea: 1.3395,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.7523,   \n",
            "loss_aug_tea: 1.0900,   \n",
            "loss_cls: 0.0919,   \n",
            "loss_aug: 0.0921,   \n",
            "loss_ori_tea: 1.3465,   \n",
            "loss_aug_tea: 0.6104,   \n",
            "loss_cls: 0.0710,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.7497,   \n",
            "loss_aug_tea: 1.1568,   \n",
            "loss_cls: 0.0619,   \n",
            "loss_aug: 0.0621,   \n",
            "loss_ori_tea: 0.9160,   \n",
            "loss_aug_tea: 0.8107,   \n",
            "loss_cls: 0.0622,   \n",
            "loss_aug: 0.0625,   \n",
            "loss_ori_tea: 0.6995,   \n",
            "loss_aug_tea: 1.1035,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.7016,   \n",
            "loss_aug_tea: 1.0117,   \n",
            "loss_cls: 0.0696,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.5274,   \n",
            "loss_aug_tea: 1.1883,   \n",
            "loss_cls: 0.0625,   \n",
            "loss_aug: 0.0627,   \n",
            "loss_ori_tea: 0.7251,   \n",
            "loss_aug_tea: 0.9206,   \n",
            "loss_cls: 0.0695,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 1.0396,   \n",
            "loss_aug_tea: 0.6920,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.5747,   \n",
            "loss_aug_tea: 1.0039,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0624,   \n",
            "loss_ori_tea: 0.6709,   \n",
            "loss_aug_tea: 0.9201,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.7236,   \n",
            "loss_aug_tea: 0.6933,   \n",
            "loss_cls: 0.0524,   \n",
            "loss_aug: 0.0527,   \n",
            "loss_ori_tea: 0.7185,   \n",
            "loss_aug_tea: 0.9533,   \n",
            "loss_cls: 0.0613,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.6354,   \n",
            "loss_aug_tea: 0.9123,   \n",
            "loss_cls: 0.0611,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.7075,   \n",
            "loss_aug_tea: 0.8535,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0700,   \n",
            "loss_ori_tea: 0.6666,   \n",
            "loss_aug_tea: 1.3656,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 0.6925,   \n",
            "loss_aug_tea: 1.3410,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 0.7637,   \n",
            "loss_aug_tea: 0.9660,   \n",
            "loss_cls: 0.0626,   \n",
            "loss_aug: 0.0628,   \n",
            "loss_ori_tea: 1.0227,   \n",
            "loss_aug_tea: 0.9081,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 1.0857,   \n",
            "loss_aug_tea: 0.8742,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 0.8906,   \n",
            "loss_aug_tea: 1.1089,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0789,   \n",
            "loss_ori_tea: 0.6165,   \n",
            "loss_aug_tea: 1.0768,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0631,   \n",
            "loss_ori_tea: 0.7792,   \n",
            "loss_aug_tea: 0.9990,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 1.0541,   \n",
            "loss_aug_tea: 0.7035,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0699,   \n",
            "loss_ori_tea: 0.5637,   \n",
            "loss_aug_tea: 1.0609,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 1.1830,   \n",
            "loss_aug_tea: 0.5651,   \n",
            "loss_cls: 0.0725,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.8734,   \n",
            "loss_aug_tea: 0.7418,   \n",
            "loss_cls: 0.0614,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.8850,   \n",
            "loss_aug_tea: 0.7593,   \n",
            "loss_cls: 0.0658,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 0.9939,   \n",
            "loss_aug_tea: 0.9156,   \n",
            "loss_cls: 0.0831,   \n",
            "loss_aug: 0.0833,   \n",
            "loss_ori_tea: 0.5148,   \n",
            "loss_aug_tea: 1.1709,   \n",
            "loss_cls: 0.0571,   \n",
            "loss_aug: 0.0571,   \n",
            "loss_ori_tea: 0.6143,   \n",
            "loss_aug_tea: 1.0257,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 0.5846,   \n",
            "loss_aug_tea: 1.0459,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.6336,   \n",
            "loss_aug_tea: 0.9024,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 0.6579,   \n",
            "loss_aug_tea: 1.1187,   \n",
            "loss_cls: 0.0484,   \n",
            "loss_aug: 0.0486,   \n",
            "loss_ori_tea: 0.8126,   \n",
            "loss_aug_tea: 0.8443,   \n",
            "loss_cls: 0.0577,   \n",
            "loss_aug: 0.0579,   \n",
            "loss_ori_tea: 0.6295,   \n",
            "loss_aug_tea: 1.1320,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 0.6883,   \n",
            "loss_aug_tea: 0.9265,   \n",
            "loss_cls: 0.0495,   \n",
            "loss_aug: 0.0495,   \n",
            "loss_ori_tea: 0.5383,   \n",
            "loss_aug_tea: 1.0048,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.6943,   \n",
            "loss_aug_tea: 0.9376,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0695,   \n",
            "loss_ori_tea: 1.1216,   \n",
            "loss_aug_tea: 0.8396,   \n",
            "Epoch [03/15],   Mean Train Loss: 0.3548,   Mean Train Dice: 0.9324,   Mean Val Dice: 0.9716,   Mean Val ASD: 0.0016,   \n",
            "loss_cls: 0.0502,   \n",
            "loss_aug: 0.0503,   \n",
            "loss_ori_tea: 0.8787,   \n",
            "loss_aug_tea: 0.8185,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0663,   \n",
            "loss_ori_tea: 0.6381,   \n",
            "loss_aug_tea: 0.7982,   \n",
            "loss_cls: 0.0585,   \n",
            "loss_aug: 0.0586,   \n",
            "loss_ori_tea: 0.6437,   \n",
            "loss_aug_tea: 0.9704,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0701,   \n",
            "loss_ori_tea: 0.7622,   \n",
            "loss_aug_tea: 0.7387,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.9509,   \n",
            "loss_aug_tea: 0.8795,   \n",
            "loss_cls: 0.0672,   \n",
            "loss_aug: 0.0672,   \n",
            "loss_ori_tea: 0.9830,   \n",
            "loss_aug_tea: 0.7966,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 0.6020,   \n",
            "loss_aug_tea: 1.3223,   \n",
            "loss_cls: 0.0660,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 0.5577,   \n",
            "loss_aug_tea: 1.0039,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 0.7376,   \n",
            "loss_aug_tea: 0.7073,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 0.7248,   \n",
            "loss_aug_tea: 1.1938,   \n",
            "loss_cls: 0.0776,   \n",
            "loss_aug: 0.0778,   \n",
            "loss_ori_tea: 0.5729,   \n",
            "loss_aug_tea: 0.8250,   \n",
            "loss_cls: 0.0559,   \n",
            "loss_aug: 0.0561,   \n",
            "loss_ori_tea: 0.5118,   \n",
            "loss_aug_tea: 1.2135,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.6328,   \n",
            "loss_aug_tea: 0.9933,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0663,   \n",
            "loss_ori_tea: 0.7798,   \n",
            "loss_aug_tea: 0.7919,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 0.7148,   \n",
            "loss_aug_tea: 0.9482,   \n",
            "loss_cls: 0.0608,   \n",
            "loss_aug: 0.0613,   \n",
            "loss_ori_tea: 0.8742,   \n",
            "loss_aug_tea: 0.9292,   \n",
            "loss_cls: 0.0547,   \n",
            "loss_aug: 0.0548,   \n",
            "loss_ori_tea: 0.5860,   \n",
            "loss_aug_tea: 0.9095,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 0.7212,   \n",
            "loss_aug_tea: 0.8543,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.7587,   \n",
            "loss_aug_tea: 0.8037,   \n",
            "loss_cls: 0.0603,   \n",
            "loss_aug: 0.0604,   \n",
            "loss_ori_tea: 0.7158,   \n",
            "loss_aug_tea: 0.8536,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0746,   \n",
            "loss_ori_tea: 1.1967,   \n",
            "loss_aug_tea: 0.5657,   \n",
            "loss_cls: 0.0542,   \n",
            "loss_aug: 0.0543,   \n",
            "loss_ori_tea: 0.5537,   \n",
            "loss_aug_tea: 0.9941,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.5709,   \n",
            "loss_aug_tea: 0.8114,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.7892,   \n",
            "loss_aug_tea: 0.7136,   \n",
            "loss_cls: 0.0763,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 0.6206,   \n",
            "loss_aug_tea: 0.9796,   \n",
            "loss_cls: 0.0621,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 1.0091,   \n",
            "loss_aug_tea: 0.6851,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 1.0716,   \n",
            "loss_aug_tea: 0.9452,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0707,   \n",
            "loss_ori_tea: 0.6097,   \n",
            "loss_aug_tea: 1.0994,   \n",
            "loss_cls: 0.0809,   \n",
            "loss_aug: 0.0811,   \n",
            "loss_ori_tea: 0.7506,   \n",
            "loss_aug_tea: 0.9208,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8771,   \n",
            "loss_aug_tea: 0.8259,   \n",
            "loss_cls: 0.0611,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.7512,   \n",
            "loss_aug_tea: 0.7474,   \n",
            "loss_cls: 0.0621,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 1.1959,   \n",
            "loss_aug_tea: 1.0106,   \n",
            "loss_cls: 0.0603,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.8139,   \n",
            "loss_aug_tea: 0.9220,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 0.8783,   \n",
            "loss_aug_tea: 0.9089,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0648,   \n",
            "loss_ori_tea: 0.7657,   \n",
            "loss_aug_tea: 0.8279,   \n",
            "loss_cls: 0.0539,   \n",
            "loss_aug: 0.0542,   \n",
            "loss_ori_tea: 1.1411,   \n",
            "loss_aug_tea: 0.7186,   \n",
            "loss_cls: 0.0947,   \n",
            "loss_aug: 0.0948,   \n",
            "loss_ori_tea: 0.7083,   \n",
            "loss_aug_tea: 0.9193,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.6186,   \n",
            "loss_aug_tea: 1.1366,   \n",
            "loss_cls: 0.0764,   \n",
            "loss_aug: 0.0766,   \n",
            "loss_ori_tea: 0.8146,   \n",
            "loss_aug_tea: 0.8369,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 1.3659,   \n",
            "loss_aug_tea: 0.6705,   \n",
            "loss_cls: 0.0509,   \n",
            "loss_aug: 0.0510,   \n",
            "loss_ori_tea: 0.6669,   \n",
            "loss_aug_tea: 0.9309,   \n",
            "loss_cls: 0.0723,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.7286,   \n",
            "loss_aug_tea: 0.9249,   \n",
            "loss_cls: 0.0885,   \n",
            "loss_aug: 0.0886,   \n",
            "loss_ori_tea: 0.6564,   \n",
            "loss_aug_tea: 0.8031,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.6698,   \n",
            "loss_aug_tea: 0.9549,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 0.8204,   \n",
            "loss_aug_tea: 0.7585,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 0.6467,   \n",
            "loss_aug_tea: 0.9320,   \n",
            "loss_cls: 0.0777,   \n",
            "loss_aug: 0.0776,   \n",
            "loss_ori_tea: 0.8049,   \n",
            "loss_aug_tea: 0.7618,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.8447,   \n",
            "loss_aug_tea: 0.5603,   \n",
            "loss_cls: 0.0577,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.7043,   \n",
            "loss_aug_tea: 0.6439,   \n",
            "loss_cls: 0.0483,   \n",
            "loss_aug: 0.0485,   \n",
            "loss_ori_tea: 0.7416,   \n",
            "loss_aug_tea: 1.0170,   \n",
            "loss_cls: 0.0628,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 0.9734,   \n",
            "loss_aug_tea: 0.5742,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.7876,   \n",
            "loss_aug_tea: 0.5243,   \n",
            "loss_cls: 0.0859,   \n",
            "loss_aug: 0.0860,   \n",
            "loss_ori_tea: 0.6434,   \n",
            "loss_aug_tea: 0.7486,   \n",
            "loss_cls: 0.0644,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.5715,   \n",
            "loss_aug_tea: 0.7969,   \n",
            "loss_cls: 0.0618,   \n",
            "loss_aug: 0.0620,   \n",
            "loss_ori_tea: 0.6480,   \n",
            "loss_aug_tea: 0.6593,   \n",
            "loss_cls: 0.0580,   \n",
            "loss_aug: 0.0582,   \n",
            "loss_ori_tea: 0.6791,   \n",
            "loss_aug_tea: 0.8093,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0601,   \n",
            "loss_ori_tea: 0.8277,   \n",
            "loss_aug_tea: 0.8163,   \n",
            "loss_cls: 0.0514,   \n",
            "loss_aug: 0.0518,   \n",
            "loss_ori_tea: 0.6600,   \n",
            "loss_aug_tea: 0.6432,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0634,   \n",
            "loss_ori_tea: 1.0761,   \n",
            "loss_aug_tea: 0.9813,   \n",
            "loss_cls: 0.0669,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.6129,   \n",
            "loss_aug_tea: 0.8109,   \n",
            "loss_cls: 0.0575,   \n",
            "loss_aug: 0.0576,   \n",
            "loss_ori_tea: 0.6053,   \n",
            "loss_aug_tea: 0.9451,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.8960,   \n",
            "loss_aug_tea: 0.9360,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0649,   \n",
            "loss_ori_tea: 0.5169,   \n",
            "loss_aug_tea: 1.3418,   \n",
            "loss_cls: 0.0729,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 1.3054,   \n",
            "loss_aug_tea: 0.6501,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.7816,   \n",
            "loss_aug_tea: 0.8139,   \n",
            "loss_cls: 0.0708,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 0.8240,   \n",
            "loss_aug_tea: 0.8346,   \n",
            "Epoch [04/15],   Mean Train Loss: 0.7051,   Mean Train Dice: 0.9338,   Mean Val Dice: 0.9625,   Mean Val ASD: 0.0019,   \n",
            "loss_cls: 0.0720,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.6706,   \n",
            "loss_aug_tea: 0.8214,   \n",
            "loss_cls: 0.1012,   \n",
            "loss_aug: 0.1017,   \n",
            "loss_ori_tea: 0.7103,   \n",
            "loss_aug_tea: 0.7408,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0641,   \n",
            "loss_ori_tea: 0.9621,   \n",
            "loss_aug_tea: 0.5873,   \n",
            "loss_cls: 0.0565,   \n",
            "loss_aug: 0.0566,   \n",
            "loss_ori_tea: 0.6367,   \n",
            "loss_aug_tea: 0.8243,   \n",
            "loss_cls: 0.0483,   \n",
            "loss_aug: 0.0485,   \n",
            "loss_ori_tea: 0.8004,   \n",
            "loss_aug_tea: 0.6505,   \n",
            "loss_cls: 0.0674,   \n",
            "loss_aug: 0.0677,   \n",
            "loss_ori_tea: 0.7081,   \n",
            "loss_aug_tea: 0.8622,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0712,   \n",
            "loss_ori_tea: 0.6337,   \n",
            "loss_aug_tea: 0.8394,   \n",
            "loss_cls: 0.0784,   \n",
            "loss_aug: 0.0787,   \n",
            "loss_ori_tea: 1.0767,   \n",
            "loss_aug_tea: 0.9729,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.5080,   \n",
            "loss_aug_tea: 0.9480,   \n",
            "loss_cls: 0.0725,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.5661,   \n",
            "loss_aug_tea: 0.8388,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0794,   \n",
            "loss_ori_tea: 0.5993,   \n",
            "loss_aug_tea: 0.7065,   \n",
            "loss_cls: 0.0771,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.9022,   \n",
            "loss_aug_tea: 0.8422,   \n",
            "loss_cls: 0.0578,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.8489,   \n",
            "loss_aug_tea: 0.8739,   \n",
            "loss_cls: 0.0695,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.6537,   \n",
            "loss_aug_tea: 0.7402,   \n",
            "loss_cls: 0.0708,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.9523,   \n",
            "loss_aug_tea: 1.2066,   \n",
            "loss_cls: 0.0730,   \n",
            "loss_aug: 0.0733,   \n",
            "loss_ori_tea: 0.7444,   \n",
            "loss_aug_tea: 0.7995,   \n",
            "loss_cls: 0.0591,   \n",
            "loss_aug: 0.0595,   \n",
            "loss_ori_tea: 0.6916,   \n",
            "loss_aug_tea: 0.8711,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0651,   \n",
            "loss_ori_tea: 0.5822,   \n",
            "loss_aug_tea: 1.0552,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0659,   \n",
            "loss_ori_tea: 0.5874,   \n",
            "loss_aug_tea: 1.0638,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.7343,   \n",
            "loss_aug_tea: 0.8795,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.8215,   \n",
            "loss_aug_tea: 0.7888,   \n",
            "loss_cls: 0.0789,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 0.8621,   \n",
            "loss_aug_tea: 0.7702,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.8397,   \n",
            "loss_aug_tea: 0.9362,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0599,   \n",
            "loss_ori_tea: 0.7882,   \n",
            "loss_aug_tea: 0.7866,   \n",
            "loss_cls: 0.0586,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.7216,   \n",
            "loss_aug_tea: 0.9028,   \n",
            "loss_cls: 0.0556,   \n",
            "loss_aug: 0.0559,   \n",
            "loss_ori_tea: 0.7416,   \n",
            "loss_aug_tea: 0.8549,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0614,   \n",
            "loss_ori_tea: 0.7146,   \n",
            "loss_aug_tea: 0.8020,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0681,   \n",
            "loss_ori_tea: 0.8195,   \n",
            "loss_aug_tea: 1.3851,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0729,   \n",
            "loss_ori_tea: 0.6148,   \n",
            "loss_aug_tea: 0.7993,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0688,   \n",
            "loss_ori_tea: 0.6308,   \n",
            "loss_aug_tea: 1.1813,   \n",
            "loss_cls: 0.0793,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 0.6485,   \n",
            "loss_aug_tea: 1.0294,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.8286,   \n",
            "loss_aug_tea: 0.6938,   \n",
            "loss_cls: 0.0815,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 0.7543,   \n",
            "loss_aug_tea: 0.8423,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 0.5229,   \n",
            "loss_aug_tea: 1.1087,   \n",
            "loss_cls: 0.0669,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.6595,   \n",
            "loss_aug_tea: 0.6704,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.6389,   \n",
            "loss_aug_tea: 0.7676,   \n",
            "loss_cls: 0.0666,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.6330,   \n",
            "loss_aug_tea: 0.8201,   \n",
            "loss_cls: 0.0798,   \n",
            "loss_aug: 0.0802,   \n",
            "loss_ori_tea: 0.7191,   \n",
            "loss_aug_tea: 0.7570,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.6981,   \n",
            "loss_aug_tea: 0.8786,   \n",
            "loss_cls: 0.0839,   \n",
            "loss_aug: 0.0841,   \n",
            "loss_ori_tea: 0.6856,   \n",
            "loss_aug_tea: 0.8682,   \n",
            "loss_cls: 0.0642,   \n",
            "loss_aug: 0.0644,   \n",
            "loss_ori_tea: 0.7290,   \n",
            "loss_aug_tea: 0.8317,   \n",
            "loss_cls: 0.0648,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 0.6489,   \n",
            "loss_aug_tea: 0.6909,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.7422,   \n",
            "loss_aug_tea: 0.7209,   \n",
            "loss_cls: 0.0716,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.7240,   \n",
            "loss_aug_tea: 0.7586,   \n",
            "loss_cls: 0.0775,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 0.6565,   \n",
            "loss_aug_tea: 0.9595,   \n",
            "loss_cls: 0.0716,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.6639,   \n",
            "loss_aug_tea: 0.9883,   \n",
            "loss_cls: 0.0895,   \n",
            "loss_aug: 0.0897,   \n",
            "loss_ori_tea: 0.6897,   \n",
            "loss_aug_tea: 0.7911,   \n",
            "loss_cls: 0.0739,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.6842,   \n",
            "loss_aug_tea: 0.7767,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 0.8437,   \n",
            "loss_aug_tea: 0.7808,   \n",
            "loss_cls: 0.0689,   \n",
            "loss_aug: 0.0690,   \n",
            "loss_ori_tea: 0.6430,   \n",
            "loss_aug_tea: 0.7391,   \n",
            "loss_cls: 0.0613,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 1.1114,   \n",
            "loss_aug_tea: 0.7591,   \n",
            "loss_cls: 0.0756,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.5847,   \n",
            "loss_aug_tea: 0.9407,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0713,   \n",
            "loss_ori_tea: 0.6878,   \n",
            "loss_aug_tea: 0.7074,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.6097,   \n",
            "loss_aug_tea: 1.0193,   \n",
            "loss_cls: 0.0672,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.6153,   \n",
            "loss_aug_tea: 0.7737,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.6310,   \n",
            "loss_aug_tea: 0.7263,   \n",
            "loss_cls: 0.0766,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.6244,   \n",
            "loss_aug_tea: 0.8602,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.3458,   \n",
            "loss_aug_tea: 0.5562,   \n",
            "loss_cls: 0.0808,   \n",
            "loss_aug: 0.0810,   \n",
            "loss_ori_tea: 0.5744,   \n",
            "loss_aug_tea: 0.9081,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 0.5945,   \n",
            "loss_aug_tea: 0.8679,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0714,   \n",
            "loss_ori_tea: 0.9581,   \n",
            "loss_aug_tea: 1.1703,   \n",
            "loss_cls: 0.0913,   \n",
            "loss_aug: 0.0915,   \n",
            "loss_ori_tea: 0.6280,   \n",
            "loss_aug_tea: 0.9844,   \n",
            "loss_cls: 0.1003,   \n",
            "loss_aug: 0.1007,   \n",
            "loss_ori_tea: 1.1800,   \n",
            "loss_aug_tea: 1.2569,   \n",
            "loss_cls: 0.0628,   \n",
            "loss_aug: 0.0628,   \n",
            "loss_ori_tea: 0.9072,   \n",
            "loss_aug_tea: 0.7360,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0743,   \n",
            "loss_ori_tea: 0.9934,   \n",
            "loss_aug_tea: 1.0900,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.7210,   \n",
            "loss_aug_tea: 1.0137,   \n",
            "Epoch [05/15],   Mean Train Loss: 1.3519,   Mean Train Dice: 0.9289,   Mean Val Dice: 0.9612,   Mean Val ASD: 0.0016,   \n",
            "loss_cls: 0.0778,   \n",
            "loss_aug: 0.0780,   \n",
            "loss_ori_tea: 0.5929,   \n",
            "loss_aug_tea: 1.2775,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0686,   \n",
            "loss_ori_tea: 0.5375,   \n",
            "loss_aug_tea: 1.3989,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 0.9724,   \n",
            "loss_aug_tea: 0.9616,   \n",
            "loss_cls: 0.0837,   \n",
            "loss_aug: 0.0840,   \n",
            "loss_ori_tea: 0.8556,   \n",
            "loss_aug_tea: 0.8555,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 1.3243,   \n",
            "loss_aug_tea: 0.6322,   \n",
            "loss_cls: 0.0609,   \n",
            "loss_aug: 0.0612,   \n",
            "loss_ori_tea: 1.0243,   \n",
            "loss_aug_tea: 0.7350,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0637,   \n",
            "loss_ori_tea: 0.9486,   \n",
            "loss_aug_tea: 0.8280,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0808,   \n",
            "loss_ori_tea: 0.9294,   \n",
            "loss_aug_tea: 0.7653,   \n",
            "loss_cls: 0.0793,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 0.6918,   \n",
            "loss_aug_tea: 0.9069,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 0.8479,   \n",
            "loss_aug_tea: 0.7923,   \n",
            "loss_cls: 0.0620,   \n",
            "loss_aug: 0.0619,   \n",
            "loss_ori_tea: 0.9302,   \n",
            "loss_aug_tea: 0.5924,   \n",
            "loss_cls: 0.0600,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 0.7649,   \n",
            "loss_aug_tea: 0.8216,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0691,   \n",
            "loss_ori_tea: 0.8072,   \n",
            "loss_aug_tea: 0.7710,   \n",
            "loss_cls: 0.0862,   \n",
            "loss_aug: 0.0862,   \n",
            "loss_ori_tea: 0.6044,   \n",
            "loss_aug_tea: 0.9787,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.5768,   \n",
            "loss_aug_tea: 1.0871,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0604,   \n",
            "loss_ori_tea: 0.7433,   \n",
            "loss_aug_tea: 0.8055,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.5454,   \n",
            "loss_aug_tea: 1.0154,   \n",
            "loss_cls: 0.0779,   \n",
            "loss_aug: 0.0780,   \n",
            "loss_ori_tea: 0.5202,   \n",
            "loss_aug_tea: 0.7887,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0764,   \n",
            "loss_ori_tea: 0.6838,   \n",
            "loss_aug_tea: 1.0611,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0764,   \n",
            "loss_ori_tea: 1.0092,   \n",
            "loss_aug_tea: 0.9284,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 1.1432,   \n",
            "loss_aug_tea: 0.6988,   \n",
            "loss_cls: 0.0758,   \n",
            "loss_aug: 0.0761,   \n",
            "loss_ori_tea: 0.6585,   \n",
            "loss_aug_tea: 0.8457,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 0.6641,   \n",
            "loss_aug_tea: 0.7864,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0764,   \n",
            "loss_ori_tea: 0.6496,   \n",
            "loss_aug_tea: 0.6617,   \n",
            "loss_cls: 0.0617,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.5891,   \n",
            "loss_aug_tea: 0.7808,   \n",
            "loss_cls: 0.0549,   \n",
            "loss_aug: 0.0550,   \n",
            "loss_ori_tea: 0.6343,   \n",
            "loss_aug_tea: 1.4138,   \n",
            "loss_cls: 0.0788,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 0.7207,   \n",
            "loss_aug_tea: 0.6981,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0697,   \n",
            "loss_ori_tea: 0.6880,   \n",
            "loss_aug_tea: 0.8073,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0820,   \n",
            "loss_ori_tea: 0.7651,   \n",
            "loss_aug_tea: 0.9376,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0625,   \n",
            "loss_ori_tea: 0.6122,   \n",
            "loss_aug_tea: 1.0369,   \n",
            "loss_cls: 0.0646,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.7977,   \n",
            "loss_aug_tea: 1.0741,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.5730,   \n",
            "loss_aug_tea: 0.8388,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0781,   \n",
            "loss_ori_tea: 0.6298,   \n",
            "loss_aug_tea: 0.7323,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.6934,   \n",
            "loss_aug_tea: 0.6822,   \n",
            "loss_cls: 0.0572,   \n",
            "loss_aug: 0.0572,   \n",
            "loss_ori_tea: 0.6797,   \n",
            "loss_aug_tea: 0.8216,   \n",
            "loss_cls: 0.0754,   \n",
            "loss_aug: 0.0755,   \n",
            "loss_ori_tea: 0.7598,   \n",
            "loss_aug_tea: 0.9774,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.8024,   \n",
            "loss_aug_tea: 0.9274,   \n",
            "loss_cls: 0.0572,   \n",
            "loss_aug: 0.0574,   \n",
            "loss_ori_tea: 1.1073,   \n",
            "loss_aug_tea: 0.9719,   \n",
            "loss_cls: 0.0744,   \n",
            "loss_aug: 0.0746,   \n",
            "loss_ori_tea: 0.7406,   \n",
            "loss_aug_tea: 1.0636,   \n",
            "loss_cls: 0.0748,   \n",
            "loss_aug: 0.0751,   \n",
            "loss_ori_tea: 0.6768,   \n",
            "loss_aug_tea: 0.9262,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 0.7083,   \n",
            "loss_aug_tea: 0.7245,   \n",
            "loss_cls: 0.0800,   \n",
            "loss_aug: 0.0802,   \n",
            "loss_ori_tea: 0.9697,   \n",
            "loss_aug_tea: 1.1979,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 0.8379,   \n",
            "loss_aug_tea: 0.8247,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0808,   \n",
            "loss_ori_tea: 0.9585,   \n",
            "loss_aug_tea: 1.0158,   \n",
            "loss_cls: 0.0881,   \n",
            "loss_aug: 0.0883,   \n",
            "loss_ori_tea: 0.5910,   \n",
            "loss_aug_tea: 0.9632,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 0.5653,   \n",
            "loss_aug_tea: 1.0867,   \n",
            "loss_cls: 0.0785,   \n",
            "loss_aug: 0.0787,   \n",
            "loss_ori_tea: 0.7862,   \n",
            "loss_aug_tea: 0.8120,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.7211,   \n",
            "loss_aug_tea: 0.8004,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.5821,   \n",
            "loss_aug_tea: 0.9229,   \n",
            "loss_cls: 0.0641,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.6679,   \n",
            "loss_aug_tea: 0.8789,   \n",
            "loss_cls: 0.0689,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.7719,   \n",
            "loss_aug_tea: 0.7655,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.8381,   \n",
            "loss_aug_tea: 1.1455,   \n",
            "loss_cls: 0.0752,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.7966,   \n",
            "loss_aug_tea: 0.7670,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.5995,   \n",
            "loss_aug_tea: 1.0949,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.6174,   \n",
            "loss_aug_tea: 1.0168,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.6624,   \n",
            "loss_aug_tea: 0.9587,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0690,   \n",
            "loss_ori_tea: 0.9249,   \n",
            "loss_aug_tea: 0.8214,   \n",
            "loss_cls: 0.0757,   \n",
            "loss_aug: 0.0760,   \n",
            "loss_ori_tea: 0.8874,   \n",
            "loss_aug_tea: 0.8631,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.7645,   \n",
            "loss_aug_tea: 0.9473,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0625,   \n",
            "loss_ori_tea: 0.8675,   \n",
            "loss_aug_tea: 0.9666,   \n",
            "loss_cls: 0.0771,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.8510,   \n",
            "loss_aug_tea: 0.6965,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.6622,   \n",
            "loss_aug_tea: 0.8700,   \n",
            "loss_cls: 0.0885,   \n",
            "loss_aug: 0.0887,   \n",
            "loss_ori_tea: 0.7890,   \n",
            "loss_aug_tea: 0.7382,   \n",
            "loss_cls: 0.0893,   \n",
            "loss_aug: 0.0895,   \n",
            "loss_ori_tea: 0.7672,   \n",
            "loss_aug_tea: 0.8273,   \n",
            "loss_cls: 0.0772,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.9053,   \n",
            "loss_aug_tea: 0.8121,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0741,   \n",
            "loss_ori_tea: 0.7106,   \n",
            "loss_aug_tea: 0.9247,   \n",
            "Epoch [06/15],   Mean Train Loss: 2.2766,   Mean Train Dice: 0.9281,   Mean Val Dice: 0.9642,   Mean Val ASD: 0.0031,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0730,   \n",
            "loss_ori_tea: 0.6255,   \n",
            "loss_aug_tea: 1.0024,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0697,   \n",
            "loss_ori_tea: 0.7056,   \n",
            "loss_aug_tea: 0.8029,   \n",
            "loss_cls: 0.0696,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.5802,   \n",
            "loss_aug_tea: 1.1521,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.8564,   \n",
            "loss_aug_tea: 0.8666,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.6551,   \n",
            "loss_aug_tea: 0.9732,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0701,   \n",
            "loss_ori_tea: 0.6098,   \n",
            "loss_aug_tea: 0.8905,   \n",
            "loss_cls: 0.0757,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.7710,   \n",
            "loss_aug_tea: 0.7695,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 0.7215,   \n",
            "loss_aug_tea: 1.0641,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 0.7361,   \n",
            "loss_aug_tea: 0.8118,   \n",
            "loss_cls: 0.0829,   \n",
            "loss_aug: 0.0830,   \n",
            "loss_ori_tea: 0.7857,   \n",
            "loss_aug_tea: 0.8279,   \n",
            "loss_cls: 0.0746,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 0.6907,   \n",
            "loss_aug_tea: 0.7374,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0845,   \n",
            "loss_ori_tea: 0.5495,   \n",
            "loss_aug_tea: 1.1827,   \n",
            "loss_cls: 0.0656,   \n",
            "loss_aug: 0.0657,   \n",
            "loss_ori_tea: 0.6011,   \n",
            "loss_aug_tea: 0.8951,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 0.8429,   \n",
            "loss_aug_tea: 0.7421,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.6337,   \n",
            "loss_aug_tea: 0.8804,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.9693,   \n",
            "loss_aug_tea: 1.1802,   \n",
            "loss_cls: 0.0566,   \n",
            "loss_aug: 0.0568,   \n",
            "loss_ori_tea: 1.1241,   \n",
            "loss_aug_tea: 0.6400,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.7136,   \n",
            "loss_aug_tea: 0.8344,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0631,   \n",
            "loss_ori_tea: 0.5737,   \n",
            "loss_aug_tea: 1.0792,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0782,   \n",
            "loss_ori_tea: 0.8065,   \n",
            "loss_aug_tea: 0.8467,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.9856,   \n",
            "loss_aug_tea: 0.7568,   \n",
            "loss_cls: 0.0696,   \n",
            "loss_aug: 0.0699,   \n",
            "loss_ori_tea: 0.6162,   \n",
            "loss_aug_tea: 0.8311,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0790,   \n",
            "loss_ori_tea: 0.6948,   \n",
            "loss_aug_tea: 0.7494,   \n",
            "loss_cls: 0.0590,   \n",
            "loss_aug: 0.0592,   \n",
            "loss_ori_tea: 1.1959,   \n",
            "loss_aug_tea: 0.9299,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.9440,   \n",
            "loss_aug_tea: 0.9962,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 0.8756,   \n",
            "loss_aug_tea: 0.8672,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0797,   \n",
            "loss_ori_tea: 0.8392,   \n",
            "loss_aug_tea: 0.7879,   \n",
            "loss_cls: 0.0803,   \n",
            "loss_aug: 0.0805,   \n",
            "loss_ori_tea: 0.7747,   \n",
            "loss_aug_tea: 0.7781,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.9200,   \n",
            "loss_aug_tea: 0.9554,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 1.0872,   \n",
            "loss_aug_tea: 0.9101,   \n",
            "loss_cls: 0.0489,   \n",
            "loss_aug: 0.0492,   \n",
            "loss_ori_tea: 0.7001,   \n",
            "loss_aug_tea: 0.6784,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0761,   \n",
            "loss_ori_tea: 0.6608,   \n",
            "loss_aug_tea: 0.9298,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0629,   \n",
            "loss_ori_tea: 0.7116,   \n",
            "loss_aug_tea: 0.8371,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0645,   \n",
            "loss_ori_tea: 0.7423,   \n",
            "loss_aug_tea: 0.6899,   \n",
            "loss_cls: 0.0709,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.9357,   \n",
            "loss_aug_tea: 0.7843,   \n",
            "loss_cls: 0.0850,   \n",
            "loss_aug: 0.0853,   \n",
            "loss_ori_tea: 0.7133,   \n",
            "loss_aug_tea: 0.7956,   \n",
            "loss_cls: 0.0733,   \n",
            "loss_aug: 0.0735,   \n",
            "loss_ori_tea: 0.7543,   \n",
            "loss_aug_tea: 0.7980,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0659,   \n",
            "loss_ori_tea: 0.6599,   \n",
            "loss_aug_tea: 1.0396,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0730,   \n",
            "loss_ori_tea: 0.8359,   \n",
            "loss_aug_tea: 1.0623,   \n",
            "loss_cls: 0.0588,   \n",
            "loss_aug: 0.0591,   \n",
            "loss_ori_tea: 0.8258,   \n",
            "loss_aug_tea: 0.9074,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 0.6776,   \n",
            "loss_aug_tea: 0.6922,   \n",
            "loss_cls: 0.0642,   \n",
            "loss_aug: 0.0644,   \n",
            "loss_ori_tea: 0.6283,   \n",
            "loss_aug_tea: 0.8796,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.6678,   \n",
            "loss_aug_tea: 0.6994,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 1.2448,   \n",
            "loss_aug_tea: 1.3365,   \n",
            "loss_cls: 0.0898,   \n",
            "loss_aug: 0.0900,   \n",
            "loss_ori_tea: 0.7136,   \n",
            "loss_aug_tea: 0.7524,   \n",
            "loss_cls: 0.0663,   \n",
            "loss_aug: 0.0664,   \n",
            "loss_ori_tea: 0.9133,   \n",
            "loss_aug_tea: 0.6804,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0700,   \n",
            "loss_ori_tea: 0.6685,   \n",
            "loss_aug_tea: 0.8049,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.9181,   \n",
            "loss_aug_tea: 1.2270,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.7191,   \n",
            "loss_aug_tea: 0.9390,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 0.9067,   \n",
            "loss_aug_tea: 1.1219,   \n",
            "loss_cls: 0.0784,   \n",
            "loss_aug: 0.0785,   \n",
            "loss_ori_tea: 0.6965,   \n",
            "loss_aug_tea: 0.7197,   \n",
            "loss_cls: 0.0890,   \n",
            "loss_aug: 0.0892,   \n",
            "loss_ori_tea: 0.6410,   \n",
            "loss_aug_tea: 0.7944,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 1.0311,   \n",
            "loss_aug_tea: 0.6414,   \n",
            "loss_cls: 0.0843,   \n",
            "loss_aug: 0.0845,   \n",
            "loss_ori_tea: 0.6254,   \n",
            "loss_aug_tea: 0.8983,   \n",
            "loss_cls: 0.0620,   \n",
            "loss_aug: 0.0620,   \n",
            "loss_ori_tea: 0.7833,   \n",
            "loss_aug_tea: 0.8182,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0596,   \n",
            "loss_ori_tea: 1.1609,   \n",
            "loss_aug_tea: 0.9423,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0637,   \n",
            "loss_ori_tea: 0.7196,   \n",
            "loss_aug_tea: 0.8813,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 1.0526,   \n",
            "loss_aug_tea: 0.9752,   \n",
            "loss_cls: 0.0790,   \n",
            "loss_aug: 0.0792,   \n",
            "loss_ori_tea: 0.8689,   \n",
            "loss_aug_tea: 1.1126,   \n",
            "loss_cls: 0.0755,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.7330,   \n",
            "loss_aug_tea: 0.8899,   \n",
            "loss_cls: 0.0606,   \n",
            "loss_aug: 0.0609,   \n",
            "loss_ori_tea: 0.7693,   \n",
            "loss_aug_tea: 0.8511,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 0.7103,   \n",
            "loss_aug_tea: 0.8829,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.6897,   \n",
            "loss_aug_tea: 0.7009,   \n",
            "loss_cls: 0.0863,   \n",
            "loss_aug: 0.0865,   \n",
            "loss_ori_tea: 0.6871,   \n",
            "loss_aug_tea: 0.9340,   \n",
            "loss_cls: 0.0715,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.8792,   \n",
            "loss_aug_tea: 0.8199,   \n",
            "loss_cls: 0.0873,   \n",
            "loss_aug: 0.0874,   \n",
            "loss_ori_tea: 0.8371,   \n",
            "loss_aug_tea: 0.8460,   \n",
            "Epoch [07/15],   Mean Train Loss: 3.0836,   Mean Train Dice: 0.9283,   Mean Val Dice: 0.9618,   Mean Val ASD: 0.0019,   \n",
            "loss_cls: 0.0586,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.7097,   \n",
            "loss_aug_tea: 0.8795,   \n",
            "loss_cls: 0.0790,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 1.1243,   \n",
            "loss_aug_tea: 0.9707,   \n",
            "loss_cls: 0.0731,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.9678,   \n",
            "loss_aug_tea: 0.8183,   \n",
            "loss_cls: 0.0761,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 0.9374,   \n",
            "loss_aug_tea: 0.7168,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.8586,   \n",
            "loss_aug_tea: 0.9121,   \n",
            "loss_cls: 0.0572,   \n",
            "loss_aug: 0.0573,   \n",
            "loss_ori_tea: 0.7385,   \n",
            "loss_aug_tea: 0.9277,   \n",
            "loss_cls: 0.0652,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 0.9918,   \n",
            "loss_aug_tea: 0.8640,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.6263,   \n",
            "loss_aug_tea: 1.0810,   \n",
            "loss_cls: 0.0835,   \n",
            "loss_aug: 0.0836,   \n",
            "loss_ori_tea: 0.8705,   \n",
            "loss_aug_tea: 0.8577,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0724,   \n",
            "loss_ori_tea: 0.6431,   \n",
            "loss_aug_tea: 0.8868,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0768,   \n",
            "loss_ori_tea: 1.0496,   \n",
            "loss_aug_tea: 0.9667,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.7789,   \n",
            "loss_aug_tea: 0.7424,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 0.7174,   \n",
            "loss_aug_tea: 0.9031,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0666,   \n",
            "loss_ori_tea: 0.6773,   \n",
            "loss_aug_tea: 0.7778,   \n",
            "loss_cls: 0.0709,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.8350,   \n",
            "loss_aug_tea: 0.8576,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0807,   \n",
            "loss_ori_tea: 1.0250,   \n",
            "loss_aug_tea: 0.9808,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0693,   \n",
            "loss_ori_tea: 0.6562,   \n",
            "loss_aug_tea: 0.7434,   \n",
            "loss_cls: 0.0819,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 0.7158,   \n",
            "loss_aug_tea: 0.9665,   \n",
            "loss_cls: 0.0494,   \n",
            "loss_aug: 0.0495,   \n",
            "loss_ori_tea: 1.1400,   \n",
            "loss_aug_tea: 0.7505,   \n",
            "loss_cls: 0.0716,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 0.7421,   \n",
            "loss_aug_tea: 0.8554,   \n",
            "loss_cls: 0.0651,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 1.0146,   \n",
            "loss_aug_tea: 0.9749,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.8169,   \n",
            "loss_aug_tea: 0.8486,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 0.9473,   \n",
            "loss_aug_tea: 0.7596,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.7106,   \n",
            "loss_aug_tea: 0.9798,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0648,   \n",
            "loss_ori_tea: 0.8426,   \n",
            "loss_aug_tea: 0.7379,   \n",
            "loss_cls: 0.0845,   \n",
            "loss_aug: 0.0848,   \n",
            "loss_ori_tea: 1.0049,   \n",
            "loss_aug_tea: 0.8391,   \n",
            "loss_cls: 0.0951,   \n",
            "loss_aug: 0.0953,   \n",
            "loss_ori_tea: 0.7304,   \n",
            "loss_aug_tea: 0.8965,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0866,   \n",
            "loss_ori_tea: 0.7317,   \n",
            "loss_aug_tea: 0.8404,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0707,   \n",
            "loss_ori_tea: 0.7725,   \n",
            "loss_aug_tea: 0.7103,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 1.0721,   \n",
            "loss_aug_tea: 1.0263,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0771,   \n",
            "loss_ori_tea: 0.9766,   \n",
            "loss_aug_tea: 1.3267,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.8538,   \n",
            "loss_aug_tea: 0.7994,   \n",
            "loss_cls: 0.0562,   \n",
            "loss_aug: 0.0565,   \n",
            "loss_ori_tea: 0.8456,   \n",
            "loss_aug_tea: 0.7485,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 1.2221,   \n",
            "loss_aug_tea: 0.6745,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 0.6866,   \n",
            "loss_aug_tea: 0.9189,   \n",
            "loss_cls: 0.0801,   \n",
            "loss_aug: 0.0803,   \n",
            "loss_ori_tea: 0.7278,   \n",
            "loss_aug_tea: 0.7663,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.8679,   \n",
            "loss_aug_tea: 0.7296,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.8916,   \n",
            "loss_aug_tea: 0.8065,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.6359,   \n",
            "loss_aug_tea: 1.0759,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.8247,   \n",
            "loss_aug_tea: 0.7512,   \n",
            "loss_cls: 0.0746,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.9332,   \n",
            "loss_aug_tea: 0.9435,   \n",
            "loss_cls: 0.0852,   \n",
            "loss_aug: 0.0854,   \n",
            "loss_ori_tea: 1.2204,   \n",
            "loss_aug_tea: 0.9528,   \n",
            "loss_cls: 0.0732,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.6838,   \n",
            "loss_aug_tea: 0.8506,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.7604,   \n",
            "loss_aug_tea: 0.9023,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 1.0570,   \n",
            "loss_aug_tea: 1.0579,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0806,   \n",
            "loss_ori_tea: 0.6434,   \n",
            "loss_aug_tea: 0.8464,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0751,   \n",
            "loss_ori_tea: 0.7351,   \n",
            "loss_aug_tea: 0.8761,   \n",
            "loss_cls: 0.0803,   \n",
            "loss_aug: 0.0805,   \n",
            "loss_ori_tea: 0.7927,   \n",
            "loss_aug_tea: 0.7729,   \n",
            "loss_cls: 0.0705,   \n",
            "loss_aug: 0.0706,   \n",
            "loss_ori_tea: 1.2437,   \n",
            "loss_aug_tea: 0.5994,   \n",
            "loss_cls: 0.0756,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.9527,   \n",
            "loss_aug_tea: 1.0038,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 0.7671,   \n",
            "loss_aug_tea: 0.8068,   \n",
            "loss_cls: 0.0937,   \n",
            "loss_aug: 0.0938,   \n",
            "loss_ori_tea: 0.6917,   \n",
            "loss_aug_tea: 0.8615,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.6946,   \n",
            "loss_aug_tea: 0.7987,   \n",
            "loss_cls: 0.0809,   \n",
            "loss_aug: 0.0811,   \n",
            "loss_ori_tea: 0.6712,   \n",
            "loss_aug_tea: 0.8856,   \n",
            "loss_cls: 0.0723,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 0.6826,   \n",
            "loss_aug_tea: 0.9504,   \n",
            "loss_cls: 0.0662,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.8085,   \n",
            "loss_aug_tea: 0.8889,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 0.7133,   \n",
            "loss_aug_tea: 0.8261,   \n",
            "loss_cls: 0.0730,   \n",
            "loss_aug: 0.0733,   \n",
            "loss_ori_tea: 0.8648,   \n",
            "loss_aug_tea: 0.7163,   \n",
            "loss_cls: 0.0831,   \n",
            "loss_aug: 0.0834,   \n",
            "loss_ori_tea: 0.9258,   \n",
            "loss_aug_tea: 0.8461,   \n",
            "loss_cls: 0.0805,   \n",
            "loss_aug: 0.0807,   \n",
            "loss_ori_tea: 1.0914,   \n",
            "loss_aug_tea: 0.8550,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0846,   \n",
            "loss_ori_tea: 1.1687,   \n",
            "loss_aug_tea: 0.6845,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0785,   \n",
            "loss_ori_tea: 0.7689,   \n",
            "loss_aug_tea: 0.7799,   \n",
            "loss_cls: 0.0694,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.7466,   \n",
            "loss_aug_tea: 0.8235,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.7111,   \n",
            "loss_aug_tea: 0.8590,   \n",
            "loss_cls: 0.0581,   \n",
            "loss_aug: 0.0583,   \n",
            "loss_ori_tea: 0.7725,   \n",
            "loss_aug_tea: 0.8317,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.7901,   \n",
            "loss_aug_tea: 1.0153,   \n",
            "Epoch [08/15],   Mean Train Loss: 3.4907,   Mean Train Dice: 0.9268,   Mean Val Dice: 0.9618,   Mean Val ASD: 0.0032,   \n",
            "loss_cls: 0.0655,   \n",
            "loss_aug: 0.0656,   \n",
            "loss_ori_tea: 0.6451,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0852,   \n",
            "loss_aug: 0.0856,   \n",
            "loss_ori_tea: 0.7989,   \n",
            "loss_aug_tea: 0.8315,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.6727,   \n",
            "loss_aug_tea: 0.8547,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 0.6933,   \n",
            "loss_aug_tea: 0.8128,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.8351,   \n",
            "loss_aug_tea: 0.7817,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.6655,   \n",
            "loss_aug_tea: 0.7620,   \n",
            "loss_cls: 0.0835,   \n",
            "loss_aug: 0.0838,   \n",
            "loss_ori_tea: 0.7267,   \n",
            "loss_aug_tea: 0.7619,   \n",
            "loss_cls: 0.0577,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.8745,   \n",
            "loss_aug_tea: 0.9897,   \n",
            "loss_cls: 0.0689,   \n",
            "loss_aug: 0.0691,   \n",
            "loss_ori_tea: 0.9383,   \n",
            "loss_aug_tea: 0.7347,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.8873,   \n",
            "loss_aug_tea: 0.9655,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.9758,   \n",
            "loss_aug_tea: 1.0247,   \n",
            "loss_cls: 0.0793,   \n",
            "loss_aug: 0.0794,   \n",
            "loss_ori_tea: 0.8230,   \n",
            "loss_aug_tea: 1.0221,   \n",
            "loss_cls: 0.0658,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.7398,   \n",
            "loss_aug_tea: 0.9522,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.8089,   \n",
            "loss_aug_tea: 0.7570,   \n",
            "loss_cls: 0.0856,   \n",
            "loss_aug: 0.0859,   \n",
            "loss_ori_tea: 0.8432,   \n",
            "loss_aug_tea: 0.8372,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0730,   \n",
            "loss_ori_tea: 0.7464,   \n",
            "loss_aug_tea: 0.9214,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 0.6870,   \n",
            "loss_aug_tea: 0.9769,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 1.2080,   \n",
            "loss_aug_tea: 1.2455,   \n",
            "loss_cls: 0.0660,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.7049,   \n",
            "loss_aug_tea: 0.8687,   \n",
            "loss_cls: 0.0733,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.6812,   \n",
            "loss_aug_tea: 0.9415,   \n",
            "loss_cls: 0.0595,   \n",
            "loss_aug: 0.0600,   \n",
            "loss_ori_tea: 1.0222,   \n",
            "loss_aug_tea: 0.7308,   \n",
            "loss_cls: 0.0702,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.7685,   \n",
            "loss_aug_tea: 1.0597,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.7073,   \n",
            "loss_aug_tea: 0.8510,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 0.9635,   \n",
            "loss_aug_tea: 0.9001,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 1.3123,   \n",
            "loss_aug_tea: 1.0495,   \n",
            "loss_cls: 0.0817,   \n",
            "loss_aug: 0.0818,   \n",
            "loss_ori_tea: 0.9623,   \n",
            "loss_aug_tea: 0.9882,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0848,   \n",
            "loss_ori_tea: 0.7284,   \n",
            "loss_aug_tea: 0.8130,   \n",
            "loss_cls: 0.0839,   \n",
            "loss_aug: 0.0841,   \n",
            "loss_ori_tea: 0.7778,   \n",
            "loss_aug_tea: 0.8999,   \n",
            "loss_cls: 0.0802,   \n",
            "loss_aug: 0.0806,   \n",
            "loss_ori_tea: 0.9487,   \n",
            "loss_aug_tea: 0.8071,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 0.6680,   \n",
            "loss_aug_tea: 0.8161,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0666,   \n",
            "loss_ori_tea: 1.1420,   \n",
            "loss_aug_tea: 0.7820,   \n",
            "loss_cls: 0.0547,   \n",
            "loss_aug: 0.0550,   \n",
            "loss_ori_tea: 0.8528,   \n",
            "loss_aug_tea: 0.9178,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0769,   \n",
            "loss_ori_tea: 0.6966,   \n",
            "loss_aug_tea: 1.0523,   \n",
            "loss_cls: 0.0880,   \n",
            "loss_aug: 0.0882,   \n",
            "loss_ori_tea: 1.1824,   \n",
            "loss_aug_tea: 0.7344,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0693,   \n",
            "loss_ori_tea: 0.8098,   \n",
            "loss_aug_tea: 0.8579,   \n",
            "loss_cls: 0.0581,   \n",
            "loss_aug: 0.0582,   \n",
            "loss_ori_tea: 0.8480,   \n",
            "loss_aug_tea: 1.0287,   \n",
            "loss_cls: 0.0628,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 0.8206,   \n",
            "loss_aug_tea: 1.0680,   \n",
            "loss_cls: 0.0836,   \n",
            "loss_aug: 0.0839,   \n",
            "loss_ori_tea: 0.8029,   \n",
            "loss_aug_tea: 0.9438,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0609,   \n",
            "loss_ori_tea: 0.8544,   \n",
            "loss_aug_tea: 0.7459,   \n",
            "loss_cls: 0.0714,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.9773,   \n",
            "loss_aug_tea: 0.7210,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0741,   \n",
            "loss_ori_tea: 0.9754,   \n",
            "loss_aug_tea: 0.7383,   \n",
            "loss_cls: 0.0798,   \n",
            "loss_aug: 0.0799,   \n",
            "loss_ori_tea: 0.8361,   \n",
            "loss_aug_tea: 0.7158,   \n",
            "loss_cls: 0.0859,   \n",
            "loss_aug: 0.0861,   \n",
            "loss_ori_tea: 0.8879,   \n",
            "loss_aug_tea: 1.0198,   \n",
            "loss_cls: 0.0655,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 1.2484,   \n",
            "loss_aug_tea: 0.8343,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0638,   \n",
            "loss_ori_tea: 0.7948,   \n",
            "loss_aug_tea: 0.9001,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0821,   \n",
            "loss_ori_tea: 0.9537,   \n",
            "loss_aug_tea: 0.9862,   \n",
            "loss_cls: 0.0663,   \n",
            "loss_aug: 0.0666,   \n",
            "loss_ori_tea: 0.8499,   \n",
            "loss_aug_tea: 0.9989,   \n",
            "loss_cls: 0.0624,   \n",
            "loss_aug: 0.0626,   \n",
            "loss_ori_tea: 0.9341,   \n",
            "loss_aug_tea: 0.8881,   \n",
            "loss_cls: 0.0633,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 1.1718,   \n",
            "loss_aug_tea: 1.1592,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.9391,   \n",
            "loss_aug_tea: 0.8399,   \n",
            "loss_cls: 0.0934,   \n",
            "loss_aug: 0.0936,   \n",
            "loss_ori_tea: 0.7929,   \n",
            "loss_aug_tea: 0.8734,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 0.7588,   \n",
            "loss_aug_tea: 0.9784,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.8100,   \n",
            "loss_aug_tea: 0.8215,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.8842,   \n",
            "loss_aug_tea: 0.8676,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0665,   \n",
            "loss_ori_tea: 0.8001,   \n",
            "loss_aug_tea: 0.9712,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 0.7711,   \n",
            "loss_aug_tea: 0.8800,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0771,   \n",
            "loss_ori_tea: 0.8541,   \n",
            "loss_aug_tea: 1.0938,   \n",
            "loss_cls: 0.0562,   \n",
            "loss_aug: 0.0563,   \n",
            "loss_ori_tea: 1.0415,   \n",
            "loss_aug_tea: 0.9299,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0677,   \n",
            "loss_ori_tea: 1.1624,   \n",
            "loss_aug_tea: 0.7702,   \n",
            "loss_cls: 0.0687,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.6745,   \n",
            "loss_aug_tea: 0.9955,   \n",
            "loss_cls: 0.0758,   \n",
            "loss_aug: 0.0760,   \n",
            "loss_ori_tea: 0.7406,   \n",
            "loss_aug_tea: 0.8806,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.6952,   \n",
            "loss_aug_tea: 0.9634,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.7422,   \n",
            "loss_aug_tea: 0.9082,   \n",
            "loss_cls: 0.0700,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.7840,   \n",
            "loss_aug_tea: 0.9802,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 0.8548,   \n",
            "loss_aug_tea: 0.8985,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0629,   \n",
            "loss_ori_tea: 0.7363,   \n",
            "loss_aug_tea: 0.7827,   \n",
            "Epoch [09/15],   Mean Train Loss: 3.5850,   Mean Train Dice: 0.9293,   Mean Val Dice: 0.9616,   Mean Val ASD: 0.0028,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.6812,   \n",
            "loss_aug_tea: 0.9096,   \n",
            "loss_cls: 0.0702,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.8645,   \n",
            "loss_aug_tea: 0.8967,   \n",
            "loss_cls: 0.0880,   \n",
            "loss_aug: 0.0884,   \n",
            "loss_ori_tea: 0.7518,   \n",
            "loss_aug_tea: 0.8479,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0724,   \n",
            "loss_ori_tea: 0.9892,   \n",
            "loss_aug_tea: 0.7091,   \n",
            "loss_cls: 0.0827,   \n",
            "loss_aug: 0.0835,   \n",
            "loss_ori_tea: 0.8874,   \n",
            "loss_aug_tea: 1.0257,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.9711,   \n",
            "loss_aug_tea: 0.7995,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8106,   \n",
            "loss_aug_tea: 0.9688,   \n",
            "loss_cls: 0.0644,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.8751,   \n",
            "loss_aug_tea: 1.0608,   \n",
            "loss_cls: 0.0763,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 1.1173,   \n",
            "loss_aug_tea: 0.6881,   \n",
            "loss_cls: 0.0566,   \n",
            "loss_aug: 0.0569,   \n",
            "loss_ori_tea: 0.7604,   \n",
            "loss_aug_tea: 0.7930,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 0.9708,   \n",
            "loss_aug_tea: 1.0221,   \n",
            "loss_cls: 0.0892,   \n",
            "loss_aug: 0.0894,   \n",
            "loss_ori_tea: 0.8754,   \n",
            "loss_aug_tea: 0.8896,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 1.0736,   \n",
            "loss_aug_tea: 1.1723,   \n",
            "loss_cls: 0.0812,   \n",
            "loss_aug: 0.0815,   \n",
            "loss_ori_tea: 1.1734,   \n",
            "loss_aug_tea: 1.0786,   \n",
            "loss_cls: 0.0666,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.9080,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0675,   \n",
            "loss_aug: 0.0679,   \n",
            "loss_ori_tea: 1.0789,   \n",
            "loss_aug_tea: 0.9007,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.7158,   \n",
            "loss_aug_tea: 1.0172,   \n",
            "loss_cls: 0.0813,   \n",
            "loss_aug: 0.0816,   \n",
            "loss_ori_tea: 0.9377,   \n",
            "loss_aug_tea: 0.9745,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0633,   \n",
            "loss_ori_tea: 0.9185,   \n",
            "loss_aug_tea: 0.8355,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 1.0176,   \n",
            "loss_aug_tea: 0.9455,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.8934,   \n",
            "loss_aug_tea: 0.9132,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 0.9218,   \n",
            "loss_aug_tea: 1.0762,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0768,   \n",
            "loss_ori_tea: 0.8299,   \n",
            "loss_aug_tea: 0.8350,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 1.2404,   \n",
            "loss_aug_tea: 0.9092,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 0.9558,   \n",
            "loss_aug_tea: 0.9720,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.9417,   \n",
            "loss_aug_tea: 0.8609,   \n",
            "loss_cls: 0.0720,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 1.2168,   \n",
            "loss_aug_tea: 0.8693,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 1.0296,   \n",
            "loss_aug_tea: 1.0039,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 1.2118,   \n",
            "loss_aug_tea: 1.1540,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.9666,   \n",
            "loss_aug_tea: 0.8791,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 0.9160,   \n",
            "loss_aug_tea: 0.8814,   \n",
            "loss_cls: 0.0610,   \n",
            "loss_aug: 0.0610,   \n",
            "loss_ori_tea: 1.2152,   \n",
            "loss_aug_tea: 0.8519,   \n",
            "loss_cls: 0.0883,   \n",
            "loss_aug: 0.0885,   \n",
            "loss_ori_tea: 0.8612,   \n",
            "loss_aug_tea: 0.9288,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.8820,   \n",
            "loss_aug_tea: 0.8533,   \n",
            "loss_cls: 0.0742,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 1.0681,   \n",
            "loss_aug_tea: 0.9060,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0866,   \n",
            "loss_ori_tea: 0.7257,   \n",
            "loss_aug_tea: 0.9976,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.8697,   \n",
            "loss_aug_tea: 1.1288,   \n",
            "loss_cls: 0.0930,   \n",
            "loss_aug: 0.0932,   \n",
            "loss_ori_tea: 0.8072,   \n",
            "loss_aug_tea: 0.8269,   \n",
            "loss_cls: 0.0715,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 0.9563,   \n",
            "loss_aug_tea: 1.0183,   \n",
            "loss_cls: 0.0617,   \n",
            "loss_aug: 0.0619,   \n",
            "loss_ori_tea: 1.2153,   \n",
            "loss_aug_tea: 0.9075,   \n",
            "loss_cls: 0.0812,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 0.8218,   \n",
            "loss_aug_tea: 0.7643,   \n",
            "loss_cls: 0.0809,   \n",
            "loss_aug: 0.0813,   \n",
            "loss_ori_tea: 0.9779,   \n",
            "loss_aug_tea: 0.9490,   \n",
            "loss_cls: 0.0785,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 0.7351,   \n",
            "loss_aug_tea: 0.9780,   \n",
            "loss_cls: 0.0620,   \n",
            "loss_aug: 0.0627,   \n",
            "loss_ori_tea: 0.8890,   \n",
            "loss_aug_tea: 0.8863,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 1.1716,   \n",
            "loss_aug_tea: 1.0173,   \n",
            "loss_cls: 0.0797,   \n",
            "loss_aug: 0.0800,   \n",
            "loss_ori_tea: 1.0676,   \n",
            "loss_aug_tea: 0.9200,   \n",
            "loss_cls: 0.0836,   \n",
            "loss_aug: 0.0838,   \n",
            "loss_ori_tea: 0.9206,   \n",
            "loss_aug_tea: 0.7728,   \n",
            "loss_cls: 0.0753,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.8503,   \n",
            "loss_aug_tea: 0.9917,   \n",
            "loss_cls: 0.1019,   \n",
            "loss_aug: 0.1024,   \n",
            "loss_ori_tea: 0.8293,   \n",
            "loss_aug_tea: 0.8984,   \n",
            "loss_cls: 0.0678,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 1.2470,   \n",
            "loss_aug_tea: 0.8070,   \n",
            "loss_cls: 0.0717,   \n",
            "loss_aug: 0.0720,   \n",
            "loss_ori_tea: 0.8911,   \n",
            "loss_aug_tea: 0.8704,   \n",
            "loss_cls: 0.0656,   \n",
            "loss_aug: 0.0659,   \n",
            "loss_ori_tea: 0.8691,   \n",
            "loss_aug_tea: 0.8458,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.7487,   \n",
            "loss_aug_tea: 0.9228,   \n",
            "loss_cls: 0.0819,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 1.0574,   \n",
            "loss_aug_tea: 0.9480,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.8134,   \n",
            "loss_aug_tea: 0.9571,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 1.0208,   \n",
            "loss_aug_tea: 0.8653,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 1.0962,   \n",
            "loss_aug_tea: 0.7975,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 1.0243,   \n",
            "loss_aug_tea: 0.7753,   \n",
            "loss_cls: 0.0779,   \n",
            "loss_aug: 0.0782,   \n",
            "loss_ori_tea: 1.0253,   \n",
            "loss_aug_tea: 1.0227,   \n",
            "loss_cls: 0.0579,   \n",
            "loss_aug: 0.0583,   \n",
            "loss_ori_tea: 1.0605,   \n",
            "loss_aug_tea: 1.0207,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0675,   \n",
            "loss_ori_tea: 1.0746,   \n",
            "loss_aug_tea: 0.9612,   \n",
            "loss_cls: 0.0789,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 1.1516,   \n",
            "loss_aug_tea: 0.7944,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 0.9223,   \n",
            "loss_aug_tea: 0.9510,   \n",
            "loss_cls: 0.0746,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.8328,   \n",
            "loss_aug_tea: 0.8914,   \n",
            "loss_cls: 0.0714,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 0.8901,   \n",
            "loss_aug_tea: 0.9377,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 0.6668,   \n",
            "loss_aug_tea: 0.9004,   \n",
            "Epoch [10/15],   Mean Train Loss: 3.8162,   Mean Train Dice: 0.9264,   Mean Val Dice: 0.9616,   Mean Val ASD: 0.0026,   \n",
            "loss_cls: 0.0739,   \n",
            "loss_aug: 0.0742,   \n",
            "loss_ori_tea: 1.3850,   \n",
            "loss_aug_tea: 0.9499,   \n",
            "loss_cls: 0.0826,   \n",
            "loss_aug: 0.0831,   \n",
            "loss_ori_tea: 0.8138,   \n",
            "loss_aug_tea: 0.9390,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 0.9870,   \n",
            "loss_aug_tea: 0.9257,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.8970,   \n",
            "loss_aug_tea: 1.0137,   \n",
            "loss_cls: 0.0811,   \n",
            "loss_aug: 0.0813,   \n",
            "loss_ori_tea: 0.9323,   \n",
            "loss_aug_tea: 0.8066,   \n",
            "loss_cls: 0.0846,   \n",
            "loss_aug: 0.0847,   \n",
            "loss_ori_tea: 0.9118,   \n",
            "loss_aug_tea: 0.9733,   \n",
            "loss_cls: 0.0897,   \n",
            "loss_aug: 0.0899,   \n",
            "loss_ori_tea: 0.9367,   \n",
            "loss_aug_tea: 0.8525,   \n",
            "loss_cls: 0.0624,   \n",
            "loss_aug: 0.0626,   \n",
            "loss_ori_tea: 1.0246,   \n",
            "loss_aug_tea: 1.3298,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0786,   \n",
            "loss_ori_tea: 0.8384,   \n",
            "loss_aug_tea: 0.8806,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.8390,   \n",
            "loss_aug_tea: 0.9002,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0720,   \n",
            "loss_ori_tea: 0.8881,   \n",
            "loss_aug_tea: 0.9693,   \n",
            "loss_cls: 0.0658,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 0.9038,   \n",
            "loss_aug_tea: 0.8169,   \n",
            "loss_cls: 0.0925,   \n",
            "loss_aug: 0.0928,   \n",
            "loss_ori_tea: 1.2793,   \n",
            "loss_aug_tea: 1.2901,   \n",
            "loss_cls: 0.0756,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.8704,   \n",
            "loss_aug_tea: 0.9550,   \n",
            "loss_cls: 0.0521,   \n",
            "loss_aug: 0.0524,   \n",
            "loss_ori_tea: 1.0222,   \n",
            "loss_aug_tea: 0.8105,   \n",
            "loss_cls: 0.0849,   \n",
            "loss_aug: 0.0850,   \n",
            "loss_ori_tea: 0.9016,   \n",
            "loss_aug_tea: 0.9284,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 0.9526,   \n",
            "loss_aug_tea: 0.9750,   \n",
            "loss_cls: 0.0527,   \n",
            "loss_aug: 0.0532,   \n",
            "loss_ori_tea: 1.0373,   \n",
            "loss_aug_tea: 0.9852,   \n",
            "loss_cls: 0.0690,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 1.3595,   \n",
            "loss_aug_tea: 1.1623,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.7984,   \n",
            "loss_aug_tea: 0.9796,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 1.0239,   \n",
            "loss_aug_tea: 1.0159,   \n",
            "loss_cls: 0.0600,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 1.0318,   \n",
            "loss_aug_tea: 0.8207,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0742,   \n",
            "loss_ori_tea: 1.2906,   \n",
            "loss_aug_tea: 0.7533,   \n",
            "loss_cls: 0.0847,   \n",
            "loss_aug: 0.0849,   \n",
            "loss_ori_tea: 0.7266,   \n",
            "loss_aug_tea: 0.9600,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0681,   \n",
            "loss_ori_tea: 1.0687,   \n",
            "loss_aug_tea: 1.2163,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0714,   \n",
            "loss_ori_tea: 0.7994,   \n",
            "loss_aug_tea: 0.9088,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.0276,   \n",
            "loss_aug_tea: 0.8947,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.8135,   \n",
            "loss_aug_tea: 0.9906,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 1.1689,   \n",
            "loss_aug_tea: 0.6428,   \n",
            "loss_cls: 0.0751,   \n",
            "loss_aug: 0.0756,   \n",
            "loss_ori_tea: 1.0409,   \n",
            "loss_aug_tea: 0.8754,   \n",
            "loss_cls: 0.0720,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 1.0597,   \n",
            "loss_aug_tea: 0.7699,   \n",
            "loss_cls: 0.0740,   \n",
            "loss_aug: 0.0743,   \n",
            "loss_ori_tea: 1.5030,   \n",
            "loss_aug_tea: 1.3681,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.9261,   \n",
            "loss_aug_tea: 0.8517,   \n",
            "loss_cls: 0.0778,   \n",
            "loss_aug: 0.0781,   \n",
            "loss_ori_tea: 0.8355,   \n",
            "loss_aug_tea: 0.8322,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.8268,   \n",
            "loss_aug_tea: 0.8924,   \n",
            "loss_cls: 0.0830,   \n",
            "loss_aug: 0.0831,   \n",
            "loss_ori_tea: 1.0728,   \n",
            "loss_aug_tea: 1.0609,   \n",
            "loss_cls: 0.0708,   \n",
            "loss_aug: 0.0711,   \n",
            "loss_ori_tea: 1.0661,   \n",
            "loss_aug_tea: 1.0207,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.9154,   \n",
            "loss_aug_tea: 1.2050,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.4356,   \n",
            "loss_aug_tea: 0.7794,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0745,   \n",
            "loss_ori_tea: 1.0050,   \n",
            "loss_aug_tea: 0.9659,   \n",
            "loss_cls: 0.0610,   \n",
            "loss_aug: 0.0612,   \n",
            "loss_ori_tea: 0.8626,   \n",
            "loss_aug_tea: 0.9009,   \n",
            "loss_cls: 0.0772,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.9225,   \n",
            "loss_aug_tea: 0.9270,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0783,   \n",
            "loss_ori_tea: 1.1683,   \n",
            "loss_aug_tea: 1.2403,   \n",
            "loss_cls: 0.0646,   \n",
            "loss_aug: 0.0648,   \n",
            "loss_ori_tea: 1.0660,   \n",
            "loss_aug_tea: 0.9646,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0690,   \n",
            "loss_ori_tea: 0.7919,   \n",
            "loss_aug_tea: 1.0237,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.7733,   \n",
            "loss_aug_tea: 1.0362,   \n",
            "loss_cls: 0.0858,   \n",
            "loss_aug: 0.0861,   \n",
            "loss_ori_tea: 1.2178,   \n",
            "loss_aug_tea: 0.7570,   \n",
            "loss_cls: 0.0715,   \n",
            "loss_aug: 0.0716,   \n",
            "loss_ori_tea: 1.3469,   \n",
            "loss_aug_tea: 1.1140,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0792,   \n",
            "loss_ori_tea: 0.8554,   \n",
            "loss_aug_tea: 0.9132,   \n",
            "loss_cls: 0.0833,   \n",
            "loss_aug: 0.0836,   \n",
            "loss_ori_tea: 1.0467,   \n",
            "loss_aug_tea: 0.9844,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.7324,   \n",
            "loss_aug_tea: 1.0812,   \n",
            "loss_cls: 0.0609,   \n",
            "loss_aug: 0.0613,   \n",
            "loss_ori_tea: 0.9604,   \n",
            "loss_aug_tea: 0.9752,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0654,   \n",
            "loss_ori_tea: 1.0363,   \n",
            "loss_aug_tea: 1.0113,   \n",
            "loss_cls: 0.0683,   \n",
            "loss_aug: 0.0685,   \n",
            "loss_ori_tea: 1.0015,   \n",
            "loss_aug_tea: 0.9853,   \n",
            "loss_cls: 0.0586,   \n",
            "loss_aug: 0.0589,   \n",
            "loss_ori_tea: 0.8180,   \n",
            "loss_aug_tea: 0.8591,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 1.0027,   \n",
            "loss_aug_tea: 1.0607,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.9582,   \n",
            "loss_aug_tea: 0.9948,   \n",
            "loss_cls: 0.0754,   \n",
            "loss_aug: 0.0756,   \n",
            "loss_ori_tea: 0.8860,   \n",
            "loss_aug_tea: 0.8183,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 0.8760,   \n",
            "loss_aug_tea: 0.8811,   \n",
            "loss_cls: 0.0835,   \n",
            "loss_aug: 0.0840,   \n",
            "loss_ori_tea: 1.1454,   \n",
            "loss_aug_tea: 1.1595,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0670,   \n",
            "loss_ori_tea: 0.9541,   \n",
            "loss_aug_tea: 0.9181,   \n",
            "loss_cls: 0.0859,   \n",
            "loss_aug: 0.0862,   \n",
            "loss_ori_tea: 1.0722,   \n",
            "loss_aug_tea: 0.8418,   \n",
            "loss_cls: 0.0830,   \n",
            "loss_aug: 0.0833,   \n",
            "loss_ori_tea: 1.1159,   \n",
            "loss_aug_tea: 1.1201,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.9706,   \n",
            "loss_aug_tea: 0.9533,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 0.9436,   \n",
            "loss_aug_tea: 0.9404,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.8216,   \n",
            "loss_aug_tea: 1.0181,   \n",
            "Epoch [11/15],   Mean Train Loss: 3.9921,   Mean Train Dice: 0.9264,   Mean Val Dice: 0.9630,   Mean Val ASD: 0.0026,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0637,   \n",
            "loss_ori_tea: 1.3524,   \n",
            "loss_aug_tea: 1.0667,   \n",
            "loss_cls: 0.0751,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 1.0839,   \n",
            "loss_aug_tea: 1.0382,   \n",
            "loss_cls: 0.0774,   \n",
            "loss_aug: 0.0778,   \n",
            "loss_ori_tea: 1.0090,   \n",
            "loss_aug_tea: 1.0419,   \n",
            "loss_cls: 0.0723,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.8444,   \n",
            "loss_aug_tea: 0.9067,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 1.0605,   \n",
            "loss_aug_tea: 0.8491,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0703,   \n",
            "loss_ori_tea: 0.8710,   \n",
            "loss_aug_tea: 0.8767,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0699,   \n",
            "loss_ori_tea: 1.1142,   \n",
            "loss_aug_tea: 0.8106,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0635,   \n",
            "loss_ori_tea: 1.2009,   \n",
            "loss_aug_tea: 0.8518,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0679,   \n",
            "loss_ori_tea: 0.9102,   \n",
            "loss_aug_tea: 1.0399,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 0.7811,   \n",
            "loss_aug_tea: 0.9867,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0751,   \n",
            "loss_ori_tea: 1.1475,   \n",
            "loss_aug_tea: 0.7174,   \n",
            "loss_cls: 0.0551,   \n",
            "loss_aug: 0.0556,   \n",
            "loss_ori_tea: 0.8359,   \n",
            "loss_aug_tea: 0.9479,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.8883,   \n",
            "loss_aug_tea: 0.9466,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.8409,   \n",
            "loss_aug_tea: 1.3057,   \n",
            "loss_cls: 0.0905,   \n",
            "loss_aug: 0.0907,   \n",
            "loss_ori_tea: 1.0194,   \n",
            "loss_aug_tea: 1.0960,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0703,   \n",
            "loss_ori_tea: 0.8576,   \n",
            "loss_aug_tea: 0.9390,   \n",
            "loss_cls: 0.0753,   \n",
            "loss_aug: 0.0755,   \n",
            "loss_ori_tea: 0.9552,   \n",
            "loss_aug_tea: 0.9483,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.7520,   \n",
            "loss_aug_tea: 0.9981,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0753,   \n",
            "loss_ori_tea: 0.9725,   \n",
            "loss_aug_tea: 0.9645,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.9264,   \n",
            "loss_aug_tea: 1.0209,   \n",
            "loss_cls: 0.0790,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 1.1568,   \n",
            "loss_aug_tea: 0.9112,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 0.7786,   \n",
            "loss_aug_tea: 1.3423,   \n",
            "loss_cls: 0.0810,   \n",
            "loss_aug: 0.0811,   \n",
            "loss_ori_tea: 1.3943,   \n",
            "loss_aug_tea: 1.3525,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.9935,   \n",
            "loss_aug_tea: 1.0057,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.1200,   \n",
            "loss_aug_tea: 1.0914,   \n",
            "loss_cls: 0.0651,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 0.8399,   \n",
            "loss_aug_tea: 1.0666,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 0.9547,   \n",
            "loss_aug_tea: 0.9952,   \n",
            "loss_cls: 0.0805,   \n",
            "loss_aug: 0.0810,   \n",
            "loss_ori_tea: 0.9842,   \n",
            "loss_aug_tea: 0.9663,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.8713,   \n",
            "loss_aug_tea: 0.9448,   \n",
            "loss_cls: 0.0725,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.9953,   \n",
            "loss_aug_tea: 0.8233,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0753,   \n",
            "loss_ori_tea: 0.9511,   \n",
            "loss_aug_tea: 1.1047,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 1.1768,   \n",
            "loss_aug_tea: 0.8854,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0663,   \n",
            "loss_ori_tea: 1.1356,   \n",
            "loss_aug_tea: 0.9177,   \n",
            "loss_cls: 0.0885,   \n",
            "loss_aug: 0.0888,   \n",
            "loss_ori_tea: 0.8633,   \n",
            "loss_aug_tea: 1.0060,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 0.9470,   \n",
            "loss_aug_tea: 0.9628,   \n",
            "loss_cls: 0.0694,   \n",
            "loss_aug: 0.0697,   \n",
            "loss_ori_tea: 1.2178,   \n",
            "loss_aug_tea: 1.1291,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0724,   \n",
            "loss_ori_tea: 0.8196,   \n",
            "loss_aug_tea: 0.9738,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.9210,   \n",
            "loss_aug_tea: 1.0392,   \n",
            "loss_cls: 0.0594,   \n",
            "loss_aug: 0.0597,   \n",
            "loss_ori_tea: 1.3447,   \n",
            "loss_aug_tea: 0.8896,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.8680,   \n",
            "loss_aug_tea: 1.1061,   \n",
            "loss_cls: 0.0555,   \n",
            "loss_aug: 0.0557,   \n",
            "loss_ori_tea: 0.8271,   \n",
            "loss_aug_tea: 1.1698,   \n",
            "loss_cls: 0.0570,   \n",
            "loss_aug: 0.0573,   \n",
            "loss_ori_tea: 0.8224,   \n",
            "loss_aug_tea: 0.8438,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 1.3603,   \n",
            "loss_aug_tea: 0.9867,   \n",
            "loss_cls: 0.0811,   \n",
            "loss_aug: 0.0815,   \n",
            "loss_ori_tea: 1.1177,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 1.0462,   \n",
            "loss_aug_tea: 0.9370,   \n",
            "loss_cls: 0.0655,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 1.1442,   \n",
            "loss_aug_tea: 1.0818,   \n",
            "loss_cls: 0.0870,   \n",
            "loss_aug: 0.0871,   \n",
            "loss_ori_tea: 1.1807,   \n",
            "loss_aug_tea: 0.8244,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0638,   \n",
            "loss_ori_tea: 0.9087,   \n",
            "loss_aug_tea: 0.7401,   \n",
            "loss_cls: 0.0823,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 1.0939,   \n",
            "loss_aug_tea: 0.9090,   \n",
            "loss_cls: 0.0866,   \n",
            "loss_aug: 0.0867,   \n",
            "loss_ori_tea: 0.9845,   \n",
            "loss_aug_tea: 0.8619,   \n",
            "loss_cls: 0.0666,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 1.1271,   \n",
            "loss_aug_tea: 0.9570,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0601,   \n",
            "loss_ori_tea: 0.8097,   \n",
            "loss_aug_tea: 0.9491,   \n",
            "loss_cls: 0.0763,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 0.8346,   \n",
            "loss_aug_tea: 1.0478,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.9238,   \n",
            "loss_aug_tea: 0.8911,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 1.1673,   \n",
            "loss_aug_tea: 0.7785,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0653,   \n",
            "loss_ori_tea: 1.2087,   \n",
            "loss_aug_tea: 0.8226,   \n",
            "loss_cls: 0.0887,   \n",
            "loss_aug: 0.0890,   \n",
            "loss_ori_tea: 0.9983,   \n",
            "loss_aug_tea: 0.8630,   \n",
            "loss_cls: 0.0813,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 1.1845,   \n",
            "loss_aug_tea: 0.9976,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 0.7926,   \n",
            "loss_aug_tea: 1.0516,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0670,   \n",
            "loss_ori_tea: 1.0150,   \n",
            "loss_aug_tea: 1.0546,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 1.0916,   \n",
            "loss_aug_tea: 0.8631,   \n",
            "loss_cls: 0.0729,   \n",
            "loss_aug: 0.0732,   \n",
            "loss_ori_tea: 0.9162,   \n",
            "loss_aug_tea: 0.9716,   \n",
            "loss_cls: 0.0801,   \n",
            "loss_aug: 0.0804,   \n",
            "loss_ori_tea: 1.1489,   \n",
            "loss_aug_tea: 0.9835,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 0.9029,   \n",
            "loss_aug_tea: 1.0081,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 0.9393,   \n",
            "loss_aug_tea: 0.8854,   \n",
            "loss_cls: 0.0640,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 1.0316,   \n",
            "loss_aug_tea: 0.9208,   \n",
            "Epoch [12/15],   Mean Train Loss: 4.0290,   Mean Train Dice: 0.9284,   Mean Val Dice: 0.9623,   Mean Val ASD: 0.0020,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.9066,   \n",
            "loss_aug_tea: 1.0840,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0635,   \n",
            "loss_ori_tea: 1.1813,   \n",
            "loss_aug_tea: 0.9089,   \n",
            "loss_cls: 0.0782,   \n",
            "loss_aug: 0.0788,   \n",
            "loss_ori_tea: 0.8365,   \n",
            "loss_aug_tea: 1.0047,   \n",
            "loss_cls: 0.0589,   \n",
            "loss_aug: 0.0593,   \n",
            "loss_ori_tea: 0.8464,   \n",
            "loss_aug_tea: 1.0466,   \n",
            "loss_cls: 0.0803,   \n",
            "loss_aug: 0.0805,   \n",
            "loss_ori_tea: 1.1570,   \n",
            "loss_aug_tea: 1.0814,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.8706,   \n",
            "loss_aug_tea: 1.0296,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0790,   \n",
            "loss_ori_tea: 0.7397,   \n",
            "loss_aug_tea: 1.0038,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 0.8647,   \n",
            "loss_aug_tea: 0.9230,   \n",
            "loss_cls: 0.0826,   \n",
            "loss_aug: 0.0828,   \n",
            "loss_ori_tea: 0.9043,   \n",
            "loss_aug_tea: 0.8989,   \n",
            "loss_cls: 0.0594,   \n",
            "loss_aug: 0.0597,   \n",
            "loss_ori_tea: 0.9442,   \n",
            "loss_aug_tea: 0.8787,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0700,   \n",
            "loss_ori_tea: 1.0463,   \n",
            "loss_aug_tea: 1.0017,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 1.0757,   \n",
            "loss_aug_tea: 0.9844,   \n",
            "loss_cls: 0.0782,   \n",
            "loss_aug: 0.0785,   \n",
            "loss_ori_tea: 1.4741,   \n",
            "loss_aug_tea: 0.9820,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0629,   \n",
            "loss_ori_tea: 1.0847,   \n",
            "loss_aug_tea: 1.0272,   \n",
            "loss_cls: 0.0585,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.9254,   \n",
            "loss_aug_tea: 0.9548,   \n",
            "loss_cls: 0.0779,   \n",
            "loss_aug: 0.0783,   \n",
            "loss_ori_tea: 1.0359,   \n",
            "loss_aug_tea: 1.0315,   \n",
            "loss_cls: 0.0848,   \n",
            "loss_aug: 0.0852,   \n",
            "loss_ori_tea: 0.8804,   \n",
            "loss_aug_tea: 1.0006,   \n",
            "loss_cls: 0.0733,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.9633,   \n",
            "loss_aug_tea: 0.9547,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 0.8680,   \n",
            "loss_aug_tea: 0.9968,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 1.2046,   \n",
            "loss_aug_tea: 0.9170,   \n",
            "loss_cls: 0.0847,   \n",
            "loss_aug: 0.0851,   \n",
            "loss_ori_tea: 0.7905,   \n",
            "loss_aug_tea: 1.1611,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0846,   \n",
            "loss_ori_tea: 0.7707,   \n",
            "loss_aug_tea: 1.0484,   \n",
            "loss_cls: 0.0731,   \n",
            "loss_aug: 0.0732,   \n",
            "loss_ori_tea: 1.4248,   \n",
            "loss_aug_tea: 1.0490,   \n",
            "loss_cls: 0.0590,   \n",
            "loss_aug: 0.0592,   \n",
            "loss_ori_tea: 0.7416,   \n",
            "loss_aug_tea: 0.9919,   \n",
            "loss_cls: 0.0685,   \n",
            "loss_aug: 0.0686,   \n",
            "loss_ori_tea: 1.0082,   \n",
            "loss_aug_tea: 0.9912,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0634,   \n",
            "loss_ori_tea: 0.9682,   \n",
            "loss_aug_tea: 1.0416,   \n",
            "loss_cls: 0.0816,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 1.0330,   \n",
            "loss_aug_tea: 0.9049,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.9183,   \n",
            "loss_aug_tea: 0.9914,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 1.0227,   \n",
            "loss_aug_tea: 1.1436,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 1.0554,   \n",
            "loss_aug_tea: 0.9334,   \n",
            "loss_cls: 0.0758,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 1.0773,   \n",
            "loss_aug_tea: 0.9892,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 0.8710,   \n",
            "loss_aug_tea: 1.0056,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 1.0741,   \n",
            "loss_aug_tea: 0.9215,   \n",
            "loss_cls: 0.0828,   \n",
            "loss_aug: 0.0829,   \n",
            "loss_ori_tea: 1.5017,   \n",
            "loss_aug_tea: 0.7993,   \n",
            "loss_cls: 0.0652,   \n",
            "loss_aug: 0.0653,   \n",
            "loss_ori_tea: 0.8311,   \n",
            "loss_aug_tea: 1.0866,   \n",
            "loss_cls: 0.0908,   \n",
            "loss_aug: 0.0912,   \n",
            "loss_ori_tea: 1.0073,   \n",
            "loss_aug_tea: 0.9651,   \n",
            "loss_cls: 0.0740,   \n",
            "loss_aug: 0.0741,   \n",
            "loss_ori_tea: 0.8356,   \n",
            "loss_aug_tea: 0.9303,   \n",
            "loss_cls: 0.0785,   \n",
            "loss_aug: 0.0790,   \n",
            "loss_ori_tea: 0.9536,   \n",
            "loss_aug_tea: 0.9097,   \n",
            "loss_cls: 0.0732,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.9295,   \n",
            "loss_aug_tea: 1.0291,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.9917,   \n",
            "loss_aug_tea: 0.8870,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 1.0250,   \n",
            "loss_aug_tea: 0.9210,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 0.8947,   \n",
            "loss_aug_tea: 0.8782,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.9461,   \n",
            "loss_aug_tea: 1.0010,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 0.8306,   \n",
            "loss_aug_tea: 1.0520,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.9815,   \n",
            "loss_aug_tea: 0.9403,   \n",
            "loss_cls: 0.0619,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 0.9733,   \n",
            "loss_aug_tea: 1.0173,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 1.0542,   \n",
            "loss_aug_tea: 1.1159,   \n",
            "loss_cls: 0.0717,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 1.0187,   \n",
            "loss_aug_tea: 1.3984,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0801,   \n",
            "loss_ori_tea: 1.3564,   \n",
            "loss_aug_tea: 1.2500,   \n",
            "loss_cls: 0.0640,   \n",
            "loss_aug: 0.0642,   \n",
            "loss_ori_tea: 0.9224,   \n",
            "loss_aug_tea: 0.9053,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0685,   \n",
            "loss_ori_tea: 0.9887,   \n",
            "loss_aug_tea: 1.0503,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.1253,   \n",
            "loss_aug_tea: 1.0269,   \n",
            "loss_cls: 0.0796,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 1.1718,   \n",
            "loss_aug_tea: 0.8742,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.8797,   \n",
            "loss_aug_tea: 1.1108,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0638,   \n",
            "loss_ori_tea: 0.9705,   \n",
            "loss_aug_tea: 1.2099,   \n",
            "loss_cls: 0.0561,   \n",
            "loss_aug: 0.0562,   \n",
            "loss_ori_tea: 1.0148,   \n",
            "loss_aug_tea: 0.9842,   \n",
            "loss_cls: 0.0499,   \n",
            "loss_aug: 0.0504,   \n",
            "loss_ori_tea: 1.0619,   \n",
            "loss_aug_tea: 1.1488,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0761,   \n",
            "loss_ori_tea: 1.1458,   \n",
            "loss_aug_tea: 1.1382,   \n",
            "loss_cls: 0.0760,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 1.1433,   \n",
            "loss_aug_tea: 1.2013,   \n",
            "loss_cls: 0.0786,   \n",
            "loss_aug: 0.0788,   \n",
            "loss_ori_tea: 0.9889,   \n",
            "loss_aug_tea: 1.0181,   \n",
            "loss_cls: 0.0752,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.9555,   \n",
            "loss_aug_tea: 0.9084,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 1.0976,   \n",
            "loss_aug_tea: 1.0297,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 1.3667,   \n",
            "loss_aug_tea: 1.2665,   \n",
            "loss_cls: 0.0700,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.9494,   \n",
            "loss_aug_tea: 0.8540,   \n",
            "loss_cls: 0.0771,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.9964,   \n",
            "loss_aug_tea: 0.9763,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 1.2656,   \n",
            "loss_aug_tea: 0.9248,   \n",
            "Epoch [13/15],   Mean Train Loss: 4.1156,   Mean Train Dice: 0.9278,   Mean Val Dice: 0.9627,   Mean Val ASD: 0.0020,   \n",
            "loss_cls: 0.0619,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 1.1713,   \n",
            "loss_aug_tea: 1.1202,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0746,   \n",
            "loss_ori_tea: 0.9867,   \n",
            "loss_aug_tea: 0.9790,   \n",
            "loss_cls: 0.0774,   \n",
            "loss_aug: 0.0776,   \n",
            "loss_ori_tea: 0.7615,   \n",
            "loss_aug_tea: 1.2616,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0745,   \n",
            "loss_ori_tea: 0.9416,   \n",
            "loss_aug_tea: 0.9989,   \n",
            "loss_cls: 0.0685,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.9020,   \n",
            "loss_aug_tea: 0.9946,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0589,   \n",
            "loss_ori_tea: 0.9777,   \n",
            "loss_aug_tea: 0.9294,   \n",
            "loss_cls: 0.0603,   \n",
            "loss_aug: 0.0606,   \n",
            "loss_ori_tea: 0.9065,   \n",
            "loss_aug_tea: 1.1445,   \n",
            "loss_cls: 0.0899,   \n",
            "loss_aug: 0.0902,   \n",
            "loss_ori_tea: 1.1078,   \n",
            "loss_aug_tea: 0.7922,   \n",
            "loss_cls: 0.0654,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 1.0413,   \n",
            "loss_aug_tea: 1.0294,   \n",
            "loss_cls: 0.0602,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 1.1480,   \n",
            "loss_aug_tea: 1.3759,   \n",
            "loss_cls: 0.0764,   \n",
            "loss_aug: 0.0766,   \n",
            "loss_ori_tea: 0.9273,   \n",
            "loss_aug_tea: 0.9799,   \n",
            "loss_cls: 0.0805,   \n",
            "loss_aug: 0.0807,   \n",
            "loss_ori_tea: 0.8808,   \n",
            "loss_aug_tea: 0.9488,   \n",
            "loss_cls: 0.0730,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8654,   \n",
            "loss_aug_tea: 1.1089,   \n",
            "loss_cls: 0.0813,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 0.8062,   \n",
            "loss_aug_tea: 1.0778,   \n",
            "loss_cls: 0.0823,   \n",
            "loss_aug: 0.0826,   \n",
            "loss_ori_tea: 1.0210,   \n",
            "loss_aug_tea: 0.8634,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 0.8517,   \n",
            "loss_aug_tea: 0.9175,   \n",
            "loss_cls: 0.0705,   \n",
            "loss_aug: 0.0707,   \n",
            "loss_ori_tea: 0.8124,   \n",
            "loss_aug_tea: 0.9822,   \n",
            "loss_cls: 0.0597,   \n",
            "loss_aug: 0.0600,   \n",
            "loss_ori_tea: 0.9408,   \n",
            "loss_aug_tea: 0.9600,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0642,   \n",
            "loss_ori_tea: 1.0497,   \n",
            "loss_aug_tea: 1.0076,   \n",
            "loss_cls: 0.0493,   \n",
            "loss_aug: 0.0495,   \n",
            "loss_ori_tea: 0.8872,   \n",
            "loss_aug_tea: 1.1587,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0750,   \n",
            "loss_ori_tea: 0.8913,   \n",
            "loss_aug_tea: 1.1151,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0706,   \n",
            "loss_ori_tea: 1.0472,   \n",
            "loss_aug_tea: 0.9606,   \n",
            "loss_cls: 0.0925,   \n",
            "loss_aug: 0.0929,   \n",
            "loss_ori_tea: 0.8788,   \n",
            "loss_aug_tea: 0.9541,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 1.0125,   \n",
            "loss_aug_tea: 0.9239,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0633,   \n",
            "loss_ori_tea: 0.9584,   \n",
            "loss_aug_tea: 0.7658,   \n",
            "loss_cls: 0.0776,   \n",
            "loss_aug: 0.0779,   \n",
            "loss_ori_tea: 0.9053,   \n",
            "loss_aug_tea: 0.9562,   \n",
            "loss_cls: 0.0579,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.7689,   \n",
            "loss_aug_tea: 1.1792,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 1.1388,   \n",
            "loss_aug_tea: 1.2013,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 1.4601,   \n",
            "loss_aug_tea: 0.7778,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 0.8615,   \n",
            "loss_aug_tea: 1.0689,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0685,   \n",
            "loss_ori_tea: 0.9968,   \n",
            "loss_aug_tea: 1.0036,   \n",
            "loss_cls: 0.0778,   \n",
            "loss_aug: 0.0779,   \n",
            "loss_ori_tea: 0.8698,   \n",
            "loss_aug_tea: 0.8543,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0732,   \n",
            "loss_ori_tea: 1.1592,   \n",
            "loss_aug_tea: 0.9035,   \n",
            "loss_cls: 0.0777,   \n",
            "loss_aug: 0.0781,   \n",
            "loss_ori_tea: 0.9358,   \n",
            "loss_aug_tea: 1.0898,   \n",
            "loss_cls: 0.0845,   \n",
            "loss_aug: 0.0847,   \n",
            "loss_ori_tea: 1.0010,   \n",
            "loss_aug_tea: 1.1357,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0809,   \n",
            "loss_ori_tea: 1.0870,   \n",
            "loss_aug_tea: 0.8998,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.9935,   \n",
            "loss_aug_tea: 1.0924,   \n",
            "loss_cls: 0.0929,   \n",
            "loss_aug: 0.0933,   \n",
            "loss_ori_tea: 0.8427,   \n",
            "loss_aug_tea: 1.0280,   \n",
            "loss_cls: 0.0751,   \n",
            "loss_aug: 0.0753,   \n",
            "loss_ori_tea: 1.1630,   \n",
            "loss_aug_tea: 1.1287,   \n",
            "loss_cls: 0.0683,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.8829,   \n",
            "loss_aug_tea: 1.0678,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0590,   \n",
            "loss_ori_tea: 0.9700,   \n",
            "loss_aug_tea: 1.3899,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0712,   \n",
            "loss_ori_tea: 1.5047,   \n",
            "loss_aug_tea: 0.9121,   \n",
            "loss_cls: 0.0766,   \n",
            "loss_aug: 0.0768,   \n",
            "loss_ori_tea: 0.8448,   \n",
            "loss_aug_tea: 0.8987,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.9832,   \n",
            "loss_aug_tea: 0.8720,   \n",
            "loss_cls: 0.0575,   \n",
            "loss_aug: 0.0577,   \n",
            "loss_ori_tea: 1.1546,   \n",
            "loss_aug_tea: 1.0530,   \n",
            "loss_cls: 0.0652,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 0.8236,   \n",
            "loss_aug_tea: 1.0324,   \n",
            "loss_cls: 0.0874,   \n",
            "loss_aug: 0.0879,   \n",
            "loss_ori_tea: 0.9999,   \n",
            "loss_aug_tea: 0.9081,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0688,   \n",
            "loss_ori_tea: 0.9974,   \n",
            "loss_aug_tea: 0.8371,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 0.9851,   \n",
            "loss_aug_tea: 0.9380,   \n",
            "loss_cls: 0.0685,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.9270,   \n",
            "loss_aug_tea: 0.9674,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 0.9181,   \n",
            "loss_aug_tea: 0.9724,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.9864,   \n",
            "loss_aug_tea: 0.9937,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 1.3231,   \n",
            "loss_aug_tea: 1.4280,   \n",
            "loss_cls: 0.0718,   \n",
            "loss_aug: 0.0720,   \n",
            "loss_ori_tea: 1.4543,   \n",
            "loss_aug_tea: 1.1717,   \n",
            "loss_cls: 0.0838,   \n",
            "loss_aug: 0.0841,   \n",
            "loss_ori_tea: 1.2686,   \n",
            "loss_aug_tea: 1.3450,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 1.1188,   \n",
            "loss_aug_tea: 1.0988,   \n",
            "loss_cls: 0.0568,   \n",
            "loss_aug: 0.0571,   \n",
            "loss_ori_tea: 1.0113,   \n",
            "loss_aug_tea: 1.1068,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 1.2680,   \n",
            "loss_aug_tea: 1.2871,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 0.9259,   \n",
            "loss_aug_tea: 1.1489,   \n",
            "loss_cls: 0.0893,   \n",
            "loss_aug: 0.0897,   \n",
            "loss_ori_tea: 1.0909,   \n",
            "loss_aug_tea: 1.2337,   \n",
            "loss_cls: 0.0801,   \n",
            "loss_aug: 0.0806,   \n",
            "loss_ori_tea: 0.9706,   \n",
            "loss_aug_tea: 1.0451,   \n",
            "loss_cls: 0.0617,   \n",
            "loss_aug: 0.0623,   \n",
            "loss_ori_tea: 0.9161,   \n",
            "loss_aug_tea: 1.3133,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0641,   \n",
            "loss_ori_tea: 1.0853,   \n",
            "loss_aug_tea: 1.1337,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 1.2034,   \n",
            "loss_aug_tea: 0.9009,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 1.1006,   \n",
            "loss_aug_tea: 1.6098,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8165,   \n",
            "loss_aug_tea: 1.0364,   \n",
            "Epoch [14/15],   Mean Train Loss: 4.1893,   Mean Train Dice: 0.9277,   Mean Val Dice: 0.9625,   Mean Val ASD: 0.0019,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 1.1062,   \n",
            "loss_aug_tea: 1.0240,   \n",
            "loss_cls: 0.0744,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.9857,   \n",
            "loss_aug_tea: 0.8804,   \n",
            "loss_cls: 0.0718,   \n",
            "loss_aug: 0.0721,   \n",
            "loss_ori_tea: 1.0281,   \n",
            "loss_aug_tea: 1.0355,   \n",
            "loss_cls: 0.0568,   \n",
            "loss_aug: 0.0571,   \n",
            "loss_ori_tea: 1.3947,   \n",
            "loss_aug_tea: 1.0073,   \n",
            "loss_cls: 0.0874,   \n",
            "loss_aug: 0.0881,   \n",
            "loss_ori_tea: 0.9368,   \n",
            "loss_aug_tea: 1.0553,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0729,   \n",
            "loss_ori_tea: 1.4780,   \n",
            "loss_aug_tea: 0.8876,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 1.0371,   \n",
            "loss_aug_tea: 0.8190,   \n",
            "loss_cls: 0.0968,   \n",
            "loss_aug: 0.0971,   \n",
            "loss_ori_tea: 0.9621,   \n",
            "loss_aug_tea: 0.8740,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 1.1725,   \n",
            "loss_aug_tea: 1.0565,   \n",
            "loss_cls: 0.0564,   \n",
            "loss_aug: 0.0566,   \n",
            "loss_ori_tea: 1.1256,   \n",
            "loss_aug_tea: 0.9384,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.9173,   \n",
            "loss_aug_tea: 1.0738,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.9110,   \n",
            "loss_aug_tea: 1.2311,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0794,   \n",
            "loss_ori_tea: 1.3519,   \n",
            "loss_aug_tea: 0.9898,   \n",
            "loss_cls: 0.0872,   \n",
            "loss_aug: 0.0876,   \n",
            "loss_ori_tea: 0.9726,   \n",
            "loss_aug_tea: 1.0408,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0825,   \n",
            "loss_ori_tea: 0.8891,   \n",
            "loss_aug_tea: 0.9989,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.8428,   \n",
            "loss_aug_tea: 1.1003,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 1.0226,   \n",
            "loss_aug_tea: 1.0197,   \n",
            "loss_cls: 0.0597,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 1.4233,   \n",
            "loss_aug_tea: 0.6649,   \n",
            "loss_cls: 0.0683,   \n",
            "loss_aug: 0.0686,   \n",
            "loss_ori_tea: 0.9650,   \n",
            "loss_aug_tea: 1.0078,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 1.2255,   \n",
            "loss_aug_tea: 1.1349,   \n",
            "loss_cls: 0.0611,   \n",
            "loss_aug: 0.0613,   \n",
            "loss_ori_tea: 0.8619,   \n",
            "loss_aug_tea: 1.0191,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0786,   \n",
            "loss_ori_tea: 1.0787,   \n",
            "loss_aug_tea: 1.1576,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 0.7696,   \n",
            "loss_aug_tea: 1.0777,   \n",
            "loss_cls: 0.0633,   \n",
            "loss_aug: 0.0635,   \n",
            "loss_ori_tea: 0.8814,   \n",
            "loss_aug_tea: 0.8926,   \n",
            "loss_cls: 0.0830,   \n",
            "loss_aug: 0.0831,   \n",
            "loss_ori_tea: 1.1012,   \n",
            "loss_aug_tea: 0.8676,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 1.0054,   \n",
            "loss_aug_tea: 0.8893,   \n",
            "loss_cls: 0.0732,   \n",
            "loss_aug: 0.0733,   \n",
            "loss_ori_tea: 1.0538,   \n",
            "loss_aug_tea: 1.0587,   \n",
            "loss_cls: 0.0669,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.9235,   \n",
            "loss_aug_tea: 1.2713,   \n",
            "loss_cls: 0.0789,   \n",
            "loss_aug: 0.0792,   \n",
            "loss_ori_tea: 0.8372,   \n",
            "loss_aug_tea: 0.9918,   \n",
            "loss_cls: 0.0712,   \n",
            "loss_aug: 0.0715,   \n",
            "loss_ori_tea: 0.8720,   \n",
            "loss_aug_tea: 0.9763,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 0.8270,   \n",
            "loss_aug_tea: 1.1009,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.9418,   \n",
            "loss_aug_tea: 0.9327,   \n",
            "loss_cls: 0.0777,   \n",
            "loss_aug: 0.0779,   \n",
            "loss_ori_tea: 0.8937,   \n",
            "loss_aug_tea: 0.9915,   \n",
            "loss_cls: 0.0858,   \n",
            "loss_aug: 0.0860,   \n",
            "loss_ori_tea: 1.1496,   \n",
            "loss_aug_tea: 0.8444,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0782,   \n",
            "loss_ori_tea: 1.1508,   \n",
            "loss_aug_tea: 1.0179,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.8308,   \n",
            "loss_aug_tea: 0.8876,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.9641,   \n",
            "loss_aug_tea: 1.6186,   \n",
            "loss_cls: 0.0525,   \n",
            "loss_aug: 0.0527,   \n",
            "loss_ori_tea: 1.2778,   \n",
            "loss_aug_tea: 1.0550,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0713,   \n",
            "loss_ori_tea: 1.0435,   \n",
            "loss_aug_tea: 1.0481,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0715,   \n",
            "loss_ori_tea: 0.9490,   \n",
            "loss_aug_tea: 1.0036,   \n",
            "loss_cls: 0.0702,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.9562,   \n",
            "loss_aug_tea: 1.1021,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0624,   \n",
            "loss_ori_tea: 1.0987,   \n",
            "loss_aug_tea: 0.9786,   \n",
            "loss_cls: 0.0712,   \n",
            "loss_aug: 0.0716,   \n",
            "loss_ori_tea: 0.9438,   \n",
            "loss_aug_tea: 1.0427,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0633,   \n",
            "loss_ori_tea: 0.7323,   \n",
            "loss_aug_tea: 1.0247,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0750,   \n",
            "loss_ori_tea: 1.0170,   \n",
            "loss_aug_tea: 1.1440,   \n",
            "loss_cls: 0.0772,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 0.9799,   \n",
            "loss_aug_tea: 0.9647,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0695,   \n",
            "loss_ori_tea: 1.1016,   \n",
            "loss_aug_tea: 0.8098,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 0.9195,   \n",
            "loss_aug_tea: 0.9781,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 1.1012,   \n",
            "loss_aug_tea: 1.2771,   \n",
            "loss_cls: 0.0753,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.9122,   \n",
            "loss_aug_tea: 1.0239,   \n",
            "loss_cls: 0.0690,   \n",
            "loss_aug: 0.0695,   \n",
            "loss_ori_tea: 0.9285,   \n",
            "loss_aug_tea: 0.9779,   \n",
            "loss_cls: 0.0602,   \n",
            "loss_aug: 0.0603,   \n",
            "loss_ori_tea: 1.4669,   \n",
            "loss_aug_tea: 0.8945,   \n",
            "loss_cls: 0.0646,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 1.3334,   \n",
            "loss_aug_tea: 0.9800,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 0.9180,   \n",
            "loss_aug_tea: 1.1282,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.8382,   \n",
            "loss_aug_tea: 1.1884,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0691,   \n",
            "loss_ori_tea: 0.9587,   \n",
            "loss_aug_tea: 0.7822,   \n",
            "loss_cls: 0.0700,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.9110,   \n",
            "loss_aug_tea: 0.8630,   \n",
            "loss_cls: 0.0812,   \n",
            "loss_aug: 0.0814,   \n",
            "loss_ori_tea: 0.9033,   \n",
            "loss_aug_tea: 0.9000,   \n",
            "loss_cls: 0.0684,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.6988,   \n",
            "loss_aug_tea: 1.1524,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 1.1221,   \n",
            "loss_aug_tea: 0.9152,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0788,   \n",
            "loss_ori_tea: 1.0731,   \n",
            "loss_aug_tea: 0.9570,   \n",
            "loss_cls: 0.0782,   \n",
            "loss_aug: 0.0787,   \n",
            "loss_ori_tea: 1.3580,   \n",
            "loss_aug_tea: 1.1422,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.8016,   \n",
            "loss_aug_tea: 1.2176,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0634,   \n",
            "loss_ori_tea: 0.8931,   \n",
            "loss_aug_tea: 1.0114,   \n",
            "loss_cls: 0.0814,   \n",
            "loss_aug: 0.0816,   \n",
            "loss_ori_tea: 0.9682,   \n",
            "loss_aug_tea: 1.4791,   \n",
            "loss_cls: 0.0816,   \n",
            "loss_aug: 0.0820,   \n",
            "loss_ori_tea: 1.0712,   \n",
            "loss_aug_tea: 0.8778,   \n",
            "Epoch [15/15],   Mean Train Loss: 4.1481,   Mean Train Dice: 0.9283,   Mean Val Dice: 0.9621,   Mean Val ASD: 0.0019,   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for it, (batch) in enumerate(val_loader):\n",
        "    if it > 0 and it < 2:\n",
        "        data, labels = batch[0].to(device), batch[1].to(device)\n",
        "        outputs = mean_teacher_model.student_model(data)\n",
        "        outputs = F.softmax(outputs,dim=1)\n",
        "      \n",
        "        plt.figure()\n",
        "        plt.title(\"Prediction \"+str(it))\n",
        "        image = outputs.cpu().detach().numpy()[0].reshape(3,64,64)\n",
        "        imgplot1 = plt.imshow(image[0])\n",
        "        plt.show(imgplot1)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title(\"Prediction \"+str(it))\n",
        "        image = outputs.cpu().detach().numpy()[1].reshape(3,64,64)\n",
        "        imgplot1 = plt.imshow(image[0])\n",
        "        plt.show(imgplot1)\n",
        "        plt.figure()\n",
        "        plt.title(\"Prediction \"+str(it))\n",
        "        image = outputs.cpu().detach().numpy()[2].reshape(3,64,64)\n",
        "        imgplot1 = plt.imshow(image[0])\n",
        "        plt.show(imgplot1)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title(\"Label \"+str(it))\n",
        "        image = labels.cpu().detach().numpy()[0].reshape(3,64,64)\n",
        "        imgplot2 = plt.imshow(image[0])\n",
        "        plt.show(imgplot2)\n"
      ],
      "metadata": {
        "id": "Sppl_mn5pF3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}