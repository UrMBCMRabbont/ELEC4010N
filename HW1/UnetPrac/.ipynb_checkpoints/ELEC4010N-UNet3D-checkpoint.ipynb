{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1677565184742,
     "user": {
      "displayName": "BonKong Lai",
      "userId": "14934257860777894140"
     },
     "user_tz": -480
    },
    "id": "x2qBxrnPrTJT"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def conv3d(in_channels, out_channels, kernel_size, bias, padding):\n",
    "    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding):\n",
    "    \"\"\"\n",
    "    Create a list of modules with together constitute a single conv layer with non-linearity\n",
    "    and optional batchnorm/groupnorm.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size(int or tuple): size of the convolving kernel\n",
    "        order (string): order of things, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'gcr' -> groupnorm + conv + ReLU\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "            'bcr' -> batchnorm + conv + ReLU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "\n",
    "    Return:\n",
    "        list of tuple (name, module)\n",
    "    \"\"\"\n",
    "    assert 'c' in order, \"Conv layer MUST be present\"\n",
    "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
    "\n",
    "    modules = []\n",
    "    for i, char in enumerate(order):\n",
    "        if char == 'r':\n",
    "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
    "        elif char == 'l':\n",
    "            modules.append(('LeakyReLU', nn.LeakyReLU(inplace=True)))\n",
    "        elif char == 'e':\n",
    "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
    "        elif char == 'c':\n",
    "            # add learnable bias only in the absence of batchnorm/groupnorm\n",
    "            bias = not ('g' in order or 'b' in order)\n",
    "            modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
    "        elif char == 'g':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                num_channels = in_channels\n",
    "            else:\n",
    "                num_channels = out_channels\n",
    "\n",
    "            # use only one group if the given number of groups is greater than the number of channels\n",
    "            if num_channels < num_groups:\n",
    "                num_groups = 1\n",
    "\n",
    "            assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
    "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
    "        elif char == 'b':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
    "            else:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c']\")\n",
    "\n",
    "    return modules\n",
    "\n",
    "\n",
    "class SingleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
    "    of operations can be specified via the `order` parameter\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple):\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
    "        super(SingleConv, self).__init__()\n",
    "\n",
    "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n",
    "            self.add_module(name, module)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
    "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
    "    This can be changed however by providing the 'order' argument, e.g. in order\n",
    "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
    "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
    "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        if encoder:\n",
    "            # we're in the encoder path\n",
    "            conv1_in_channels = in_channels\n",
    "            conv1_out_channels = out_channels // 2\n",
    "            if conv1_out_channels < in_channels:\n",
    "                conv1_out_channels = in_channels\n",
    "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
    "        else:\n",
    "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
    "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
    "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
    "\n",
    "        # conv1\n",
    "        self.add_module('SingleConv1',\n",
    "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))\n",
    "        # conv2\n",
    "        self.add_module('SingleConv2',\n",
    "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))\n",
    "\n",
    "\n",
    "class ExtResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic UNet block consisting of a SingleConv followed by the residual block.\n",
    "    The SingleConv takes care of increasing/decreasing the number of channels and also ensures that the number\n",
    "    of output channels is compatible with the residual block that follows.\n",
    "    This block can be used instead of standard DoubleConv in the Encoder module.\n",
    "    Motivated by: https://arxiv.org/pdf/1706.00120.pdf\n",
    "\n",
    "    Notice we use ELU instead of ReLU (order='cge') and put non-linearity after the groupnorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cge', num_groups=8, **kwargs):\n",
    "        super(ExtResNetBlock, self).__init__()\n",
    "\n",
    "        # first convolution\n",
    "        self.conv1 = SingleConv(in_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
    "        # residual block\n",
    "        self.conv2 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
    "        # remove non-linearity from the 3rd convolution since it's going to be applied after adding the residual\n",
    "        n_order = order\n",
    "        for c in 'rel':\n",
    "            n_order = n_order.replace(c, '')\n",
    "        self.conv3 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=n_order,\n",
    "                                num_groups=num_groups)\n",
    "\n",
    "        # create non-linearity separately\n",
    "        if 'l' in order:\n",
    "            self.non_linearity = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        elif 'e' in order:\n",
    "            self.non_linearity = nn.ELU(inplace=True)\n",
    "        else:\n",
    "            self.non_linearity = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply first convolution and save the output as a residual\n",
    "        out = self.conv1(x)\n",
    "        residual = out\n",
    "\n",
    "        # residual block\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.non_linearity(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module from the encoder path consisting of the optional max\n",
    "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
    "    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
    "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
    "    a DoubleConv module.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        pool_type (str): pooling layer: 'max' or 'avg'\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
    "                 pool_kernel_size=2, pool_type='max', basic_module=DoubleConv, conv_layer_order='gcr',\n",
    "                 num_groups=8, padding=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        assert pool_type in ['max', 'avg']\n",
    "        if apply_pooling:\n",
    "            if pool_type == 'max':\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
    "            else:\n",
    "                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
    "        else:\n",
    "            self.pooling = None\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=True,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pooling is not None:\n",
    "            x = self.pooling(x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module for decoder path consisting of the upsampling layer\n",
    "    (either learned ConvTranspose3d or nearest neighbor interpolation) followed by a basic module (DoubleConv or ExtResNetBlock).\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        scale_factor (tuple): used as the multiplier for the image H/W/D in\n",
    "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
    "            from the corresponding encoder\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "        upsample (boole): should the input be upsampled\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=(2, 2, 2), basic_module=DoubleConv,\n",
    "                 conv_layer_order='gcr', num_groups=8, mode='nearest', padding=1, upsample=True):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        if upsample:\n",
    "            if basic_module == DoubleConv:\n",
    "                # if DoubleConv is the basic_module use interpolation for upsampling and concatenation joining\n",
    "                self.upsampling = InterpolateUpsampling(mode=mode)\n",
    "                # concat joining\n",
    "                self.joining = partial(self._joining, concat=True)\n",
    "            else:\n",
    "                # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n",
    "                self.upsampling = TransposeConvUpsampling(in_channels=in_channels, out_channels=out_channels,\n",
    "                                                          kernel_size=conv_kernel_size, scale_factor=scale_factor)\n",
    "                # sum joining\n",
    "                self.joining = partial(self._joining, concat=False)\n",
    "                # adapt the number of in_channels for the ExtResNetBlock\n",
    "                in_channels = out_channels\n",
    "        else:\n",
    "            # no upsampling\n",
    "            self.upsampling = NoUpsampling()\n",
    "            # concat joining\n",
    "            self.joining = partial(self._joining, concat=True)\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=False,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        x = self.upsampling(encoder_features=encoder_features, x=x)\n",
    "        x = self.joining(encoder_features, x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _joining(encoder_features, x, concat):\n",
    "        if concat:\n",
    "            return torch.cat((encoder_features, x), dim=1)\n",
    "        else:\n",
    "            return encoder_features + x\n",
    "\n",
    "\n",
    "def create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
    "                    pool_kernel_size):\n",
    "    # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n",
    "    encoders = []\n",
    "    for i, out_feature_num in enumerate(f_maps):\n",
    "        if i == 0:\n",
    "            encoder = Encoder(in_channels, out_feature_num,\n",
    "                              apply_pooling=False,  # skip pooling in the firs encoder\n",
    "                              basic_module=basic_module,\n",
    "                              conv_layer_order=layer_order,\n",
    "                              conv_kernel_size=conv_kernel_size,\n",
    "                              num_groups=num_groups,\n",
    "                              padding=conv_padding)\n",
    "        else:\n",
    "            # TODO: adapt for anisotropy in the data, i.e. use proper pooling kernel to make the data isotropic after 1-2 pooling operations\n",
    "            encoder = Encoder(f_maps[i - 1], out_feature_num,\n",
    "                              basic_module=basic_module,\n",
    "                              conv_layer_order=layer_order,\n",
    "                              conv_kernel_size=conv_kernel_size,\n",
    "                              num_groups=num_groups,\n",
    "                              pool_kernel_size=pool_kernel_size,\n",
    "                              padding=conv_padding)\n",
    "\n",
    "        encoders.append(encoder)\n",
    "\n",
    "    return nn.ModuleList(encoders)\n",
    "\n",
    "\n",
    "def create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups, upsample):\n",
    "    # create decoder path consisting of the Decoder modules. The length of the decoder list is equal to `len(f_maps) - 1`\n",
    "    decoders = []\n",
    "    reversed_f_maps = list(reversed(f_maps))\n",
    "    for i in range(len(reversed_f_maps) - 1):\n",
    "        if basic_module == DoubleConv:\n",
    "            in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
    "        else:\n",
    "            in_feature_num = reversed_f_maps[i]\n",
    "\n",
    "        out_feature_num = reversed_f_maps[i + 1]\n",
    "\n",
    "        # TODO: if non-standard pooling was used, make sure to use correct striding for transpose conv\n",
    "        # currently strides with a constant stride: (2, 2, 2)\n",
    "\n",
    "        _upsample = True\n",
    "        if i == 0:\n",
    "            # upsampling can be skipped only for the 1st decoder, afterwards it should always be present\n",
    "            _upsample = upsample\n",
    "\n",
    "        decoder = Decoder(in_feature_num, out_feature_num,\n",
    "                          basic_module=basic_module,\n",
    "                          conv_layer_order=layer_order,\n",
    "                          conv_kernel_size=conv_kernel_size,\n",
    "                          num_groups=num_groups,\n",
    "                          padding=conv_padding,\n",
    "                          upsample=_upsample)\n",
    "        decoders.append(decoder)\n",
    "    return nn.ModuleList(decoders)\n",
    "\n",
    "\n",
    "class AbstractUpsampling(nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract class for upsampling. A given implementation should upsample a given 5D input tensor using either\n",
    "    interpolation or learned transposed convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, upsample):\n",
    "        super(AbstractUpsampling, self).__init__()\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        # get the spatial dimensions of the output given the encoder_features\n",
    "        output_size = encoder_features.size()[2:]\n",
    "        # upsample the input and return\n",
    "        return self.upsample(x, output_size)\n",
    "\n",
    "\n",
    "class InterpolateUpsampling(AbstractUpsampling):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mode (str): algorithm used for upsampling:\n",
    "            'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. Default: 'nearest'\n",
    "            used only if transposed_conv is False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='nearest'):\n",
    "        upsample = partial(self._interpolate, mode=mode)\n",
    "        super().__init__(upsample)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interpolate(x, size, mode):\n",
    "        return F.interpolate(x, size=size, mode=mode)\n",
    "\n",
    "\n",
    "class TransposeConvUpsampling(AbstractUpsampling):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): number of input channels for transposed conv\n",
    "            used only if transposed_conv is True\n",
    "        out_channels (int): number of output channels for transpose conv\n",
    "            used only if transposed_conv is True\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "            used only if transposed_conv is True\n",
    "        scale_factor (int or tuple): stride of the convolution\n",
    "            used only if transposed_conv is True\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=None, out_channels=None, kernel_size=3, scale_factor=(2, 2, 2)):\n",
    "        # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
    "        upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=scale_factor,\n",
    "                                      padding=1)\n",
    "        super().__init__(upsample)\n",
    "\n",
    "\n",
    "class NoUpsampling(AbstractUpsampling):\n",
    "    def __init__(self):\n",
    "        super().__init__(self._no_upsampling)\n",
    "\n",
    "    @staticmethod\n",
    "    def _no_upsampling(x, size):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1677565365963,
     "user": {
      "displayName": "BonKong Lai",
      "userId": "14934257860777894140"
     },
     "user_tz": -480
    },
    "id": "hhl9CKUyr_2H"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_dir):\n",
    "    \"\"\"Saves model and training parameters at '{checkpoint_dir}/last_checkpoint.pytorch'.\n",
    "    If is_best==True saves '{checkpoint_dir}/best_checkpoint.pytorch' as well.\n",
    "\n",
    "    Args:\n",
    "        state (dict): contains model's state_dict, optimizer's state_dict, epoch\n",
    "            and best evaluation metric value so far\n",
    "        is_best (bool): if True state contains the best model seen so far\n",
    "        checkpoint_dir (string): directory where the checkpoint are to be saved\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    last_file_path = os.path.join(checkpoint_dir, 'last_checkpoint.pytorch')\n",
    "    torch.save(state, last_file_path)\n",
    "    if is_best:\n",
    "        best_file_path = os.path.join(checkpoint_dir, 'best_checkpoint.pytorch')\n",
    "        shutil.copyfile(last_file_path, best_file_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer=None,\n",
    "                    model_key='model_state_dict', optimizer_key='optimizer_state_dict'):\n",
    "    \"\"\"Loads model and training parameters from a given checkpoint_path\n",
    "    If optimizer is provided, loads optimizer's state_dict of as well.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (string): path to the checkpoint to be loaded\n",
    "        model (torch.nn.Module): model into which the parameters are to be copied\n",
    "        optimizer (torch.optim.Optimizer) optional: optimizer instance into\n",
    "            which the parameters are to be copied\n",
    "\n",
    "    Returns:\n",
    "        state\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise IOError(f\"Checkpoint '{checkpoint_path}' does not exist\")\n",
    "\n",
    "    state = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(state[model_key])\n",
    "\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state[optimizer_key])\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def save_network_output(output_path, output, logger=None):\n",
    "    if logger is not None:\n",
    "        logger.info(f'Saving network output to: {output_path}...')\n",
    "    output = output.detach().cpu()[0]\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        f.create_dataset('predictions', data=output, compression='gzip')\n",
    "\n",
    "\n",
    "loggers = {}\n",
    "\n",
    "\n",
    "def get_logger(name, level=logging.INFO):\n",
    "    global loggers\n",
    "    if loggers.get(name) is not None:\n",
    "        return loggers[name]\n",
    "    else:\n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(level)\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s [%(threadName)s] %(levelname)s %(name)s - %(message)s')\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "        loggers[name] = logger\n",
    "\n",
    "        return logger\n",
    "\n",
    "\n",
    "def get_number_of_learnable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class RunningAverage:\n",
    "    \"\"\"Computes and stores the average\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.count += n\n",
    "        self.sum += value * n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def find_maximum_patch_size(model, device):\n",
    "    \"\"\"Tries to find the biggest patch size that can be send to GPU for inference\n",
    "    without throwing CUDA out of memory\"\"\"\n",
    "    logger = get_logger('PatchFinder')\n",
    "    in_channels = model.in_channels\n",
    "\n",
    "    patch_shapes = [(64, 128, 128), (96, 128, 128),\n",
    "                    (64, 160, 160), (96, 160, 160),\n",
    "                    (64, 192, 192), (96, 192, 192)]\n",
    "\n",
    "    for shape in patch_shapes:\n",
    "        # generate random patch of a given size\n",
    "        patch = np.random.randn(*shape).astype('float32')\n",
    "\n",
    "        patch = torch \\\n",
    "            .from_numpy(patch) \\\n",
    "            .view((1, in_channels) + patch.shape) \\\n",
    "            .to(device)\n",
    "\n",
    "        logger.info(f\"Current patch size: {shape}\")\n",
    "        model(patch)\n",
    "\n",
    "\n",
    "def remove_halo(patch, index, shape, patch_halo):\n",
    "    \"\"\"\n",
    "    Remove `pad_width` voxels around the edges of a given patch.\n",
    "    \"\"\"\n",
    "    assert len(patch_halo) == 3\n",
    "\n",
    "    def _new_slices(slicing, max_size, pad):\n",
    "        if slicing.start == 0:\n",
    "            p_start = 0\n",
    "            i_start = 0\n",
    "        else:\n",
    "            p_start = pad\n",
    "            i_start = slicing.start + pad\n",
    "\n",
    "        if slicing.stop == max_size:\n",
    "            p_stop = None\n",
    "            i_stop = max_size\n",
    "        else:\n",
    "            p_stop = -pad if pad != 0 else 1\n",
    "            i_stop = slicing.stop - pad\n",
    "\n",
    "        return slice(p_start, p_stop), slice(i_start, i_stop)\n",
    "\n",
    "    D, H, W = shape\n",
    "\n",
    "    i_c, i_z, i_y, i_x = index\n",
    "    p_c = slice(0, patch.shape[0])\n",
    "\n",
    "    p_z, i_z = _new_slices(i_z, D, patch_halo[0])\n",
    "    p_y, i_y = _new_slices(i_y, H, patch_halo[1])\n",
    "    p_x, i_x = _new_slices(i_x, W, patch_halo[2])\n",
    "\n",
    "    patch_index = (p_c, p_z, p_y, p_x)\n",
    "    index = (i_c, i_z, i_y, i_x)\n",
    "    return patch[patch_index], index\n",
    "\n",
    "\n",
    "def number_of_features_per_level(init_channel_number, num_levels):\n",
    "    return [init_channel_number * 2 ** k for k in range(num_levels)]\n",
    "\n",
    "\n",
    "class _TensorboardFormatter:\n",
    "    \"\"\"\n",
    "    Tensorboard formatters converts a given batch of images (be it input/output to the network or the target segmentation\n",
    "    image) to a series of images that can be displayed in tensorboard. This is the parent class for all tensorboard\n",
    "    formatters which ensures that returned images are in the 'CHW' format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, name, batch):\n",
    "        \"\"\"\n",
    "        Transform a batch to a series of tuples of the form (tag, img), where `tag` corresponds to the image tag\n",
    "        and `img` is the image itself.\n",
    "\n",
    "        Args:\n",
    "             name (str): one of 'inputs'/'targets'/'predictions'\n",
    "             batch (torch.tensor): 4D or 5D torch tensor\n",
    "        \"\"\"\n",
    "\n",
    "        def _check_img(tag_img):\n",
    "            tag, img = tag_img\n",
    "\n",
    "            assert img.ndim == 2 or img.ndim == 3, 'Only 2D (HW) and 3D (CHW) images are accepted for display'\n",
    "\n",
    "            if img.ndim == 2:\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "            else:\n",
    "                C = img.shape[0]\n",
    "                assert C == 1 or C == 3, 'Only (1, H, W) or (3, H, W) images are supported'\n",
    "\n",
    "            return tag, img\n",
    "\n",
    "        tagged_images = self.process_batch(name, batch)\n",
    "\n",
    "        return list(map(_check_img, tagged_images))\n",
    "\n",
    "    def process_batch(self, name, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class DefaultTensorboardFormatter(_TensorboardFormatter):\n",
    "    def __init__(self, skip_last_target=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.skip_last_target = skip_last_target\n",
    "\n",
    "    def process_batch(self, name, batch):\n",
    "        if name == 'targets' and self.skip_last_target:\n",
    "            batch = batch[:, :-1, ...]\n",
    "\n",
    "        tag_template = '{}/batch_{}/channel_{}/slice_{}'\n",
    "\n",
    "        tagged_images = []\n",
    "\n",
    "        if batch.ndim == 5:\n",
    "            # NCDHW\n",
    "            slice_idx = batch.shape[2] // 2  # get the middle slice\n",
    "            for batch_idx in range(batch.shape[0]):\n",
    "                for channel_idx in range(batch.shape[1]):\n",
    "                    tag = tag_template.format(name, batch_idx, channel_idx, slice_idx)\n",
    "                    img = batch[batch_idx, channel_idx, slice_idx, ...]\n",
    "                    tagged_images.append((tag, self._normalize_img(img)))\n",
    "        else:\n",
    "            # batch has no channel dim: NDHW\n",
    "            slice_idx = batch.shape[1] // 2  # get the middle slice\n",
    "            for batch_idx in range(batch.shape[0]):\n",
    "                tag = tag_template.format(name, batch_idx, 0, slice_idx)\n",
    "                img = batch[batch_idx, slice_idx, ...]\n",
    "                tagged_images.append((tag, self._normalize_img(img)))\n",
    "\n",
    "        return tagged_images\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_img(img):\n",
    "        return np.nan_to_num((img - np.min(img)) / np.ptp(img))\n",
    "\n",
    "\n",
    "def _find_masks(batch, min_size=10):\n",
    "    \"\"\"Center the z-slice in the 'middle' of a given instance, given a batch of instances\n",
    "\n",
    "    Args:\n",
    "        batch (ndarray): 5d numpy tensor (NCDHW)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for b in batch:\n",
    "        assert b.shape[0] == 1\n",
    "        patch = b[0]\n",
    "        z_sum = patch.sum(axis=(1, 2))\n",
    "        coords = np.where(z_sum > min_size)[0]\n",
    "        if len(coords) > 0:\n",
    "            ind = coords[len(coords) // 2]\n",
    "            result.append(b[:, ind:ind + 1, ...])\n",
    "        else:\n",
    "            ind = b.shape[1] // 2\n",
    "            result.append(b[:, ind:ind + 1, ...])\n",
    "\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "\n",
    "def get_tensorboard_formatter(formatter_config):\n",
    "    if formatter_config is None:\n",
    "        return DefaultTensorboardFormatter()\n",
    "\n",
    "    class_name = formatter_config['name']\n",
    "    m = importlib.import_module('pytorch3dunet.unet3d.utils')\n",
    "    clazz = getattr(m, class_name)\n",
    "    return clazz(**formatter_config)\n",
    "\n",
    "\n",
    "def expand_as_one_hot(input, C, ignore_index=None):\n",
    "    \"\"\"\n",
    "    Converts NxSPATIAL label image to NxCxSPATIAL, where each label gets converted to its corresponding one-hot vector.\n",
    "    It is assumed that the batch dimension is present.\n",
    "    Args:\n",
    "        input (torch.Tensor): 3D/4D input image\n",
    "        C (int): number of channels/labels\n",
    "        ignore_index (int): ignore index to be kept during the expansion\n",
    "    Returns:\n",
    "        4D/5D output torch.Tensor (NxCxSPATIAL)\n",
    "    \"\"\"\n",
    "    assert input.dim() == 4\n",
    "\n",
    "    # expand the input tensor to Nx1xSPATIAL before scattering\n",
    "    input = input.unsqueeze(1)\n",
    "    # create output tensor shape (NxCxSPATIAL)\n",
    "    shape = list(input.size())\n",
    "    shape[1] = C\n",
    "\n",
    "    if ignore_index is not None:\n",
    "        # create ignore_index mask for the result\n",
    "        mask = input.expand(shape) == ignore_index\n",
    "        # clone the src tensor and zero out ignore_index in the input\n",
    "        input = input.clone()\n",
    "        input[input == ignore_index] = 0\n",
    "        # scatter to get the one-hot tensor\n",
    "        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
    "        # bring back the ignore_index in the result\n",
    "        result[mask] = ignore_index\n",
    "        return result\n",
    "    else:\n",
    "        # scatter to get the one-hot tensor\n",
    "        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
    "\n",
    "\n",
    "def convert_to_numpy(*inputs):\n",
    "    \"\"\"\n",
    "    Coverts input tensors to numpy ndarrays\n",
    "\n",
    "    Args:\n",
    "        inputs (iteable of torch.Tensor): torch tensor\n",
    "\n",
    "    Returns:\n",
    "        tuple of ndarrays\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_numpy(i):\n",
    "        assert isinstance(i, torch.Tensor), \"Expected input to be torch.Tensor\"\n",
    "        return i.detach().cpu().numpy()\n",
    "\n",
    "    return (_to_numpy(i) for i in inputs)\n",
    "\n",
    "\n",
    "def create_optimizer(optimizer_config, model):\n",
    "    learning_rate = optimizer_config['learning_rate']\n",
    "    weight_decay = optimizer_config.get('weight_decay', 0)\n",
    "    betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def create_lr_scheduler(lr_config, optimizer):\n",
    "    if lr_config is None:\n",
    "        return None\n",
    "    class_name = lr_config.pop('name')\n",
    "    m = importlib.import_module('torch.optim.lr_scheduler')\n",
    "    clazz = getattr(m, class_name)\n",
    "    # add optimizer to the config\n",
    "    lr_config['optimizer'] = optimizer\n",
    "    return clazz(**lr_config)\n",
    "\n",
    "\n",
    "def get_class(class_name, modules):\n",
    "    for module in modules:\n",
    "        m = importlib.import_module(module)\n",
    "        clazz = getattr(m, class_name, None)\n",
    "        if clazz is not None:\n",
    "            return clazz\n",
    "    raise RuntimeError(f'Unsupported dataset class: {class_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1677566604544,
     "user": {
      "displayName": "BonKong Lai",
      "userId": "14934257860777894140"
     },
     "user_tz": -480
    },
    "id": "OF34wz2gq6Ei"
   },
   "outputs": [],
   "source": [
    "class Abstract3DUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for standard and residual UNet.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output segmentation masks;\n",
    "            Note that that the of out_channels might correspond to either\n",
    "            different semantic classes or to different binary segmentation mask.\n",
    "            It's up to the user of the class to interpret the out_channels and\n",
    "            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n",
    "            or BCEWithLogitsLoss (two-class) respectively)\n",
    "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
    "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
    "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n",
    "            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n",
    "            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n",
    "        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)\n",
    "        layer_order (string): determines the order of layers\n",
    "            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.\n",
    "            See `SingleConv` for more info\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
    "        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied\n",
    "            after the final convolution; if False (regression problem) the normalization layer is skipped at the end\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_kernel_size=3, pool_kernel_size=2,\n",
    "                 conv_padding=1, **kwargs):\n",
    "        super(Abstract3DUNet, self).__init__()\n",
    "\n",
    "        if isinstance(f_maps, int):\n",
    "            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
    "\n",
    "        assert isinstance(f_maps, list) or isinstance(f_maps, tuple)\n",
    "        assert len(f_maps) > 1, \"Required at least 2 levels in the U-Net\"\n",
    "\n",
    "        # create encoder path\n",
    "        self.encoders = create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order,\n",
    "                                        num_groups, pool_kernel_size)\n",
    "\n",
    "        # create decoder path\n",
    "        self.decoders = create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
    "                                        upsample=True)\n",
    "\n",
    "        # in the last layer a 1×1 convolution reduces the number of output\n",
    "        # channels to the number of labels\n",
    "        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n",
    "\n",
    "        if is_segmentation:\n",
    "            # semantic segmentation problem\n",
    "            if final_sigmoid:\n",
    "                self.final_activation = nn.Sigmoid()\n",
    "            else:\n",
    "                self.final_activation = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            # regression problem\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder part\n",
    "        encoders_features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            print(\"Current x.shape:\", x.shape)\n",
    "            # reverse the encoder outputs to be aligned with the decoder\n",
    "            encoders_features.insert(0, x)\n",
    "\n",
    "        # remove the last encoder's output from the list\n",
    "        # !!remember: it's the 1st in the list\n",
    "        encoders_features = encoders_features[1:]\n",
    "\n",
    "        # decoder part\n",
    "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
    "            # pass the output from the corresponding encoder and the output\n",
    "            # of the previous decoder\n",
    "            x = decoder(encoder_features, x)\n",
    "            print(\"Current x.shape:\", x.shape)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        print(\"Current x.shape:\", x.shape)\n",
    "        # apply final_activation (i.e. Sigmoid or Softmax) only during prediction. During training the network outputs logits\n",
    "        if not self.training and self.final_activation is not None:\n",
    "            x = self.final_activation(x)\n",
    "            print(\"Current x.shape:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet3D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    3DUnet model from\n",
    "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
    "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
    "\n",
    "    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        super(UNet3D, self).__init__(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     final_sigmoid=final_sigmoid,\n",
    "                                     basic_module=DoubleConv,\n",
    "                                     f_maps=f_maps,\n",
    "                                     layer_order=layer_order,\n",
    "                                     num_groups=num_groups,\n",
    "                                     num_levels=num_levels,\n",
    "                                     is_segmentation=is_segmentation,\n",
    "                                     conv_padding=conv_padding,\n",
    "                                     **kwargs)\n",
    "\n",
    "\n",
    "class ResidualUNet3D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    Residual 3DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.\n",
    "    Uses ExtResNetBlock as a basic building block, summation joining instead\n",
    "    of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).\n",
    "    Since the model effectively becomes a residual net, in theory it allows for deeper UNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=5, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        super(ResidualUNet3D, self).__init__(in_channels=in_channels,\n",
    "                                             out_channels=out_channels,\n",
    "                                             final_sigmoid=final_sigmoid,\n",
    "                                             basic_module=ExtResNetBlock,\n",
    "                                             f_maps=f_maps,\n",
    "                                             layer_order=layer_order,\n",
    "                                             num_groups=num_groups,\n",
    "                                             num_levels=num_levels,\n",
    "                                             is_segmentation=is_segmentation,\n",
    "                                             conv_padding=conv_padding,\n",
    "                                             **kwargs)\n",
    "\n",
    "\n",
    "class UNet2D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    Just a standard 2D Unet. Arises naturally by specifying conv_kernel_size=(1, 3, 3), pool_kernel_size=(1, 2, 2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        if conv_padding == 1:\n",
    "            conv_padding = (0, 1, 1)\n",
    "        super(UNet2D, self).__init__(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     final_sigmoid=final_sigmoid,\n",
    "                                     basic_module=DoubleConv,\n",
    "                                     f_maps=f_maps,\n",
    "                                     layer_order=layer_order,\n",
    "                                     num_groups=num_groups,\n",
    "                                     num_levels=num_levels,\n",
    "                                     is_segmentation=is_segmentation,\n",
    "                                     conv_kernel_size=(1, 3, 3),\n",
    "                                     pool_kernel_size=(1, 2, 2),\n",
    "                                     conv_padding=conv_padding,\n",
    "                                     **kwargs)\n",
    "\n",
    "\n",
    "def get_model(model_config):\n",
    "    model_class = get_class(model_config['name'], modules=['pytorch3dunet.unet3d.model'])\n",
    "    return model_class(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fPVd8nKEry00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224, 32])\n",
      "Current x.shape: torch.Size([1, 64, 224, 224, 32])\n",
      "Current x.shape: torch.Size([1, 128, 112, 112, 16])\n",
      "Current x.shape: torch.Size([1, 256, 56, 56, 8])\n",
      "Current x.shape: torch.Size([1, 512, 28, 28, 4])\n",
      "Current x.shape: torch.Size([1, 256, 56, 56, 8])\n",
      "Current x.shape: torch.Size([1, 128, 112, 112, 16])\n",
      "Current x.shape: torch.Size([1, 64, 224, 224, 32])\n",
      "Current x.shape: torch.Size([1, 1, 224, 224, 32])\n",
      "torch.Size([1, 1, 224, 224, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.rand(1, 3, 224 ,224, 32)\n",
    "print(input.shape)\n",
    "Unet = UNet3D(in_channels=3, out_channels=1)\n",
    "output = Unet.forward(input)\n",
    "print(output.shape)\n",
    "# def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "#                  num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhRHCxdPL9JmZC8ItRyR8I",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
