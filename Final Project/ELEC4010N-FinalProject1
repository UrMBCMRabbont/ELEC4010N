{"cells":[{"cell_type":"markdown","source":["### 1, Download and unzip data"],"metadata":{"id":"Hci3KvN7ivcH"}},{"cell_type":"code","source":["!wget https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_Data.zip\n","!wget https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Test_Data.zip\n","!wget https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_GroundTruth.csv\n","!wget https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Test_GroundTruth.csv\n","!unzip \"./ISBI2016_ISIC_Part3_Test_Data.zip\"\n","!unzip \"./ISBI2016_ISIC_Part3_Training_Data.zip\""],"metadata":{"id":"ZXeyyLwuD6mr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2, Make training and test image folders by traning and test csv files"],"metadata":{"id":"RovDpxZFi-KA"}},{"cell_type":"code","source":["import os\n","import shutil\n","import pandas as pd\n","import numpy as np\n","\n","if not os.path.exists(\"./train\"):\n","    os.makedirs(\"./train\")\n","    os.makedirs(\"./train/benign\")\n","    os.makedirs(\"./train/malignant\")\n","if not os.path.exists(\"./val\"):\n","    os.makedirs(\"./val\")\n","    os.makedirs(\"./val/benign\")\n","    os.makedirs(\"./val/malignant\")\n","df = pd.read_csv(\"./ISBI2016_ISIC_Part3_Training_GroundTruth.csv\")\n","img_lis = []\n","lbl_lis = []\n","img_lis.append(\"ISIC_0000000\")\n","lbl_lis.append(\"benign\")\n","for i in range(len(df)):\n","    name, label = df[\"ISIC_0000000\"][i], df[\"benign\"][i]\n","    img_lis.append(name)\n","    lbl_lis.append(label)\n","N_train_val = len(img_lis)\n","N_train = int(N_train_val * 0.3)\n","print(N_train_val)\n","print(N_train)\n","shuffle_ix = np.random.permutation(np.arange(N_train_val))\n","ix_train = shuffle_ix[:N_train]\n","ix_val = shuffle_ix[N_train:]\n","for i in ix_train:\n","    name, label = img_lis[i], lbl_lis[i]\n","    shutil.copy(\"./ISBI2016_ISIC_Part3_Training_Data/\"+name+\".jpg\", \"./train/\"+label+\"/\"+name+\".jpg\")\n","for i in ix_val:\n","    name, label = img_lis[i], lbl_lis[i]\n","    shutil.copy(\"./ISBI2016_ISIC_Part3_Training_Data/\"+name+\".jpg\", \"./val/\"+label+\"/\"+name+\".jpg\")  \n","    \n","if not os.path.exists(\"./test\"):\n","    os.makedirs(\"./test\")\n","    os.makedirs(\"./test/benign\")\n","    os.makedirs(\"./test/malignant\")\n","df = pd.read_csv(\"./ISBI2016_ISIC_Part3_Test_GroundTruth.csv\")\n","shutil.copy(\"./ISBI2016_ISIC_Part3_Test_Data/ISIC_0000003.jpg\", \"./test/benign/ISIC_0000003.jpg\")\n","for i in range(len(df)):\n","    name, label = df[\"ISIC_0000003\"][i], df[\"0.0\"][i]\n","    label = \"benign\" if label == 0 else \"malignant\"\n","    shutil.copy(\"./ISBI2016_ISIC_Part3_Test_Data/\"+name+\".jpg\", \"./test/\"+label+\"/\"+name+\".jpg\")\n","    "],"metadata":{"id":"2WLrB9eWG3Nn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3, Build training and test loaders"],"metadata":{"id":"B77B7mO0jw6K"}},{"cell_type":"code","source":["from torchvision.datasets import ImageFolder\n","from torchvision import transforms\n","import torch\n","\n","root_train = './train'\n","train_transform = transforms.Compose([\n","    transforms.Resize(244),\n","    #transforms.RandomVerticalFlip(),       # using stronger augmentation functions to enhance performance or solve LT problem\n","    #transforms.RandomHorizontalFlip(),\n","    #transforms.ColorJitter(brightness=0.5, contrast=1, saturation=0.1, hue=0.5),\n","    transforms.RandomCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","root_val = './val'\n","root_test = './test'\n","test_transform = transforms.Compose([\n","    transforms.Resize(244),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","def get_train_test_set(batch_size=12):\n","    train_dataset = ImageFolder(root_train, transform=train_transform)\n","    loader_train = torch.utils.data.DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True\n","        )\n","    \n","    val_dataset = ImageFolder(root_val, transform=test_transform)\n","    loader_val = torch.utils.data.DataLoader(\n","        dataset=val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False\n","        )\n","    test_dataset = ImageFolder(root_test, transform=test_transform)\n","    loader_test = torch.utils.data.DataLoader(\n","        dataset=test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False\n","        )\n","    return loader_train, loader_val, loader_test\n","batch_size = 12\n","loader_train, loader_val, loader_test = get_train_test_set(batch_size)"],"metadata":{"id":"vizcUhavjvm3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4, Build the network"],"metadata":{"id":"zrEdDAUPmJtt"}},{"cell_type":"code","source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import resnet50\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","    \n","class Bottleneck(nn.Module):\n","    expansion = 4\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, layers, use_fc=False, dropout=None):\n","        self.inplanes = 64\n","        super(ResNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        self.avgpool = nn.AvgPool2d(7, stride=1)\n","        self.use_fc = use_fc\n","        self.use_dropout = True if dropout else False\n","        if self.use_fc:\n","            print('Using fc.')\n","            self.fc_add = nn.Linear(512*block.expansion, 512)\n","        if self.use_dropout:\n","            print('Using dropout.')\n","            self.dropout = nn.Dropout(p=dropout)\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, *args):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.avgpool(x)\n","        x = x.mean(dim=(-2, -1))\n","        x = x.view(x.size(0), -1)\n","        if self.use_fc:\n","            x = F.relu(self.fc_add(x))\n","        if self.use_dropout:\n","            x = self.dropout(x)\n","        return x\n","    \n","def get_resnet50(pre_trained=True):\n","    Resnet50 = ResNet(Bottleneck, [3, 4, 6, 3], dropout=None)\n","    if pre_trained:\n","        pre_trained = resnet50(weights = \"IMAGENET1K_V2\").state_dict() \n","        new_weights = {k: pre_trained[k] for k in Resnet50.state_dict()}\n","        Resnet50.load_state_dict(new_weights)\n","    return Resnet50\n","class Model(nn.Module):\n","    def __init__(self, encoder, num_classes=1):\n","        super(Model, self).__init__()\n","        self.encoder = encoder\n","        self.classifier = nn.Linear(2048, num_classes)\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.classifier(x)\n","        return x\n","resnet50 = get_resnet50(pre_trained=True)\n","model = Model(resnet50, 1)"],"metadata":{"id":"ggxJrmzPmP8L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5, Train and test your model"],"metadata":{"id":"5Q4AZq_enGEI"}},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","import torch.nn as nn\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","max_epoch = 10\n","use_cuda = True\n","if use_cuda:\n","    model = model.cuda()\n","class BCEFocalLoss(nn.Module):\n","    def __init__(self, gamma=2.0, alpha=0.6, reduction='mean'):\n","        super(BCEFocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.reduction = reduction\n","\n","    def forward(self, logits, target):\n","        alpha = self.alpha\n","        gamma = self.gamma\n","        loss = - alpha * (1 - logits) ** gamma * target * torch.log(logits) - \\\n","               (1 - alpha) * logits ** gamma * (1 - target) * torch.log(1 - logits)\n","        if self.reduction == 'mean':\n","            loss = loss.mean()\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        return loss\n","#criterion = nn.BCELoss()\n","criterion = BCEFocalLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","\n","loss_list = []\n","auc_train_list = []\n","auc_test_list = []\n","acc_train_list = []\n","acc_test_list = []\n","for epoch in range(max_epoch):\n","    train_lbl, train_pred = [], []\n","    running_loss = 0.0\n","    print(\" -- Epoch {}/{}\".format(epoch + 1, max_epoch))\n","    model.train()\n","    for data in tqdm(loader_train):\n","        optimizer.zero_grad()\n","        images, labels = data\n","        if use_cuda:\n","            images = images.cuda()\n","            labels = labels.float().cuda()\n","        outputs = model(images)[:,0]\n","        outputs = F.sigmoid(outputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        y_scores = list(outputs.detach().cpu().numpy())\n","        y_true = list(labels.detach().cpu().numpy())\n","        train_lbl += y_true\n","        train_pred += y_scores\n","\n","    loss = running_loss / len(loader_train)\n","    loss_list.append(loss)\n","    train_lbl, train_pred = np.array(train_lbl), np.array(train_pred)\n","    train_pred_lbl = np.around(train_pred) # pred >= 0.5 pred_lbl = 1 else pred_lbl = 0\n","    train_auc = roc_auc_score(train_lbl, train_pred)\n","    train_acc = accuracy_score(train_lbl, train_pred_lbl)\n","    auc_train_list.append(train_auc)\n","    acc_train_list.append(train_acc)\n","    \n","\n","    # evaluate on validation set\n","    model.eval()\n","    test_lbl, test_pred = [], []\n","    for data in loader_val:\n","        test_images, test_labels = data\n","        if use_cuda:\n","            test_images = test_images.cuda()\n","            test_labels = test_labels.float().cuda()\n","        test_outputs = model(test_images)[:, 0]\n","        test_outputs = F.sigmoid(test_outputs)\n","        y_scores = list(test_outputs.detach().cpu().numpy())\n","        y_true = list(test_labels.detach().cpu().numpy())\n","        test_lbl += y_true\n","        test_pred += y_scores\n","        \n","    test_lbl, test_pred = np.array(test_lbl), np.array(test_pred)\n","    test_pred_lbl = np.around(test_pred) # pred >= 0.5 pred_lbl = 1 else pred_lbl = 0\n","    test_auc = roc_auc_score(test_lbl, test_pred)\n","    test_acc = accuracy_score(test_lbl, test_pred_lbl)\n","    auc_test_list.append(test_auc)\n","    acc_test_list.append(test_acc)\n","    print(loss, train_auc, test_auc)\n","\n","\n","\n","#### evaluate on test set\n","model.eval()\n","test_lbl, test_pred = [], []\n","for data in loader_val:\n","    test_images, test_labels = data\n","    if use_cuda:\n","        test_images = test_images.cuda()\n","        test_labels = test_labels.float().cuda()\n","    test_outputs = model(test_images)[:, 0]\n","    test_outputs = F.sigmoid(test_outputs)\n","    y_scores = list(test_outputs.detach().cpu().numpy())\n","    y_true = list(test_labels.detach().cpu().numpy())\n","    test_lbl += y_true\n","    test_pred += y_scores\n","        \n","test_lbl, test_pred = np.array(test_lbl), np.array(test_pred)\n","test_pred_lbl = np.around(test_pred) # pred >= 0.5 pred_lbl = 1 else pred_lbl = 0\n","test_auc = roc_auc_score(test_lbl, test_pred)\n","test_acc = accuracy_score(test_lbl, test_pred_lbl)\n","print(test_auc, test_acc)"],"metadata":{"id":"vfSsJ4AbnEID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 6, plot your training and test curves"],"metadata":{"id":"oRiwcbu7oWx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"3HLpLM2ioXWf"}},{"cell_type":"markdown","source":["### Notes\n","\n","If there is a validation set, you should plot the training curves of validation set."],"metadata":{"id":"g2A9NuuniHdi"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(9,8))\n","plt.subplot(2,2,1)\n","plt.plot(auc_train_list)\n","plt.title(\"Train AUC\")\n","plt.subplot(2,2,2)\n","plt.plot(auc_test_list)\n","plt.title(\"Val AUC\")\n","plt.subplot(2,2,3)\n","plt.plot(acc_train_list)\n","plt.title(\"Train ACC\")\n","plt.subplot(2,2,4)\n","plt.plot(acc_test_list)\n","plt.title(\"Val ACC\")"],"metadata":{"id":"PB3pjrhWneY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(loss_list)\n","plt.title(\"Train Loss\")"],"metadata":{"id":"GyJdtfJ8tqR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"seg4VUUWw-3u"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1DLtw9mX04n0VIn7RPIrnJafnEyfX_CoY","timestamp":1683711057979},{"file_id":"https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022-labs/blob/main/lab02/notebooks/lab02b_cnn.ipynb","timestamp":1676776020914}],"gpuType":"A100"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}