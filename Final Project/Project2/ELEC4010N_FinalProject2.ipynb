{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jLPJjp76lZMY"
      ],
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "\n",
        "Baseline code: https://github.com/emma-sjwang/Dofe"
      ],
      "metadata": {
        "id": "z7rWTqmmi34W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm Implementation\n",
        "\n",
        "1. Implement the naive baseline model for comparison. This is typically a simple model with basic feature extraction and classification steps.\n",
        "\n",
        "2. Choose at least one Domain Generalization (DG) method to implement and compare against the naive baseline. This could be FACT, Dofe, or another method of your choice.\n",
        "\n",
        "3. Report the segmentation performance in terms of dice coefficient and average surface distance."
      ],
      "metadata": {
        "id": "ikoXfMF_lOJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and unzip data\n",
        "# Download from https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2\"\n",
        "!unzip \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2/Fundus-doFE.zip\" -d \"/content/\"\n",
        "\n",
        "# Install additional libraries\n",
        "!pip install -U albumentations\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install torchmetrics\n",
        "!git clone https://github.com/deepmind/surface-distance.git\n",
        "!pip install surface-distance/\n",
        "!pip install seg-metrics\n",
        "!pip install MedPy"
      ],
      "metadata": {
        "id": "IBA8GHAbJ-Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Baseline Model"
      ],
      "metadata": {
        "id": "jLPJjp76lZMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just for reference\n",
        "# UNet implementation\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv_down1 = double_conv(3, 64)      # Number of channel\n",
        "        self.conv_down2 = double_conv(64, 128)\n",
        "        self.conv_down3 = double_conv(128, 256)\n",
        "        self.conv_down4 = double_conv(256, 512)        \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        self.conv_up3 = double_conv(256 + 512, 256)\n",
        "        self.conv_up2 = double_conv(128 + 256, 128)\n",
        "        self.conv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.last_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        conv2 = self.conv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        conv3 = self.conv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "        x = self.conv_down4(x)\n",
        "        x = self.upsample(x)\n",
        "        \n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "        \n",
        "        x = self.conv_up1(x)\n",
        "        out = self.last_conv(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "2cHXXW2wj-Cj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "da2cf772-f401-4e18-c1e7-432036a0e30e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6ff8d284c3c9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         nn.ReLU(inplace=True))\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model = UNet(num_classes=1).to(device)\n",
        "output = model(torch.randn(1,3,256,256).to(device))\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "1LQxOa7tkAA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FACT\n",
        "\n",
        "https://github.com/MediaBrain-SJTU/FACT"
      ],
      "metadata": {
        "id": "gEwWV9bcle8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import math\n",
        "import copy\n",
        "import gc\n",
        "import random as randpy\n",
        "from copy import deepcopy\n",
        "from math import sqrt\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageFilter\n",
        "from pylab import *\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from ast import In\n",
        "import albumentations as Augm\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# PyTorch imports for deep learning\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50 as _resnet50\n",
        "from torchsummary import summary\n",
        "\n",
        "# Utilities for image data handling\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# For segmentation models\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Metrics\n",
        "from medpy import metric\n",
        "from segmentation_models_pytorch.losses import DiceLoss\n",
        "import surface_distance as surfdist\n",
        "import seg_metrics.seg_metrics as sg\n",
        "from torchmetrics.classification import BinaryJaccardIndex\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "w_qdq2stHBS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e64a55-f6ba-40a0-e619-e47bc28a698d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset info\n",
        "[cup]Domian1: Drishti-GS dataset [101] including training[50] and testing[51]\n",
        "\n",
        "[cup]Domain2: RIM-ONE_r3 dataset [159] including training and[99] testing[60]. \n",
        "\n",
        "[cup]Domain3: REFUGE training [400]  MICCAI 2018 workshop including training and[320] testing[80]. \n",
        "\n",
        "[cup]Domian4: REFUGE val [400]  including training and[320] testing[80]. \n",
        "Domain5: ISBI [81]  IDRID chanllenge\n",
        "\n"
      ],
      "metadata": {
        "id": "KB_yBwBhTu45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 3 classes\n",
        "label = cv2.imread(\"/content/val/mask/g0033.bmp\")\n",
        "np.unique(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlDSxZpb1uBo",
        "outputId": "0f3a9ef5-2e0e-4eaf-e7c5-c5b6d1be41ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([None], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmXdlqNFhHCN",
        "outputId": "ba988c55-3518-471c-bf4a-c29a9dfa1098"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to move the data\n",
        "train_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "\n",
        "# Define the path to load the data\n",
        "D_mask_dir = [[\"./Fundus/Domain1/train/mask\", \"./Fundus/Domain1/test/mask\"],\n",
        "              [\"./Fundus/Domain2/train/mask\", \"./Fundus/Domain2/test/mask\"],\n",
        "              [\"./Fundus/Domain3/train/mask\", \"./Fundus/Domain3/test/mask\"],\n",
        "              [\"./Fundus/Domain4/train/mask\", \"./Fundus/Domain4/test/mask\"]]\n",
        "\n",
        "# Define the fundus dataset class to load the data by functions\n",
        "class FundusDataset(Dataset):\n",
        "    # Define the constructor\n",
        "    def __init__(self, path, transform, transform_mask, split_idx):\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "        self.split_idx = split_idx\n",
        "        self.path_df = self.create_path_df()\n",
        "    \n",
        "    # Creates a DataFrame with image and mask paths\n",
        "    def create_path_df(self):\n",
        "        dirsImg, images, dirsMask, masks = [], [], [], []\n",
        "        \n",
        "        for mask_dir in self.path:\n",
        "            for img_path in mask_dir:\n",
        "                for _, _, files in os.walk(img_path):\n",
        "                    for file in files:\n",
        "                        check_mask = cv2.imread(os.path.join(img_path, file))\n",
        "                        \n",
        "                        if check_mask.sum() > 0:\n",
        "                            domain = re.search(r'Domain\\d', img_path).group() # Extract domain\n",
        "                            match (domain):\n",
        "                                case 'Domain1':\n",
        "                                    images.append(file)\n",
        "                                case 'Domain2':\n",
        "                                    images.append(file.replace('png','jpg'))\n",
        "                                case 'Domain3':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "                                case 'Domain4':\n",
        "                                    images.append(file.replace('bmp','jpg'))\n",
        "\n",
        "                            masks.append(file)\n",
        "                            dirsMask.append(img_path+'/')\n",
        "                            dirsImg.append(img_path.replace('mask','image')+'/')\n",
        "\n",
        "        return pd.DataFrame({'direcImg':dirsImg, 'images':images, 'direcMask':dirsMask, 'masks':masks})\n",
        "    \n",
        "    # Splits the train dataset into train and validation based on the split index\n",
        "    def split_traindataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1] + '/' + self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1] + '/' + self.path_df.loc[i]['masks']\n",
        "            domain = re.search(r'Domain\\d', image_direc).group() if image_direc else None\n",
        "            \n",
        "            # Split the dataset based on the project description\n",
        "            if domain:\n",
        "                if domain != \"Domain4\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain3\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain2\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain1\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "\n",
        "    # Moves the test dataset to the test path\n",
        "    def split_testdataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1]+'/'+self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1]+'/'+self.path_df.loc[i]['masks']\n",
        "            shutil.copy(image_direc, test_path+\"/image/\")\n",
        "            shutil.copy(mask_direc, test_path+\"/mask/\")\n",
        "\n",
        "    # Returns length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.path_df)\n",
        "\n",
        "    #  Creates a list of image and mask paths\n",
        "    def direc(self):\n",
        "        FundusImg_direc = []\n",
        "        Mask_direc = []\n",
        "        train_folders = os.listdir(train_path)\n",
        "        \n",
        "        for folder in train_folders:\n",
        "            files = os.listdir(os.path.join(train_path, folder))\n",
        "            \n",
        "            for file in files:\n",
        "                direc = os.path.join(train_path, folder, file)\n",
        "                \n",
        "                if 'mask' in folder:\n",
        "                    Mask_direc.append(direc)\n",
        "                else:\n",
        "                    FundusImg_direc.append(direc)\n",
        "        \n",
        "        return FundusImg_direc, Mask_direc"
      ],
      "metadata": {
        "id": "d9xTyMmkHiT6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the train, validation, and test folders\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path)\n",
        "    os.makedirs(os.path.join(path, \"image\"))\n",
        "    os.makedirs(os.path.join(path, \"mask\"))\n",
        "\n",
        "# Normalization\n",
        "normalization = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(64),\n",
        "    transforms.RandomCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "# train = 1,2,3 test= 4\n",
        "train_class = FundusDataset(D_mask_dir[:3], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "test_class = FundusDataset(D_mask_dir[3:], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "\n",
        "# Split files of system\n",
        "train_class.split_traindataset()\n",
        "test_class.split_testdataset()"
      ],
      "metadata": {
        "id": "4p8xLZdGxAqZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data_utils.py"
      ],
      "metadata": {
        "id": "ASCCtcZzsZbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/\""
      ],
      "metadata": {
        "id": "R-xsxxEzYJAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2e9e24-d8c3-46a1-9ed7-2afb31619f7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_info(filepath):\n",
        "    img_direcs = []\n",
        "    mask_direcs = []\n",
        "    for folder in os.listdir(filepath):\n",
        "        files = os.listdir(filepath+'/'+folder)\n",
        "        for file in files:\n",
        "            direc = filepath+'/'+folder+'/'+file\n",
        "            if 'mask' in direc: mask_direcs.append(direc)\n",
        "            else: img_direcs.append(direc)\n",
        "    return img_direcs, mask_direcs\n",
        "\n",
        "\n",
        "def get_img_transform(train=False, image_size=224, crop=False, jitter=0):\n",
        "    mean = [0.5]\n",
        "    std = [0.5]\n",
        "    if train:\n",
        "        if crop:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "        else:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.Resize((image_size, image_size))]\n",
        "        if jitter > 0:\n",
        "            img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                        contrast=jitter,\n",
        "                                                        saturation=jitter,\n",
        "                                                        hue=min(0.5, jitter)))\n",
        "        img_transform += [transforms.RandomHorizontalFlip(),\n",
        "                          transforms.Normalize(mean, std)]\n",
        "        img_transform = transforms.Compose(img_transform)\n",
        "    else:\n",
        "        img_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "    return img_transform\n",
        "\n",
        "def get_label_transform(train=False, image_size=224, crop=False, jitter=0):\n",
        "    mean = [0.5]\n",
        "    std = [0.5]\n",
        "    if train:\n",
        "        if crop:\n",
        "            label_transform = [transforms.ToPILImage(),transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0], interpolation=Image.NEAREST)]\n",
        "        else:\n",
        "            label_transform = [transforms.ToPILImage(),transforms.Resize((image_size, image_size), interpolation=Image.NEAREST)]\n",
        "        if jitter > 0:\n",
        "            label_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                        contrast=jitter,\n",
        "                                                        saturation=jitter,\n",
        "                                                        hue=min(0.5, jitter)))\n",
        "        label_transform += [transforms.RandomHorizontalFlip(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize(mean, std)]\n",
        "        label_transform = transforms.Compose(label_transform)\n",
        "    else:\n",
        "        label_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((image_size, image_size), interpolation=Image.NEAREST),\n",
        "        ])\n",
        "    return label_transform\n",
        "    \n",
        "\n",
        "def get_pre_transform(image_size=224, crop=False, jitter=0):\n",
        "    if crop:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "    else:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.Resize((image_size, image_size))]\n",
        "    if jitter > 0:\n",
        "        img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                    contrast=jitter,\n",
        "                                                    saturation=jitter,\n",
        "                                                    hue=min(0.5, jitter)))\n",
        "    img_transform += [transforms.RandomHorizontalFlip(), lambda x: np.asarray(x)]\n",
        "    img_transform = transforms.Compose(img_transform)\n",
        "    return img_transform\n",
        "\n",
        "def get_post_transform(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    img_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    return img_transform\n",
        "\n",
        "def colorful_spectrum_mix(img1, img2, alpha, ratio=1.0):\n",
        "    \"\"\"Input image size: ndarray of [H, W, C]\"\"\"\n",
        "    lam = np.random.uniform(0, alpha)\n",
        "\n",
        "    assert img1.shape == img2.shape\n",
        "    h, w, c = img1.shape\n",
        "    h_crop = int(h * sqrt(ratio))\n",
        "    w_crop = int(w * sqrt(ratio))\n",
        "    h_start = h // 2 - h_crop // 2\n",
        "    w_start = w // 2 - w_crop // 2\n",
        "\n",
        "    img1_fft = np.fft.fft2(img1, axes=(0, 1))\n",
        "    img2_fft = np.fft.fft2(img2, axes=(0, 1))\n",
        "    img1_abs, img1_pha = np.abs(img1_fft), np.angle(img1_fft)\n",
        "    img2_abs, img2_pha = np.abs(img2_fft), np.angle(img2_fft)\n",
        "\n",
        "    img1_abs = np.fft.fftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.fftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img1_abs_ = np.copy(img1_abs)\n",
        "    img2_abs_ = np.copy(img2_abs)\n",
        "    img1_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img2_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img1_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "    img2_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img1_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img2_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "\n",
        "    img1_abs = np.fft.ifftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.ifftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img21 = img1_abs * (np.e ** (1j * img1_pha))\n",
        "    img12 = img2_abs * (np.e ** (1j * img2_pha))\n",
        "    img21 = np.real(np.fft.ifft2(img21, axes=(0, 1)))\n",
        "    img12 = np.real(np.fft.ifft2(img12, axes=(0, 1)))\n",
        "    img21 = np.uint8(np.clip(img21, 0, 255))\n",
        "    img12 = np.uint8(np.clip(img12, 0, 255))\n",
        "\n",
        "    return img21, img12"
      ],
      "metadata": {
        "id": "Kyu2yCvnsAnj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1.1 Data Read(Fourier) & Load "
      ],
      "metadata": {
        "id": "dBgSpAIZIh-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, lbl_transformer=None):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.lbl_transformer = lbl_transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.names[index]\n",
        "        mask_name = self.labels[index]\n",
        "        img = cv2.imread(img_name).copy()\n",
        "        mask = cv2.imread(mask_name).copy()\n",
        "        if self.transformer is not None:\n",
        "            img = self.transformer(img)\n",
        "            mask = self.lbl_transformer(mask)\n",
        "        return img, mask\n",
        "\n",
        "class FourierDGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, lbl_transformer=None, from_domain=None, alpha=1.0):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.lbl_transformer = lbl_transformer\n",
        "        self.post_transform = get_post_transform()\n",
        "        self.from_domain = from_domain\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.flat_names = []\n",
        "        self.flat_labels = []\n",
        "        self.flat_domains = []\n",
        "        for i in range(len(names)):\n",
        "            self.flat_domains += [i] * len(names[i])\n",
        "            self.flat_names += names[i]\n",
        "            self.flat_labels += labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.names[index]\n",
        "        mask_name = self.labels[index]\n",
        "        img = cv2.imread(img_name).copy()\n",
        "        mask = cv2.imread(mask_name).copy()\n",
        "        img_o = self.transformer(img)\n",
        "        mask_o = self.lbl_transformer(mask)\n",
        "\n",
        "        img_s, mask_s, _ = self.sample_image()  ## random pick image\n",
        "        img_s2o, img_o2s = colorful_spectrum_mix(img_o, img_s, alpha=self.alpha)  ## mix their amplitude\n",
        "        img_o, img_s = self.post_transform(img_o), self.post_transform(img_s)\n",
        "        img_s2o, img_o2s = self.post_transform(img_s2o), self.post_transform(img_o2s)\n",
        "        img = [img_o, img_s, img_s2o, img_o2s]  ## [original, img2, img1 x img2, img2 x img1] \n",
        "        mask = [mask_o, mask_s, mask_o, mask_s]\n",
        "        # domain = [domain, domain_s, domain, domain_s]\n",
        "        return img, mask\n",
        "\n",
        "    def sample_image(self, domain=None):\n",
        "        if self.from_domain == 'all':\n",
        "            domain_idx = randpy.randint(0, len(self.names)-1)\n",
        "        elif self.from_domain == 'inter':\n",
        "            domains = list(range(len(self.names)))\n",
        "            # domains.remove(domain)\n",
        "            domain_idx = randpy.sample(domains, 1)[0]\n",
        "        elif self.from_domain == 'intra':\n",
        "            domain_idx = domain\n",
        "        else:\n",
        "            raise ValueError(\"Not implemented\")\n",
        "        img_idx = randpy.randint(0, len(self.names[domain_idx])-1)\n",
        "        imgn_ame_sampled = self.names[img_idx]\n",
        "        img_sampled = cv2.imread(imgn_ame_sampled)\n",
        "        label_ame_sampled = self.labels[img_idx]\n",
        "        label_sampled = cv2.imread(label_ame_sampled)\n",
        "        label_sampled = self.lbl_transformer(label_sampled)\n",
        "        return self.transformer(img_sampled), label_sampled, domain_idx\n",
        "\n",
        "\n",
        "def get_dataset(path, train=False, image_size=64, crop=False, jitter=0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_img_transform(train, image_size, crop, jitter)\n",
        "    lbl_transform = get_label_transform(train, image_size=image_size, crop=crop, jitter=jitter)\n",
        "    return DGDataset(names, labels, img_transform, lbl_transform)\n",
        "\n",
        "def get_fourier_dataset(path, image_size=64, crop=False, jitter=0, from_domain='all', alpha=1.0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_pre_transform(image_size, crop, jitter)\n",
        "    lbl_transform = get_label_transform(train=True, image_size=image_size, crop=crop, jitter=jitter)\n",
        "    return FourierDGDataset(names, labels, img_transform, lbl_transform, from_domain, alpha)\n",
        "\n",
        "\n",
        "\n",
        "trian_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "\n",
        "train_fourier_dataset = get_fourier_dataset(path=train_path,crop=True,jitter=0.1)\n",
        "val_dataset = get_dataset(path=val_path, train=False)\n",
        "test_dataset = get_dataset(path=test_path, train=False)\n",
        "\n",
        "train_fourier_loader = torch.utils.data.DataLoader(train_fourier_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "print(f'Shape of image: {train_fourier_loader.dataset[0][0][0].numpy().shape}')\n",
        "print(f'Number of training batches: {len(train_fourier_loader)}')\n",
        "print(f'Number of validation batches: {len(val_loader)}')\n",
        "print(f'Number of test batches: {len(test_loader)}')"
      ],
      "metadata": {
        "id": "lpwjv5xjlfPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a05dfc0-87c0-4834-fbf5-ce7bdc60f716"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image: (3, 64, 64)\n",
            "Number of training batches: 66\n",
            "Number of validation batches: 17\n",
            "Number of test batches: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Build Unet2D Model"
      ],
      "metadata": {
        "id": "3SL8XcjmAnQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Teacher Model"
      ],
      "metadata": {
        "id": "O38A5I8dBnrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Teacher Model\n",
        "# Student model would be ResNet50 model\n",
        "class MeanTeacherModel(nn.Module):\n",
        "    # Core\n",
        "    def __init__(self, student_model, ema_decay):\n",
        "        super().__init__()\n",
        "        self.student_model = student_model\n",
        "        self.teacher_model = deepcopy(student_model)\n",
        "        self.ema_decay = ema_decay\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.student_model(x)\n",
        "\n",
        "    def update_teacher_model(self, current_epoch, momentum=0.9995):\n",
        "        # The momentum increases from 0 to ema_decay\n",
        "        # Useful for improving quickly at the beginning\n",
        "        momentum = min(1 - 1 / (current_epoch + 1), self.ema_decay)\n",
        "        with torch.no_grad():\n",
        "            for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):\n",
        "                teacher_params.data.mul_(momentum).add_((1 - momentum) * student_params.data)\n",
        "\n",
        "    # Adjust the weight of the consistency loss to rely on teacher's prediction\n",
        "    # The weight factor decreases from 1 to 0 during the first 5 epochs\n",
        "    def sigmoid_rampup(self, current_epoch):\n",
        "        current_epoch = np.clip(current_epoch, 0.0, 7.0)\n",
        "        phase = 1.0 - current_epoch / 7.0\n",
        "        return np.exp(-5.0 * phase * phase).astype(np.float32)\n",
        "\n",
        "    # The weight decreases from 2\n",
        "    def get_consistency_weight(self, epoch):\n",
        "        return 2.0 * self.sigmoid_rampup(epoch)"
      ],
      "metadata": {
        "id": "zkXkYCFGBdJj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medical Image Metrics"
      ],
      "metadata": {
        "id": "wPIglb1nI4d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice Loss\n",
        "class SoftDiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(SoftDiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, targets, smooth=1):\n",
        "        num = targets.size(0)\n",
        "        probs = logits\n",
        "        print(probs.shape)\n",
        "        print(targets.shape)\n",
        "        m1 = probs.view(num, -1)\n",
        "        m2 = targets.view(num, -1)\n",
        "        print(m1.shape)\n",
        "        print(m2.shape)\n",
        "        intersection = (m1 * m2)\n",
        " \n",
        "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
        "        return 1 - score.sum() / num\n",
        "\n",
        "    def dice_coeff(self, logits, targets):\n",
        "        return 1-(self.forward(logits, targets))\n",
        "\n",
        "# Implement ASD\n",
        "def compute_asd(outputs, labels):\n",
        "  OC, OD, BG = torch.unique(labels)[0].item(), torch.unique(labels)[1].item(), torch.unique(labels)[2].item()\n",
        "  outputs = F.softmax(outputs,dim=1)\n",
        "  outputs_cpy1, outputs_cpy2 = outputs.clone(), outputs.clone()\n",
        "  labels_cpy1, labels_cpy2 = labels.clone(), labels.clone()\n",
        "  ## [-1, 0.xx..., 1]\n",
        "  mean, std, max, min = outputs.mean(), outputs.std(), torch.max(outputs), torch.min(outputs)\n",
        "\n",
        "  ## OC asd\n",
        "  labels_cpy1[labels_cpy1 == OC] = 1.0\n",
        "  labels_cpy1[labels_cpy1 != OC] = 0.0\n",
        "  outputs_cpy1[outputs_cpy1 > (mean-std)] = 0.0\n",
        "  labels_np = labels.cpu().detach().numpy().astype(np.bool_)\n",
        "  outputs_np = outputs_cpy1.cpu().detach().numpy().astype(np.bool_)\n",
        "  surface_dist = surfdist.compute_surface_distances(labels_np.reshape((1,-1,labels_np.shape[3])),\n",
        "                                                    outputs_np.reshape((1,-1,outputs_np.shape[3])),spacing_mm=(1.0,1.0,1.0))\n",
        "  oc_avg_asd = surfdist.compute_average_surface_distance(surface_dist)[1]\n",
        "\n",
        "  ## OD asd\n",
        "  labels_cpy2[labels_cpy2 == OD] = 1.0\n",
        "  labels_cpy2[labels_cpy2 != OD] = 0.0\n",
        "  outputs_cpy2[outputs_cpy2 < (mean-std)] = 0.0\n",
        "  outputs_cpy2[outputs_cpy2 > (mean+std)] = 0.0\n",
        "  labels_np = labels.cpu().detach().numpy().astype(np.bool_)\n",
        "  outputs_np = outputs_cpy2.cpu().detach().numpy().astype(np.bool_)\n",
        "  surface_dist = surfdist.compute_surface_distances(labels_np.reshape((1,-1,labels_np.shape[3])),\n",
        "                                                    outputs_np.reshape((1,-1,outputs_np.shape[3])),spacing_mm=(1.0,1.0,1.0))\n",
        "  od_avg_asd = surfdist.compute_average_surface_distance(surface_dist)[1]\n",
        "\n",
        "  return oc_avg_asd, od_avg_asd"
      ],
      "metadata": {
        "id": "TmR0BSOXI26V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(8,3,64,64)\n",
        "print(torch.max(a), torch.min(a))\n",
        "print(a.mean()+a.std())\n",
        "print(a.mean()-a.std())\n",
        "\n",
        "a = np.array([[0.1,0.8689,1.12], [1.1,0.8689,1.12]])\n",
        "# print(a.astype(np.bool_))\n",
        "b = np.array([[0,0.89,1.12], [1.1,0.8689,1.12]])\n",
        "metric.binary.asd(a, b)"
      ],
      "metadata": {
        "id": "qYxtLwBvjCCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Training"
      ],
      "metadata": {
        "id": "j3_18EFUCceP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load model\n",
        "Unet2D_model = smp.Unet(\n",
        "    encoder_name=\"resnet50\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=3,\n",
        ")\n",
        "\n",
        "\n",
        "def train_mean_teacher(model, train_loader, val_loader, optimizer, scheduler,\n",
        "                       consistency_criterion, supervised_criterion, device, epochs):\n",
        "    # Clear GPU cache\n",
        "    if torch.cuda.is_available():\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    loss_total_list = []\n",
        "    dice_train_list = []\n",
        "    dice_val_list = []\n",
        "    asd_val_list = []\n",
        "\n",
        "    model.student_model.train()\n",
        "    model.teacher_model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        loss_total_train, dice_total_train, super_loss_train, const_loss_train = 0,0,0,0\n",
        "        for it, (batch, label) in enumerate(train_loader):\n",
        "            batch = torch.cat(batch, dim=0).to(device)\n",
        "            label = torch.cat(label, dim=0).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get predictions\n",
        "            scores = model.student_model(batch)\n",
        "            with torch.no_grad():\n",
        "                scores_teacher = model.teacher_model(batch)\n",
        "\n",
        "            # Split data and predictions into original and augmented parts\n",
        "            assert batch.size(0) % 2 == 0\n",
        "            split_idx = int(batch.size(0) / 2)\n",
        "            scores_ori, scores_aug = torch.split(scores, split_idx)\n",
        "            scores_ori_tea, scores_aug_tea = torch.split(scores_teacher, split_idx)\n",
        "            scores_ori_tea, scores_aug_tea = scores_ori_tea.detach(), scores_aug_tea.detach()\n",
        "            labels_ori, labels_aug = torch.split(label, split_idx)\n",
        "            assert scores_ori.size(0) == scores_aug.size(0)\n",
        "\n",
        "            # Compute supervised losses for original and augmented data\n",
        "            # Use KL Divergence for consistency loss\n",
        "            loss_cls = supervised_criterion(scores_ori, labels_ori)\n",
        "            loss_aug = supervised_criterion(scores_aug, labels_aug)\n",
        "\n",
        "            # Calculate probability\n",
        "            p_ori, p_aug = F.softmax(scores_ori / 10.0, dim=1), F.softmax(scores_aug / 10.0, dim=1)\n",
        "            p_ori_tea, p_aug_tea = F.softmax(scores_ori_tea / 10.0, dim=1), F.softmax(scores_aug_tea / 10.0, dim=1)\n",
        "\n",
        "            # Compute consistency losses\n",
        "            loss_ori_tea = consistency_criterion(p_aug.log(), p_ori_tea, reduction='batchmean')\n",
        "            loss_aug_tea = consistency_criterion(p_ori.log(), p_aug_tea, reduction='batchmean')\n",
        "\n",
        "            const_weight = model.get_consistency_weight(epoch)\n",
        "\n",
        "            # Compute total loss\n",
        "            train_dice_coeff = (-0.5) * (loss_cls-1) + (-0.5) * (loss_aug-1)\n",
        "            total_loss = 0.5 * loss_cls + 0.5 * loss_aug + const_weight * loss_ori_tea + const_weight * loss_aug_tea\n",
        "            print(f\"loss_cls: {loss_cls:.4f},   \")\n",
        "            print(f\"loss_aug: {loss_aug:.4f},   \")\n",
        "            print(f\"loss_ori_tea: {loss_ori_tea:.4f},   \")\n",
        "            print(f\"loss_aug_tea: {loss_aug_tea:.4f},   \")\n",
        "\n",
        "            # Backward pass and update weights\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update teacher model parameters\n",
        "            model.update_teacher_model(current_epoch=epoch)\n",
        "            loss_total_train += total_loss.item()\n",
        "            dice_total_train += train_dice_coeff.item()\n",
        "        loss_total_list.append(loss_total_train)\n",
        "        dice_train_list.append(dice_total_train)\n",
        "\n",
        "        ## Validation\n",
        "        model.student_model.eval()\n",
        "        dice_total_val, asd_total_val = 0,0\n",
        "        for it, (batch) in enumerate(val_loader):\n",
        "            data, labels = batch[0].to(device), batch[1].to(device)\n",
        "            scores = model.student_model(data)\n",
        "\n",
        "            loss = supervised_criterion(scores, labels)\n",
        "            oc_asd, od_asd = compute_asd(scores, labels)\n",
        "            asd_ori = 0.5*oc_asd + 0.5*od_asd\n",
        "            # Compute dice\n",
        "            val_dice_coeff = (-1) * (loss-1)\n",
        "            dice_total_val += val_dice_coeff.item()\n",
        "            asd_total_val += asd_ori\n",
        "        dice_val_list.append(dice_total_val)\n",
        "        asd_val_list.append(asd_total_val)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Epoch summary\n",
        "        print(f\"Epoch [{epoch+1:02}/{epochs}],   \"\n",
        "              f\"Mean Train Loss: {loss_total_train/len(train_loader):.4f},   \"\n",
        "              f\"Mean Train Dice: {dice_total_train/len(train_loader):.4f},   \"\n",
        "              f\"Mean Val Dice: {dice_total_val/len(val_loader):.4f},   \"\n",
        "              f\"Mean Val ASD: {asd_total_val/len(val_loader):.4f},   \")\n",
        "\n",
        "        # Clear GPU cache\n",
        "        if torch.cuda.is_available():\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return model, loss_total_list, dice_train_list, dice_val_list, asd_val_list\n",
        "\n",
        "# Load Unet2D as Student model and Mean Teacher model\n",
        "base_model = Unet2D_model.to(device)\n",
        "mean_teacher_model = MeanTeacherModel(base_model, ema_decay=0.99).to(device)\n",
        "\n",
        "# Optimizer, loss functions and scheduler\n",
        "optimizer = Adam(mean_teacher_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "consistency_criterion = F.kl_div\n",
        "supervised_criterion = DiceLoss(mode='multilabel', classes=3)\n",
        "\n",
        "epochs = 15\n",
        "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=epochs)\n",
        "\n",
        "# Run training\n",
        "mean_teacher_model, loss_total_list, dice_train_list, dice_val_list, asd_val_list = train_mean_teacher(\n",
        "    mean_teacher_model, train_fourier_loader, val_loader, optimizer, scheduler,\n",
        "    consistency_criterion, supervised_criterion, device, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "0f1zLnoeCWYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5bcd8d-19d7-466d-c52c-6f460982bb67"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_cls: 0.4183,   \n",
            "loss_aug: 0.4196,   \n",
            "loss_ori_tea: 10.4485,   \n",
            "loss_aug_tea: 10.4570,   \n",
            "loss_cls: 0.4145,   \n",
            "loss_aug: 0.4178,   \n",
            "loss_ori_tea: 9.1970,   \n",
            "loss_aug_tea: 9.1979,   \n",
            "loss_cls: 0.4235,   \n",
            "loss_aug: 0.4162,   \n",
            "loss_ori_tea: 8.7201,   \n",
            "loss_aug_tea: 8.7242,   \n",
            "loss_cls: 0.4175,   \n",
            "loss_aug: 0.4142,   \n",
            "loss_ori_tea: 7.9394,   \n",
            "loss_aug_tea: 7.9422,   \n",
            "loss_cls: 0.4208,   \n",
            "loss_aug: 0.4143,   \n",
            "loss_ori_tea: 7.5690,   \n",
            "loss_aug_tea: 7.5898,   \n",
            "loss_cls: 0.4109,   \n",
            "loss_aug: 0.4077,   \n",
            "loss_ori_tea: 7.2184,   \n",
            "loss_aug_tea: 7.2348,   \n",
            "loss_cls: 0.4095,   \n",
            "loss_aug: 0.4091,   \n",
            "loss_ori_tea: 4.8180,   \n",
            "loss_aug_tea: 4.8009,   \n",
            "loss_cls: 0.4103,   \n",
            "loss_aug: 0.4079,   \n",
            "loss_ori_tea: 4.8186,   \n",
            "loss_aug_tea: 4.8183,   \n",
            "loss_cls: 0.4108,   \n",
            "loss_aug: 0.4047,   \n",
            "loss_ori_tea: 5.5828,   \n",
            "loss_aug_tea: 5.6029,   \n",
            "loss_cls: 0.4143,   \n",
            "loss_aug: 0.4044,   \n",
            "loss_ori_tea: 4.9533,   \n",
            "loss_aug_tea: 4.9682,   \n",
            "loss_cls: 0.3862,   \n",
            "loss_aug: 0.3854,   \n",
            "loss_ori_tea: 3.2349,   \n",
            "loss_aug_tea: 3.2358,   \n",
            "loss_cls: 0.3963,   \n",
            "loss_aug: 0.3859,   \n",
            "loss_ori_tea: 4.1860,   \n",
            "loss_aug_tea: 4.2175,   \n",
            "loss_cls: 0.3897,   \n",
            "loss_aug: 0.3862,   \n",
            "loss_ori_tea: 1.8893,   \n",
            "loss_aug_tea: 1.8900,   \n",
            "loss_cls: 0.3931,   \n",
            "loss_aug: 0.3935,   \n",
            "loss_ori_tea: 1.2732,   \n",
            "loss_aug_tea: 1.2731,   \n",
            "loss_cls: 0.4016,   \n",
            "loss_aug: 0.3919,   \n",
            "loss_ori_tea: 2.4901,   \n",
            "loss_aug_tea: 2.4936,   \n",
            "loss_cls: 0.3870,   \n",
            "loss_aug: 0.3758,   \n",
            "loss_ori_tea: 2.4354,   \n",
            "loss_aug_tea: 2.4473,   \n",
            "loss_cls: 0.3890,   \n",
            "loss_aug: 0.3758,   \n",
            "loss_ori_tea: 2.5233,   \n",
            "loss_aug_tea: 2.5394,   \n",
            "loss_cls: 0.4016,   \n",
            "loss_aug: 0.3952,   \n",
            "loss_ori_tea: 1.3735,   \n",
            "loss_aug_tea: 1.3758,   \n",
            "loss_cls: 0.3868,   \n",
            "loss_aug: 0.3837,   \n",
            "loss_ori_tea: 1.2990,   \n",
            "loss_aug_tea: 1.3014,   \n",
            "loss_cls: 0.3916,   \n",
            "loss_aug: 0.3843,   \n",
            "loss_ori_tea: 1.1258,   \n",
            "loss_aug_tea: 1.1267,   \n",
            "loss_cls: 0.3764,   \n",
            "loss_aug: 0.3731,   \n",
            "loss_ori_tea: 0.8623,   \n",
            "loss_aug_tea: 0.8624,   \n",
            "loss_cls: 0.3718,   \n",
            "loss_aug: 0.3681,   \n",
            "loss_ori_tea: 0.9599,   \n",
            "loss_aug_tea: 0.9607,   \n",
            "loss_cls: 0.3757,   \n",
            "loss_aug: 0.3666,   \n",
            "loss_ori_tea: 1.4702,   \n",
            "loss_aug_tea: 1.4743,   \n",
            "loss_cls: 0.3801,   \n",
            "loss_aug: 0.3609,   \n",
            "loss_ori_tea: 1.6027,   \n",
            "loss_aug_tea: 1.6056,   \n",
            "loss_cls: 0.3774,   \n",
            "loss_aug: 0.3692,   \n",
            "loss_ori_tea: 1.2363,   \n",
            "loss_aug_tea: 1.2390,   \n",
            "loss_cls: 0.3658,   \n",
            "loss_aug: 0.3620,   \n",
            "loss_ori_tea: 0.4858,   \n",
            "loss_aug_tea: 0.4859,   \n",
            "loss_cls: 0.3637,   \n",
            "loss_aug: 0.3703,   \n",
            "loss_ori_tea: 0.8197,   \n",
            "loss_aug_tea: 0.8193,   \n",
            "loss_cls: 0.3748,   \n",
            "loss_aug: 0.3742,   \n",
            "loss_ori_tea: 0.3481,   \n",
            "loss_aug_tea: 0.3480,   \n",
            "loss_cls: 0.3606,   \n",
            "loss_aug: 0.3510,   \n",
            "loss_ori_tea: 1.1341,   \n",
            "loss_aug_tea: 1.1370,   \n",
            "loss_cls: 0.3590,   \n",
            "loss_aug: 0.3592,   \n",
            "loss_ori_tea: 0.4046,   \n",
            "loss_aug_tea: 0.4031,   \n",
            "loss_cls: 0.3653,   \n",
            "loss_aug: 0.3540,   \n",
            "loss_ori_tea: 0.9826,   \n",
            "loss_aug_tea: 0.9849,   \n",
            "loss_cls: 0.3575,   \n",
            "loss_aug: 0.3498,   \n",
            "loss_ori_tea: 0.5979,   \n",
            "loss_aug_tea: 0.5988,   \n",
            "loss_cls: 0.3598,   \n",
            "loss_aug: 0.3550,   \n",
            "loss_ori_tea: 0.6068,   \n",
            "loss_aug_tea: 0.6086,   \n",
            "loss_cls: 0.3560,   \n",
            "loss_aug: 0.3488,   \n",
            "loss_ori_tea: 0.8517,   \n",
            "loss_aug_tea: 0.8564,   \n",
            "loss_cls: 0.3585,   \n",
            "loss_aug: 0.3524,   \n",
            "loss_ori_tea: 0.5445,   \n",
            "loss_aug_tea: 0.5445,   \n",
            "loss_cls: 0.3568,   \n",
            "loss_aug: 0.3507,   \n",
            "loss_ori_tea: 0.3486,   \n",
            "loss_aug_tea: 0.3484,   \n",
            "loss_cls: 0.3553,   \n",
            "loss_aug: 0.3499,   \n",
            "loss_ori_tea: 0.5235,   \n",
            "loss_aug_tea: 0.5240,   \n",
            "loss_cls: 0.3547,   \n",
            "loss_aug: 0.3506,   \n",
            "loss_ori_tea: 0.3424,   \n",
            "loss_aug_tea: 0.3426,   \n",
            "loss_cls: 0.3558,   \n",
            "loss_aug: 0.3551,   \n",
            "loss_ori_tea: 0.2312,   \n",
            "loss_aug_tea: 0.2312,   \n",
            "loss_cls: 0.3646,   \n",
            "loss_aug: 0.3623,   \n",
            "loss_ori_tea: 0.2090,   \n",
            "loss_aug_tea: 0.2090,   \n",
            "loss_cls: 0.3515,   \n",
            "loss_aug: 0.3490,   \n",
            "loss_ori_tea: 0.2600,   \n",
            "loss_aug_tea: 0.2600,   \n",
            "loss_cls: 0.3516,   \n",
            "loss_aug: 0.3503,   \n",
            "loss_ori_tea: 0.3852,   \n",
            "loss_aug_tea: 0.3860,   \n",
            "loss_cls: 0.3460,   \n",
            "loss_aug: 0.3364,   \n",
            "loss_ori_tea: 0.6556,   \n",
            "loss_aug_tea: 0.6569,   \n",
            "loss_cls: 0.3530,   \n",
            "loss_aug: 0.3410,   \n",
            "loss_ori_tea: 0.7548,   \n",
            "loss_aug_tea: 0.7552,   \n",
            "loss_cls: 0.3383,   \n",
            "loss_aug: 0.3387,   \n",
            "loss_ori_tea: 0.1579,   \n",
            "loss_aug_tea: 0.1579,   \n",
            "loss_cls: 0.3408,   \n",
            "loss_aug: 0.3330,   \n",
            "loss_ori_tea: 0.6465,   \n",
            "loss_aug_tea: 0.6492,   \n",
            "loss_cls: 0.3444,   \n",
            "loss_aug: 0.3437,   \n",
            "loss_ori_tea: 0.1665,   \n",
            "loss_aug_tea: 0.1665,   \n",
            "loss_cls: 0.3470,   \n",
            "loss_aug: 0.3382,   \n",
            "loss_ori_tea: 0.5438,   \n",
            "loss_aug_tea: 0.5444,   \n",
            "loss_cls: 0.3490,   \n",
            "loss_aug: 0.3471,   \n",
            "loss_ori_tea: 0.2325,   \n",
            "loss_aug_tea: 0.2325,   \n",
            "loss_cls: 0.3414,   \n",
            "loss_aug: 0.3337,   \n",
            "loss_ori_tea: 0.7294,   \n",
            "loss_aug_tea: 0.7300,   \n",
            "loss_cls: 0.3480,   \n",
            "loss_aug: 0.3417,   \n",
            "loss_ori_tea: 0.5162,   \n",
            "loss_aug_tea: 0.5164,   \n",
            "loss_cls: 0.3346,   \n",
            "loss_aug: 0.3337,   \n",
            "loss_ori_tea: 0.1584,   \n",
            "loss_aug_tea: 0.1583,   \n",
            "loss_cls: 0.3379,   \n",
            "loss_aug: 0.3275,   \n",
            "loss_ori_tea: 0.6398,   \n",
            "loss_aug_tea: 0.6406,   \n",
            "loss_cls: 0.3363,   \n",
            "loss_aug: 0.3386,   \n",
            "loss_ori_tea: 0.2276,   \n",
            "loss_aug_tea: 0.2276,   \n",
            "loss_cls: 0.3369,   \n",
            "loss_aug: 0.3322,   \n",
            "loss_ori_tea: 0.6745,   \n",
            "loss_aug_tea: 0.6741,   \n",
            "loss_cls: 0.3395,   \n",
            "loss_aug: 0.3287,   \n",
            "loss_ori_tea: 0.4826,   \n",
            "loss_aug_tea: 0.4824,   \n",
            "loss_cls: 0.3297,   \n",
            "loss_aug: 0.3268,   \n",
            "loss_ori_tea: 0.1743,   \n",
            "loss_aug_tea: 0.1743,   \n",
            "loss_cls: 0.3346,   \n",
            "loss_aug: 0.3359,   \n",
            "loss_ori_tea: 0.6212,   \n",
            "loss_aug_tea: 0.6210,   \n",
            "loss_cls: 0.3408,   \n",
            "loss_aug: 0.3394,   \n",
            "loss_ori_tea: 0.2007,   \n",
            "loss_aug_tea: 0.2008,   \n",
            "loss_cls: 0.3324,   \n",
            "loss_aug: 0.3249,   \n",
            "loss_ori_tea: 0.4625,   \n",
            "loss_aug_tea: 0.4627,   \n",
            "loss_cls: 0.3307,   \n",
            "loss_aug: 0.3304,   \n",
            "loss_ori_tea: 0.1562,   \n",
            "loss_aug_tea: 0.1561,   \n",
            "loss_cls: 0.3380,   \n",
            "loss_aug: 0.3326,   \n",
            "loss_ori_tea: 0.3503,   \n",
            "loss_aug_tea: 0.3506,   \n",
            "loss_cls: 0.3251,   \n",
            "loss_aug: 0.3259,   \n",
            "loss_ori_tea: 0.1551,   \n",
            "loss_aug_tea: 0.1551,   \n",
            "loss_cls: 0.3296,   \n",
            "loss_aug: 0.3325,   \n",
            "loss_ori_tea: 0.7800,   \n",
            "loss_aug_tea: 0.7787,   \n",
            "loss_cls: 0.3348,   \n",
            "loss_aug: 0.3279,   \n",
            "loss_ori_tea: 0.3824,   \n",
            "loss_aug_tea: 0.3832,   \n",
            "loss_cls: 0.3255,   \n",
            "loss_aug: 0.3210,   \n",
            "loss_ori_tea: 0.2160,   \n",
            "loss_aug_tea: 0.2161,   \n",
            "Epoch [01/15],   Mean Train Loss: 0.4124,   Mean Train Dice: 0.6360,   Mean Val Dice: 0.6820,   Mean Val ASD: 0.0020,   \n",
            "loss_cls: 0.3154,   \n",
            "loss_aug: 0.3155,   \n",
            "loss_ori_tea: 0.3336,   \n",
            "loss_aug_tea: 0.4351,   \n",
            "loss_cls: 0.3242,   \n",
            "loss_aug: 0.3205,   \n",
            "loss_ori_tea: 0.5785,   \n",
            "loss_aug_tea: 1.6291,   \n",
            "loss_cls: 0.3169,   \n",
            "loss_aug: 0.3100,   \n",
            "loss_ori_tea: 0.5030,   \n",
            "loss_aug_tea: 0.3713,   \n",
            "loss_cls: 0.2979,   \n",
            "loss_aug: 0.2941,   \n",
            "loss_ori_tea: 0.4473,   \n",
            "loss_aug_tea: 0.3509,   \n",
            "loss_cls: 0.2968,   \n",
            "loss_aug: 0.2950,   \n",
            "loss_ori_tea: 0.5098,   \n",
            "loss_aug_tea: 0.4435,   \n",
            "loss_cls: 0.2823,   \n",
            "loss_aug: 0.2807,   \n",
            "loss_ori_tea: 0.6926,   \n",
            "loss_aug_tea: 0.5788,   \n",
            "loss_cls: 0.2749,   \n",
            "loss_aug: 0.2747,   \n",
            "loss_ori_tea: 0.6119,   \n",
            "loss_aug_tea: 0.5960,   \n",
            "loss_cls: 0.2698,   \n",
            "loss_aug: 0.2694,   \n",
            "loss_ori_tea: 0.6386,   \n",
            "loss_aug_tea: 0.6805,   \n",
            "loss_cls: 0.2596,   \n",
            "loss_aug: 0.2587,   \n",
            "loss_ori_tea: 0.8027,   \n",
            "loss_aug_tea: 0.8188,   \n",
            "loss_cls: 0.2513,   \n",
            "loss_aug: 0.2510,   \n",
            "loss_ori_tea: 1.1495,   \n",
            "loss_aug_tea: 0.9439,   \n",
            "loss_cls: 0.2501,   \n",
            "loss_aug: 0.2498,   \n",
            "loss_ori_tea: 0.8642,   \n",
            "loss_aug_tea: 0.7884,   \n",
            "loss_cls: 0.2619,   \n",
            "loss_aug: 0.2625,   \n",
            "loss_ori_tea: 0.8560,   \n",
            "loss_aug_tea: 0.9934,   \n",
            "loss_cls: 0.2351,   \n",
            "loss_aug: 0.2359,   \n",
            "loss_ori_tea: 1.0907,   \n",
            "loss_aug_tea: 0.8274,   \n",
            "loss_cls: 0.2376,   \n",
            "loss_aug: 0.2376,   \n",
            "loss_ori_tea: 1.3479,   \n",
            "loss_aug_tea: 0.5739,   \n",
            "loss_cls: 0.2264,   \n",
            "loss_aug: 0.2259,   \n",
            "loss_ori_tea: 1.1373,   \n",
            "loss_aug_tea: 1.4541,   \n",
            "loss_cls: 0.2051,   \n",
            "loss_aug: 0.2032,   \n",
            "loss_ori_tea: 1.2952,   \n",
            "loss_aug_tea: 0.7657,   \n",
            "loss_cls: 0.2081,   \n",
            "loss_aug: 0.2082,   \n",
            "loss_ori_tea: 0.9575,   \n",
            "loss_aug_tea: 1.7109,   \n",
            "loss_cls: 0.2039,   \n",
            "loss_aug: 0.2033,   \n",
            "loss_ori_tea: 0.6373,   \n",
            "loss_aug_tea: 1.5343,   \n",
            "loss_cls: 0.1807,   \n",
            "loss_aug: 0.1820,   \n",
            "loss_ori_tea: 1.2358,   \n",
            "loss_aug_tea: 0.8158,   \n",
            "loss_cls: 0.1830,   \n",
            "loss_aug: 0.1831,   \n",
            "loss_ori_tea: 0.8850,   \n",
            "loss_aug_tea: 1.4266,   \n",
            "loss_cls: 0.1745,   \n",
            "loss_aug: 0.1753,   \n",
            "loss_ori_tea: 1.0450,   \n",
            "loss_aug_tea: 1.3410,   \n",
            "loss_cls: 0.1412,   \n",
            "loss_aug: 0.1417,   \n",
            "loss_ori_tea: 1.0159,   \n",
            "loss_aug_tea: 1.4187,   \n",
            "loss_cls: 0.1574,   \n",
            "loss_aug: 0.1594,   \n",
            "loss_ori_tea: 1.4229,   \n",
            "loss_aug_tea: 1.0304,   \n",
            "loss_cls: 0.1180,   \n",
            "loss_aug: 0.1191,   \n",
            "loss_ori_tea: 1.2884,   \n",
            "loss_aug_tea: 1.1729,   \n",
            "loss_cls: 0.1178,   \n",
            "loss_aug: 0.1184,   \n",
            "loss_ori_tea: 0.9626,   \n",
            "loss_aug_tea: 1.4057,   \n",
            "loss_cls: 0.1076,   \n",
            "loss_aug: 0.1081,   \n",
            "loss_ori_tea: 1.0175,   \n",
            "loss_aug_tea: 1.4274,   \n",
            "loss_cls: 0.1098,   \n",
            "loss_aug: 0.1103,   \n",
            "loss_ori_tea: 1.0061,   \n",
            "loss_aug_tea: 1.4953,   \n",
            "loss_cls: 0.1005,   \n",
            "loss_aug: 0.1007,   \n",
            "loss_ori_tea: 1.1876,   \n",
            "loss_aug_tea: 1.2332,   \n",
            "loss_cls: 0.0902,   \n",
            "loss_aug: 0.0904,   \n",
            "loss_ori_tea: 0.9351,   \n",
            "loss_aug_tea: 1.7161,   \n",
            "loss_cls: 0.0825,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 0.8847,   \n",
            "loss_aug_tea: 1.4921,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.7664,   \n",
            "loss_aug_tea: 1.5464,   \n",
            "loss_cls: 0.0836,   \n",
            "loss_aug: 0.0844,   \n",
            "loss_ori_tea: 1.4954,   \n",
            "loss_aug_tea: 1.3193,   \n",
            "loss_cls: 0.0889,   \n",
            "loss_aug: 0.0887,   \n",
            "loss_ori_tea: 1.3735,   \n",
            "loss_aug_tea: 1.1457,   \n",
            "loss_cls: 0.0842,   \n",
            "loss_aug: 0.0843,   \n",
            "loss_ori_tea: 0.9581,   \n",
            "loss_aug_tea: 1.4883,   \n",
            "loss_cls: 0.0760,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 1.1629,   \n",
            "loss_aug_tea: 1.3119,   \n",
            "loss_cls: 0.0829,   \n",
            "loss_aug: 0.0834,   \n",
            "loss_ori_tea: 1.0476,   \n",
            "loss_aug_tea: 1.3153,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0865,   \n",
            "loss_ori_tea: 1.2727,   \n",
            "loss_aug_tea: 1.4769,   \n",
            "loss_cls: 0.0841,   \n",
            "loss_aug: 0.0844,   \n",
            "loss_ori_tea: 1.0880,   \n",
            "loss_aug_tea: 1.2244,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0789,   \n",
            "loss_ori_tea: 1.3567,   \n",
            "loss_aug_tea: 1.0945,   \n",
            "loss_cls: 0.0912,   \n",
            "loss_aug: 0.0915,   \n",
            "loss_ori_tea: 1.5945,   \n",
            "loss_aug_tea: 1.1353,   \n",
            "loss_cls: 0.0757,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 0.9323,   \n",
            "loss_aug_tea: 1.5302,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0867,   \n",
            "loss_ori_tea: 1.5094,   \n",
            "loss_aug_tea: 1.0508,   \n",
            "loss_cls: 0.0826,   \n",
            "loss_aug: 0.0833,   \n",
            "loss_ori_tea: 0.8615,   \n",
            "loss_aug_tea: 1.4241,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 1.0480,   \n",
            "loss_aug_tea: 1.0590,   \n",
            "loss_cls: 0.0931,   \n",
            "loss_aug: 0.0934,   \n",
            "loss_ori_tea: 1.2100,   \n",
            "loss_aug_tea: 1.0229,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 0.9074,   \n",
            "loss_aug_tea: 1.4475,   \n",
            "loss_cls: 0.0678,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 0.9532,   \n",
            "loss_aug_tea: 1.0906,   \n",
            "loss_cls: 0.0650,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 1.3258,   \n",
            "loss_aug_tea: 1.4977,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 1.0672,   \n",
            "loss_aug_tea: 1.0497,   \n",
            "loss_cls: 0.0884,   \n",
            "loss_aug: 0.0889,   \n",
            "loss_ori_tea: 1.1167,   \n",
            "loss_aug_tea: 1.1240,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 0.8845,   \n",
            "loss_aug_tea: 1.2715,   \n",
            "loss_cls: 0.0764,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 0.9160,   \n",
            "loss_aug_tea: 1.5941,   \n",
            "loss_cls: 0.0651,   \n",
            "loss_aug: 0.0654,   \n",
            "loss_ori_tea: 1.0824,   \n",
            "loss_aug_tea: 0.9358,   \n",
            "loss_cls: 0.0928,   \n",
            "loss_aug: 0.0931,   \n",
            "loss_ori_tea: 1.0207,   \n",
            "loss_aug_tea: 1.0921,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0716,   \n",
            "loss_ori_tea: 0.6029,   \n",
            "loss_aug_tea: 1.5006,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.8104,   \n",
            "loss_aug_tea: 1.3160,   \n",
            "loss_cls: 0.0833,   \n",
            "loss_aug: 0.0835,   \n",
            "loss_ori_tea: 0.7047,   \n",
            "loss_aug_tea: 1.5089,   \n",
            "loss_cls: 0.0863,   \n",
            "loss_aug: 0.0867,   \n",
            "loss_ori_tea: 1.0652,   \n",
            "loss_aug_tea: 1.5237,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 1.1428,   \n",
            "loss_aug_tea: 1.2815,   \n",
            "loss_cls: 0.0754,   \n",
            "loss_aug: 0.0756,   \n",
            "loss_ori_tea: 0.7805,   \n",
            "loss_aug_tea: 1.3176,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.9269,   \n",
            "loss_aug_tea: 1.4453,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0665,   \n",
            "loss_ori_tea: 0.7678,   \n",
            "loss_aug_tea: 1.3093,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0590,   \n",
            "loss_ori_tea: 0.7762,   \n",
            "loss_aug_tea: 1.3806,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0656,   \n",
            "loss_ori_tea: 0.9280,   \n",
            "loss_aug_tea: 1.3314,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 1.0051,   \n",
            "loss_aug_tea: 1.1504,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 1.2908,   \n",
            "loss_aug_tea: 0.9689,   \n",
            "Epoch [02/15],   Mean Train Loss: 0.2469,   Mean Train Dice: 0.8630,   Mean Val Dice: 0.9663,   Mean Val ASD: 0.0015,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 1.0363,   \n",
            "loss_aug_tea: 0.8550,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.8779,   \n",
            "loss_aug_tea: 1.4929,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.9397,   \n",
            "loss_aug_tea: 0.9920,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0721,   \n",
            "loss_ori_tea: 0.8796,   \n",
            "loss_aug_tea: 1.0231,   \n",
            "loss_cls: 0.0718,   \n",
            "loss_aug: 0.0719,   \n",
            "loss_ori_tea: 0.8804,   \n",
            "loss_aug_tea: 0.7900,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.6530,   \n",
            "loss_aug_tea: 1.4126,   \n",
            "loss_cls: 0.0792,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 0.7212,   \n",
            "loss_aug_tea: 1.4122,   \n",
            "loss_cls: 0.0569,   \n",
            "loss_aug: 0.0574,   \n",
            "loss_ori_tea: 1.4515,   \n",
            "loss_aug_tea: 0.8054,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.8037,   \n",
            "loss_aug_tea: 1.2379,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.6372,   \n",
            "loss_aug_tea: 1.2041,   \n",
            "loss_cls: 0.0911,   \n",
            "loss_aug: 0.0914,   \n",
            "loss_ori_tea: 0.9286,   \n",
            "loss_aug_tea: 0.9359,   \n",
            "loss_cls: 0.0643,   \n",
            "loss_aug: 0.0644,   \n",
            "loss_ori_tea: 0.7500,   \n",
            "loss_aug_tea: 0.9159,   \n",
            "loss_cls: 0.0672,   \n",
            "loss_aug: 0.0675,   \n",
            "loss_ori_tea: 1.1533,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0575,   \n",
            "loss_aug: 0.0579,   \n",
            "loss_ori_tea: 0.9524,   \n",
            "loss_aug_tea: 0.9526,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 1.3190,   \n",
            "loss_aug_tea: 1.3574,   \n",
            "loss_cls: 0.0512,   \n",
            "loss_aug: 0.0514,   \n",
            "loss_ori_tea: 0.9925,   \n",
            "loss_aug_tea: 0.9823,   \n",
            "loss_cls: 0.0845,   \n",
            "loss_aug: 0.0846,   \n",
            "loss_ori_tea: 0.9930,   \n",
            "loss_aug_tea: 0.9232,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 1.4387,   \n",
            "loss_aug_tea: 0.6314,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.6395,   \n",
            "loss_aug_tea: 1.3081,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 0.9254,   \n",
            "loss_aug_tea: 0.9744,   \n",
            "loss_cls: 0.0690,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 0.7495,   \n",
            "loss_aug_tea: 1.2152,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0769,   \n",
            "loss_ori_tea: 1.1188,   \n",
            "loss_aug_tea: 0.7306,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0642,   \n",
            "loss_ori_tea: 0.5678,   \n",
            "loss_aug_tea: 1.4054,   \n",
            "loss_cls: 0.0548,   \n",
            "loss_aug: 0.0548,   \n",
            "loss_ori_tea: 0.9616,   \n",
            "loss_aug_tea: 1.3817,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0745,   \n",
            "loss_ori_tea: 0.6722,   \n",
            "loss_aug_tea: 1.1905,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0672,   \n",
            "loss_ori_tea: 0.6860,   \n",
            "loss_aug_tea: 1.3395,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.7523,   \n",
            "loss_aug_tea: 1.0900,   \n",
            "loss_cls: 0.0919,   \n",
            "loss_aug: 0.0921,   \n",
            "loss_ori_tea: 1.3465,   \n",
            "loss_aug_tea: 0.6104,   \n",
            "loss_cls: 0.0710,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.7497,   \n",
            "loss_aug_tea: 1.1568,   \n",
            "loss_cls: 0.0619,   \n",
            "loss_aug: 0.0621,   \n",
            "loss_ori_tea: 0.9160,   \n",
            "loss_aug_tea: 0.8107,   \n",
            "loss_cls: 0.0622,   \n",
            "loss_aug: 0.0625,   \n",
            "loss_ori_tea: 0.6995,   \n",
            "loss_aug_tea: 1.1035,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.7016,   \n",
            "loss_aug_tea: 1.0117,   \n",
            "loss_cls: 0.0696,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.5274,   \n",
            "loss_aug_tea: 1.1883,   \n",
            "loss_cls: 0.0625,   \n",
            "loss_aug: 0.0627,   \n",
            "loss_ori_tea: 0.7251,   \n",
            "loss_aug_tea: 0.9206,   \n",
            "loss_cls: 0.0695,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 1.0396,   \n",
            "loss_aug_tea: 0.6920,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.5747,   \n",
            "loss_aug_tea: 1.0039,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0624,   \n",
            "loss_ori_tea: 0.6709,   \n",
            "loss_aug_tea: 0.9201,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.7236,   \n",
            "loss_aug_tea: 0.6933,   \n",
            "loss_cls: 0.0524,   \n",
            "loss_aug: 0.0527,   \n",
            "loss_ori_tea: 0.7185,   \n",
            "loss_aug_tea: 0.9533,   \n",
            "loss_cls: 0.0613,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.6354,   \n",
            "loss_aug_tea: 0.9123,   \n",
            "loss_cls: 0.0611,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.7075,   \n",
            "loss_aug_tea: 0.8535,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0700,   \n",
            "loss_ori_tea: 0.6666,   \n",
            "loss_aug_tea: 1.3656,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 0.6925,   \n",
            "loss_aug_tea: 1.3410,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 0.7637,   \n",
            "loss_aug_tea: 0.9660,   \n",
            "loss_cls: 0.0626,   \n",
            "loss_aug: 0.0628,   \n",
            "loss_ori_tea: 1.0227,   \n",
            "loss_aug_tea: 0.9081,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 1.0857,   \n",
            "loss_aug_tea: 0.8742,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 0.8906,   \n",
            "loss_aug_tea: 1.1089,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0789,   \n",
            "loss_ori_tea: 0.6165,   \n",
            "loss_aug_tea: 1.0768,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0631,   \n",
            "loss_ori_tea: 0.7792,   \n",
            "loss_aug_tea: 0.9990,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 1.0541,   \n",
            "loss_aug_tea: 0.7035,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0699,   \n",
            "loss_ori_tea: 0.5637,   \n",
            "loss_aug_tea: 1.0609,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 1.1830,   \n",
            "loss_aug_tea: 0.5651,   \n",
            "loss_cls: 0.0725,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.8734,   \n",
            "loss_aug_tea: 0.7418,   \n",
            "loss_cls: 0.0614,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.8850,   \n",
            "loss_aug_tea: 0.7593,   \n",
            "loss_cls: 0.0658,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 0.9939,   \n",
            "loss_aug_tea: 0.9156,   \n",
            "loss_cls: 0.0831,   \n",
            "loss_aug: 0.0833,   \n",
            "loss_ori_tea: 0.5148,   \n",
            "loss_aug_tea: 1.1709,   \n",
            "loss_cls: 0.0571,   \n",
            "loss_aug: 0.0571,   \n",
            "loss_ori_tea: 0.6143,   \n",
            "loss_aug_tea: 1.0257,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 0.5846,   \n",
            "loss_aug_tea: 1.0459,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.6336,   \n",
            "loss_aug_tea: 0.9024,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 0.6579,   \n",
            "loss_aug_tea: 1.1187,   \n",
            "loss_cls: 0.0484,   \n",
            "loss_aug: 0.0486,   \n",
            "loss_ori_tea: 0.8126,   \n",
            "loss_aug_tea: 0.8443,   \n",
            "loss_cls: 0.0577,   \n",
            "loss_aug: 0.0579,   \n",
            "loss_ori_tea: 0.6295,   \n",
            "loss_aug_tea: 1.1320,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 0.6883,   \n",
            "loss_aug_tea: 0.9265,   \n",
            "loss_cls: 0.0495,   \n",
            "loss_aug: 0.0495,   \n",
            "loss_ori_tea: 0.5383,   \n",
            "loss_aug_tea: 1.0048,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.6943,   \n",
            "loss_aug_tea: 0.9376,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0695,   \n",
            "loss_ori_tea: 1.1216,   \n",
            "loss_aug_tea: 0.8396,   \n",
            "Epoch [03/15],   Mean Train Loss: 0.3548,   Mean Train Dice: 0.9324,   Mean Val Dice: 0.9716,   Mean Val ASD: 0.0016,   \n",
            "loss_cls: 0.0502,   \n",
            "loss_aug: 0.0503,   \n",
            "loss_ori_tea: 0.8787,   \n",
            "loss_aug_tea: 0.8185,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0663,   \n",
            "loss_ori_tea: 0.6381,   \n",
            "loss_aug_tea: 0.7982,   \n",
            "loss_cls: 0.0585,   \n",
            "loss_aug: 0.0586,   \n",
            "loss_ori_tea: 0.6437,   \n",
            "loss_aug_tea: 0.9704,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0701,   \n",
            "loss_ori_tea: 0.7622,   \n",
            "loss_aug_tea: 0.7387,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.9509,   \n",
            "loss_aug_tea: 0.8795,   \n",
            "loss_cls: 0.0672,   \n",
            "loss_aug: 0.0672,   \n",
            "loss_ori_tea: 0.9830,   \n",
            "loss_aug_tea: 0.7966,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 0.6020,   \n",
            "loss_aug_tea: 1.3223,   \n",
            "loss_cls: 0.0660,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 0.5577,   \n",
            "loss_aug_tea: 1.0039,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 0.7376,   \n",
            "loss_aug_tea: 0.7073,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 0.7248,   \n",
            "loss_aug_tea: 1.1938,   \n",
            "loss_cls: 0.0776,   \n",
            "loss_aug: 0.0778,   \n",
            "loss_ori_tea: 0.5729,   \n",
            "loss_aug_tea: 0.8250,   \n",
            "loss_cls: 0.0559,   \n",
            "loss_aug: 0.0561,   \n",
            "loss_ori_tea: 0.5118,   \n",
            "loss_aug_tea: 1.2135,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.6328,   \n",
            "loss_aug_tea: 0.9933,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0663,   \n",
            "loss_ori_tea: 0.7798,   \n",
            "loss_aug_tea: 0.7919,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 0.7148,   \n",
            "loss_aug_tea: 0.9482,   \n",
            "loss_cls: 0.0608,   \n",
            "loss_aug: 0.0613,   \n",
            "loss_ori_tea: 0.8742,   \n",
            "loss_aug_tea: 0.9292,   \n",
            "loss_cls: 0.0547,   \n",
            "loss_aug: 0.0548,   \n",
            "loss_ori_tea: 0.5860,   \n",
            "loss_aug_tea: 0.9095,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 0.7212,   \n",
            "loss_aug_tea: 0.8543,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.7587,   \n",
            "loss_aug_tea: 0.8037,   \n",
            "loss_cls: 0.0603,   \n",
            "loss_aug: 0.0604,   \n",
            "loss_ori_tea: 0.7158,   \n",
            "loss_aug_tea: 0.8536,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0746,   \n",
            "loss_ori_tea: 1.1967,   \n",
            "loss_aug_tea: 0.5657,   \n",
            "loss_cls: 0.0542,   \n",
            "loss_aug: 0.0543,   \n",
            "loss_ori_tea: 0.5537,   \n",
            "loss_aug_tea: 0.9941,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.5709,   \n",
            "loss_aug_tea: 0.8114,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.7892,   \n",
            "loss_aug_tea: 0.7136,   \n",
            "loss_cls: 0.0763,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 0.6206,   \n",
            "loss_aug_tea: 0.9796,   \n",
            "loss_cls: 0.0621,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 1.0091,   \n",
            "loss_aug_tea: 0.6851,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 1.0716,   \n",
            "loss_aug_tea: 0.9452,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0707,   \n",
            "loss_ori_tea: 0.6097,   \n",
            "loss_aug_tea: 1.0994,   \n",
            "loss_cls: 0.0809,   \n",
            "loss_aug: 0.0811,   \n",
            "loss_ori_tea: 0.7506,   \n",
            "loss_aug_tea: 0.9208,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8771,   \n",
            "loss_aug_tea: 0.8259,   \n",
            "loss_cls: 0.0611,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 0.7512,   \n",
            "loss_aug_tea: 0.7474,   \n",
            "loss_cls: 0.0621,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 1.1959,   \n",
            "loss_aug_tea: 1.0106,   \n",
            "loss_cls: 0.0603,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.8139,   \n",
            "loss_aug_tea: 0.9220,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 0.8783,   \n",
            "loss_aug_tea: 0.9089,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0648,   \n",
            "loss_ori_tea: 0.7657,   \n",
            "loss_aug_tea: 0.8279,   \n",
            "loss_cls: 0.0539,   \n",
            "loss_aug: 0.0542,   \n",
            "loss_ori_tea: 1.1411,   \n",
            "loss_aug_tea: 0.7186,   \n",
            "loss_cls: 0.0947,   \n",
            "loss_aug: 0.0948,   \n",
            "loss_ori_tea: 0.7083,   \n",
            "loss_aug_tea: 0.9193,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.6186,   \n",
            "loss_aug_tea: 1.1366,   \n",
            "loss_cls: 0.0764,   \n",
            "loss_aug: 0.0766,   \n",
            "loss_ori_tea: 0.8146,   \n",
            "loss_aug_tea: 0.8369,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 1.3659,   \n",
            "loss_aug_tea: 0.6705,   \n",
            "loss_cls: 0.0509,   \n",
            "loss_aug: 0.0510,   \n",
            "loss_ori_tea: 0.6669,   \n",
            "loss_aug_tea: 0.9309,   \n",
            "loss_cls: 0.0723,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.7286,   \n",
            "loss_aug_tea: 0.9249,   \n",
            "loss_cls: 0.0885,   \n",
            "loss_aug: 0.0886,   \n",
            "loss_ori_tea: 0.6564,   \n",
            "loss_aug_tea: 0.8031,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.6698,   \n",
            "loss_aug_tea: 0.9549,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 0.8204,   \n",
            "loss_aug_tea: 0.7585,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 0.6467,   \n",
            "loss_aug_tea: 0.9320,   \n",
            "loss_cls: 0.0777,   \n",
            "loss_aug: 0.0776,   \n",
            "loss_ori_tea: 0.8049,   \n",
            "loss_aug_tea: 0.7618,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.8447,   \n",
            "loss_aug_tea: 0.5603,   \n",
            "loss_cls: 0.0577,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.7043,   \n",
            "loss_aug_tea: 0.6439,   \n",
            "loss_cls: 0.0483,   \n",
            "loss_aug: 0.0485,   \n",
            "loss_ori_tea: 0.7416,   \n",
            "loss_aug_tea: 1.0170,   \n",
            "loss_cls: 0.0628,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 0.9734,   \n",
            "loss_aug_tea: 0.5742,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.7876,   \n",
            "loss_aug_tea: 0.5243,   \n",
            "loss_cls: 0.0859,   \n",
            "loss_aug: 0.0860,   \n",
            "loss_ori_tea: 0.6434,   \n",
            "loss_aug_tea: 0.7486,   \n",
            "loss_cls: 0.0644,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.5715,   \n",
            "loss_aug_tea: 0.7969,   \n",
            "loss_cls: 0.0618,   \n",
            "loss_aug: 0.0620,   \n",
            "loss_ori_tea: 0.6480,   \n",
            "loss_aug_tea: 0.6593,   \n",
            "loss_cls: 0.0580,   \n",
            "loss_aug: 0.0582,   \n",
            "loss_ori_tea: 0.6791,   \n",
            "loss_aug_tea: 0.8093,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0601,   \n",
            "loss_ori_tea: 0.8277,   \n",
            "loss_aug_tea: 0.8163,   \n",
            "loss_cls: 0.0514,   \n",
            "loss_aug: 0.0518,   \n",
            "loss_ori_tea: 0.6600,   \n",
            "loss_aug_tea: 0.6432,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0634,   \n",
            "loss_ori_tea: 1.0761,   \n",
            "loss_aug_tea: 0.9813,   \n",
            "loss_cls: 0.0669,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.6129,   \n",
            "loss_aug_tea: 0.8109,   \n",
            "loss_cls: 0.0575,   \n",
            "loss_aug: 0.0576,   \n",
            "loss_ori_tea: 0.6053,   \n",
            "loss_aug_tea: 0.9451,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.8960,   \n",
            "loss_aug_tea: 0.9360,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0649,   \n",
            "loss_ori_tea: 0.5169,   \n",
            "loss_aug_tea: 1.3418,   \n",
            "loss_cls: 0.0729,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 1.3054,   \n",
            "loss_aug_tea: 0.6501,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.7816,   \n",
            "loss_aug_tea: 0.8139,   \n",
            "loss_cls: 0.0708,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 0.8240,   \n",
            "loss_aug_tea: 0.8346,   \n",
            "Epoch [04/15],   Mean Train Loss: 0.7051,   Mean Train Dice: 0.9338,   Mean Val Dice: 0.9625,   Mean Val ASD: 0.0019,   \n",
            "loss_cls: 0.0720,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.6706,   \n",
            "loss_aug_tea: 0.8214,   \n",
            "loss_cls: 0.1012,   \n",
            "loss_aug: 0.1017,   \n",
            "loss_ori_tea: 0.7103,   \n",
            "loss_aug_tea: 0.7408,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0641,   \n",
            "loss_ori_tea: 0.9621,   \n",
            "loss_aug_tea: 0.5873,   \n",
            "loss_cls: 0.0565,   \n",
            "loss_aug: 0.0566,   \n",
            "loss_ori_tea: 0.6367,   \n",
            "loss_aug_tea: 0.8243,   \n",
            "loss_cls: 0.0483,   \n",
            "loss_aug: 0.0485,   \n",
            "loss_ori_tea: 0.8004,   \n",
            "loss_aug_tea: 0.6505,   \n",
            "loss_cls: 0.0674,   \n",
            "loss_aug: 0.0677,   \n",
            "loss_ori_tea: 0.7081,   \n",
            "loss_aug_tea: 0.8622,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0712,   \n",
            "loss_ori_tea: 0.6337,   \n",
            "loss_aug_tea: 0.8394,   \n",
            "loss_cls: 0.0784,   \n",
            "loss_aug: 0.0787,   \n",
            "loss_ori_tea: 1.0767,   \n",
            "loss_aug_tea: 0.9729,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.5080,   \n",
            "loss_aug_tea: 0.9480,   \n",
            "loss_cls: 0.0725,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.5661,   \n",
            "loss_aug_tea: 0.8388,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0794,   \n",
            "loss_ori_tea: 0.5993,   \n",
            "loss_aug_tea: 0.7065,   \n",
            "loss_cls: 0.0771,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.9022,   \n",
            "loss_aug_tea: 0.8422,   \n",
            "loss_cls: 0.0578,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.8489,   \n",
            "loss_aug_tea: 0.8739,   \n",
            "loss_cls: 0.0695,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.6537,   \n",
            "loss_aug_tea: 0.7402,   \n",
            "loss_cls: 0.0708,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.9523,   \n",
            "loss_aug_tea: 1.2066,   \n",
            "loss_cls: 0.0730,   \n",
            "loss_aug: 0.0733,   \n",
            "loss_ori_tea: 0.7444,   \n",
            "loss_aug_tea: 0.7995,   \n",
            "loss_cls: 0.0591,   \n",
            "loss_aug: 0.0595,   \n",
            "loss_ori_tea: 0.6916,   \n",
            "loss_aug_tea: 0.8711,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0651,   \n",
            "loss_ori_tea: 0.5822,   \n",
            "loss_aug_tea: 1.0552,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0659,   \n",
            "loss_ori_tea: 0.5874,   \n",
            "loss_aug_tea: 1.0638,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.7343,   \n",
            "loss_aug_tea: 0.8795,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.8215,   \n",
            "loss_aug_tea: 0.7888,   \n",
            "loss_cls: 0.0789,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 0.8621,   \n",
            "loss_aug_tea: 0.7702,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.8397,   \n",
            "loss_aug_tea: 0.9362,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0599,   \n",
            "loss_ori_tea: 0.7882,   \n",
            "loss_aug_tea: 0.7866,   \n",
            "loss_cls: 0.0586,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.7216,   \n",
            "loss_aug_tea: 0.9028,   \n",
            "loss_cls: 0.0556,   \n",
            "loss_aug: 0.0559,   \n",
            "loss_ori_tea: 0.7416,   \n",
            "loss_aug_tea: 0.8549,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0614,   \n",
            "loss_ori_tea: 0.7146,   \n",
            "loss_aug_tea: 0.8020,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0681,   \n",
            "loss_ori_tea: 0.8195,   \n",
            "loss_aug_tea: 1.3851,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0729,   \n",
            "loss_ori_tea: 0.6148,   \n",
            "loss_aug_tea: 0.7993,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0688,   \n",
            "loss_ori_tea: 0.6308,   \n",
            "loss_aug_tea: 1.1813,   \n",
            "loss_cls: 0.0793,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 0.6485,   \n",
            "loss_aug_tea: 1.0294,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.8286,   \n",
            "loss_aug_tea: 0.6938,   \n",
            "loss_cls: 0.0815,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 0.7543,   \n",
            "loss_aug_tea: 0.8423,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 0.5229,   \n",
            "loss_aug_tea: 1.1087,   \n",
            "loss_cls: 0.0669,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.6595,   \n",
            "loss_aug_tea: 0.6704,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.6389,   \n",
            "loss_aug_tea: 0.7676,   \n",
            "loss_cls: 0.0666,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.6330,   \n",
            "loss_aug_tea: 0.8201,   \n",
            "loss_cls: 0.0798,   \n",
            "loss_aug: 0.0802,   \n",
            "loss_ori_tea: 0.7191,   \n",
            "loss_aug_tea: 0.7570,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.6981,   \n",
            "loss_aug_tea: 0.8786,   \n",
            "loss_cls: 0.0839,   \n",
            "loss_aug: 0.0841,   \n",
            "loss_ori_tea: 0.6856,   \n",
            "loss_aug_tea: 0.8682,   \n",
            "loss_cls: 0.0642,   \n",
            "loss_aug: 0.0644,   \n",
            "loss_ori_tea: 0.7290,   \n",
            "loss_aug_tea: 0.8317,   \n",
            "loss_cls: 0.0648,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 0.6489,   \n",
            "loss_aug_tea: 0.6909,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.7422,   \n",
            "loss_aug_tea: 0.7209,   \n",
            "loss_cls: 0.0716,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.7240,   \n",
            "loss_aug_tea: 0.7586,   \n",
            "loss_cls: 0.0775,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 0.6565,   \n",
            "loss_aug_tea: 0.9595,   \n",
            "loss_cls: 0.0716,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.6639,   \n",
            "loss_aug_tea: 0.9883,   \n",
            "loss_cls: 0.0895,   \n",
            "loss_aug: 0.0897,   \n",
            "loss_ori_tea: 0.6897,   \n",
            "loss_aug_tea: 0.7911,   \n",
            "loss_cls: 0.0739,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.6842,   \n",
            "loss_aug_tea: 0.7767,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 0.8437,   \n",
            "loss_aug_tea: 0.7808,   \n",
            "loss_cls: 0.0689,   \n",
            "loss_aug: 0.0690,   \n",
            "loss_ori_tea: 0.6430,   \n",
            "loss_aug_tea: 0.7391,   \n",
            "loss_cls: 0.0613,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 1.1114,   \n",
            "loss_aug_tea: 0.7591,   \n",
            "loss_cls: 0.0756,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.5847,   \n",
            "loss_aug_tea: 0.9407,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0713,   \n",
            "loss_ori_tea: 0.6878,   \n",
            "loss_aug_tea: 0.7074,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.6097,   \n",
            "loss_aug_tea: 1.0193,   \n",
            "loss_cls: 0.0672,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.6153,   \n",
            "loss_aug_tea: 0.7737,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.6310,   \n",
            "loss_aug_tea: 0.7263,   \n",
            "loss_cls: 0.0766,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.6244,   \n",
            "loss_aug_tea: 0.8602,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.3458,   \n",
            "loss_aug_tea: 0.5562,   \n",
            "loss_cls: 0.0808,   \n",
            "loss_aug: 0.0810,   \n",
            "loss_ori_tea: 0.5744,   \n",
            "loss_aug_tea: 0.9081,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 0.5945,   \n",
            "loss_aug_tea: 0.8679,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0714,   \n",
            "loss_ori_tea: 0.9581,   \n",
            "loss_aug_tea: 1.1703,   \n",
            "loss_cls: 0.0913,   \n",
            "loss_aug: 0.0915,   \n",
            "loss_ori_tea: 0.6280,   \n",
            "loss_aug_tea: 0.9844,   \n",
            "loss_cls: 0.1003,   \n",
            "loss_aug: 0.1007,   \n",
            "loss_ori_tea: 1.1800,   \n",
            "loss_aug_tea: 1.2569,   \n",
            "loss_cls: 0.0628,   \n",
            "loss_aug: 0.0628,   \n",
            "loss_ori_tea: 0.9072,   \n",
            "loss_aug_tea: 0.7360,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0743,   \n",
            "loss_ori_tea: 0.9934,   \n",
            "loss_aug_tea: 1.0900,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.7210,   \n",
            "loss_aug_tea: 1.0137,   \n",
            "Epoch [05/15],   Mean Train Loss: 1.3519,   Mean Train Dice: 0.9289,   Mean Val Dice: 0.9612,   Mean Val ASD: 0.0016,   \n",
            "loss_cls: 0.0778,   \n",
            "loss_aug: 0.0780,   \n",
            "loss_ori_tea: 0.5929,   \n",
            "loss_aug_tea: 1.2775,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0686,   \n",
            "loss_ori_tea: 0.5375,   \n",
            "loss_aug_tea: 1.3989,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 0.9724,   \n",
            "loss_aug_tea: 0.9616,   \n",
            "loss_cls: 0.0837,   \n",
            "loss_aug: 0.0840,   \n",
            "loss_ori_tea: 0.8556,   \n",
            "loss_aug_tea: 0.8555,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 1.3243,   \n",
            "loss_aug_tea: 0.6322,   \n",
            "loss_cls: 0.0609,   \n",
            "loss_aug: 0.0612,   \n",
            "loss_ori_tea: 1.0243,   \n",
            "loss_aug_tea: 0.7350,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0637,   \n",
            "loss_ori_tea: 0.9486,   \n",
            "loss_aug_tea: 0.8280,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0808,   \n",
            "loss_ori_tea: 0.9294,   \n",
            "loss_aug_tea: 0.7653,   \n",
            "loss_cls: 0.0793,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 0.6918,   \n",
            "loss_aug_tea: 0.9069,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 0.8479,   \n",
            "loss_aug_tea: 0.7923,   \n",
            "loss_cls: 0.0620,   \n",
            "loss_aug: 0.0619,   \n",
            "loss_ori_tea: 0.9302,   \n",
            "loss_aug_tea: 0.5924,   \n",
            "loss_cls: 0.0600,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 0.7649,   \n",
            "loss_aug_tea: 0.8216,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0691,   \n",
            "loss_ori_tea: 0.8072,   \n",
            "loss_aug_tea: 0.7710,   \n",
            "loss_cls: 0.0862,   \n",
            "loss_aug: 0.0862,   \n",
            "loss_ori_tea: 0.6044,   \n",
            "loss_aug_tea: 0.9787,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.5768,   \n",
            "loss_aug_tea: 1.0871,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0604,   \n",
            "loss_ori_tea: 0.7433,   \n",
            "loss_aug_tea: 0.8055,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.5454,   \n",
            "loss_aug_tea: 1.0154,   \n",
            "loss_cls: 0.0779,   \n",
            "loss_aug: 0.0780,   \n",
            "loss_ori_tea: 0.5202,   \n",
            "loss_aug_tea: 0.7887,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0764,   \n",
            "loss_ori_tea: 0.6838,   \n",
            "loss_aug_tea: 1.0611,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0764,   \n",
            "loss_ori_tea: 1.0092,   \n",
            "loss_aug_tea: 0.9284,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 1.1432,   \n",
            "loss_aug_tea: 0.6988,   \n",
            "loss_cls: 0.0758,   \n",
            "loss_aug: 0.0761,   \n",
            "loss_ori_tea: 0.6585,   \n",
            "loss_aug_tea: 0.8457,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 0.6641,   \n",
            "loss_aug_tea: 0.7864,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0764,   \n",
            "loss_ori_tea: 0.6496,   \n",
            "loss_aug_tea: 0.6617,   \n",
            "loss_cls: 0.0617,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.5891,   \n",
            "loss_aug_tea: 0.7808,   \n",
            "loss_cls: 0.0549,   \n",
            "loss_aug: 0.0550,   \n",
            "loss_ori_tea: 0.6343,   \n",
            "loss_aug_tea: 1.4138,   \n",
            "loss_cls: 0.0788,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 0.7207,   \n",
            "loss_aug_tea: 0.6981,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0697,   \n",
            "loss_ori_tea: 0.6880,   \n",
            "loss_aug_tea: 0.8073,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0820,   \n",
            "loss_ori_tea: 0.7651,   \n",
            "loss_aug_tea: 0.9376,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0625,   \n",
            "loss_ori_tea: 0.6122,   \n",
            "loss_aug_tea: 1.0369,   \n",
            "loss_cls: 0.0646,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.7977,   \n",
            "loss_aug_tea: 1.0741,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.5730,   \n",
            "loss_aug_tea: 0.8388,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0781,   \n",
            "loss_ori_tea: 0.6298,   \n",
            "loss_aug_tea: 0.7323,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.6934,   \n",
            "loss_aug_tea: 0.6822,   \n",
            "loss_cls: 0.0572,   \n",
            "loss_aug: 0.0572,   \n",
            "loss_ori_tea: 0.6797,   \n",
            "loss_aug_tea: 0.8216,   \n",
            "loss_cls: 0.0754,   \n",
            "loss_aug: 0.0755,   \n",
            "loss_ori_tea: 0.7598,   \n",
            "loss_aug_tea: 0.9774,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.8024,   \n",
            "loss_aug_tea: 0.9274,   \n",
            "loss_cls: 0.0572,   \n",
            "loss_aug: 0.0574,   \n",
            "loss_ori_tea: 1.1073,   \n",
            "loss_aug_tea: 0.9719,   \n",
            "loss_cls: 0.0744,   \n",
            "loss_aug: 0.0746,   \n",
            "loss_ori_tea: 0.7406,   \n",
            "loss_aug_tea: 1.0636,   \n",
            "loss_cls: 0.0748,   \n",
            "loss_aug: 0.0751,   \n",
            "loss_ori_tea: 0.6768,   \n",
            "loss_aug_tea: 0.9262,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 0.7083,   \n",
            "loss_aug_tea: 0.7245,   \n",
            "loss_cls: 0.0800,   \n",
            "loss_aug: 0.0802,   \n",
            "loss_ori_tea: 0.9697,   \n",
            "loss_aug_tea: 1.1979,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 0.8379,   \n",
            "loss_aug_tea: 0.8247,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0808,   \n",
            "loss_ori_tea: 0.9585,   \n",
            "loss_aug_tea: 1.0158,   \n",
            "loss_cls: 0.0881,   \n",
            "loss_aug: 0.0883,   \n",
            "loss_ori_tea: 0.5910,   \n",
            "loss_aug_tea: 0.9632,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 0.5653,   \n",
            "loss_aug_tea: 1.0867,   \n",
            "loss_cls: 0.0785,   \n",
            "loss_aug: 0.0787,   \n",
            "loss_ori_tea: 0.7862,   \n",
            "loss_aug_tea: 0.8120,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.7211,   \n",
            "loss_aug_tea: 0.8004,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.5821,   \n",
            "loss_aug_tea: 0.9229,   \n",
            "loss_cls: 0.0641,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.6679,   \n",
            "loss_aug_tea: 0.8789,   \n",
            "loss_cls: 0.0689,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.7719,   \n",
            "loss_aug_tea: 0.7655,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.8381,   \n",
            "loss_aug_tea: 1.1455,   \n",
            "loss_cls: 0.0752,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.7966,   \n",
            "loss_aug_tea: 0.7670,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.5995,   \n",
            "loss_aug_tea: 1.0949,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.6174,   \n",
            "loss_aug_tea: 1.0168,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 0.6624,   \n",
            "loss_aug_tea: 0.9587,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0690,   \n",
            "loss_ori_tea: 0.9249,   \n",
            "loss_aug_tea: 0.8214,   \n",
            "loss_cls: 0.0757,   \n",
            "loss_aug: 0.0760,   \n",
            "loss_ori_tea: 0.8874,   \n",
            "loss_aug_tea: 0.8631,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.7645,   \n",
            "loss_aug_tea: 0.9473,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0625,   \n",
            "loss_ori_tea: 0.8675,   \n",
            "loss_aug_tea: 0.9666,   \n",
            "loss_cls: 0.0771,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.8510,   \n",
            "loss_aug_tea: 0.6965,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.6622,   \n",
            "loss_aug_tea: 0.8700,   \n",
            "loss_cls: 0.0885,   \n",
            "loss_aug: 0.0887,   \n",
            "loss_ori_tea: 0.7890,   \n",
            "loss_aug_tea: 0.7382,   \n",
            "loss_cls: 0.0893,   \n",
            "loss_aug: 0.0895,   \n",
            "loss_ori_tea: 0.7672,   \n",
            "loss_aug_tea: 0.8273,   \n",
            "loss_cls: 0.0772,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.9053,   \n",
            "loss_aug_tea: 0.8121,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0741,   \n",
            "loss_ori_tea: 0.7106,   \n",
            "loss_aug_tea: 0.9247,   \n",
            "Epoch [06/15],   Mean Train Loss: 2.2766,   Mean Train Dice: 0.9281,   Mean Val Dice: 0.9642,   Mean Val ASD: 0.0031,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0730,   \n",
            "loss_ori_tea: 0.6255,   \n",
            "loss_aug_tea: 1.0024,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0697,   \n",
            "loss_ori_tea: 0.7056,   \n",
            "loss_aug_tea: 0.8029,   \n",
            "loss_cls: 0.0696,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.5802,   \n",
            "loss_aug_tea: 1.1521,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.8564,   \n",
            "loss_aug_tea: 0.8666,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.6551,   \n",
            "loss_aug_tea: 0.9732,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0701,   \n",
            "loss_ori_tea: 0.6098,   \n",
            "loss_aug_tea: 0.8905,   \n",
            "loss_cls: 0.0757,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.7710,   \n",
            "loss_aug_tea: 0.7695,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 0.7215,   \n",
            "loss_aug_tea: 1.0641,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 0.7361,   \n",
            "loss_aug_tea: 0.8118,   \n",
            "loss_cls: 0.0829,   \n",
            "loss_aug: 0.0830,   \n",
            "loss_ori_tea: 0.7857,   \n",
            "loss_aug_tea: 0.8279,   \n",
            "loss_cls: 0.0746,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 0.6907,   \n",
            "loss_aug_tea: 0.7374,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0845,   \n",
            "loss_ori_tea: 0.5495,   \n",
            "loss_aug_tea: 1.1827,   \n",
            "loss_cls: 0.0656,   \n",
            "loss_aug: 0.0657,   \n",
            "loss_ori_tea: 0.6011,   \n",
            "loss_aug_tea: 0.8951,   \n",
            "loss_cls: 0.0762,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 0.8429,   \n",
            "loss_aug_tea: 0.7421,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.6337,   \n",
            "loss_aug_tea: 0.8804,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.9693,   \n",
            "loss_aug_tea: 1.1802,   \n",
            "loss_cls: 0.0566,   \n",
            "loss_aug: 0.0568,   \n",
            "loss_ori_tea: 1.1241,   \n",
            "loss_aug_tea: 0.6400,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.7136,   \n",
            "loss_aug_tea: 0.8344,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0631,   \n",
            "loss_ori_tea: 0.5737,   \n",
            "loss_aug_tea: 1.0792,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0782,   \n",
            "loss_ori_tea: 0.8065,   \n",
            "loss_aug_tea: 0.8467,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.9856,   \n",
            "loss_aug_tea: 0.7568,   \n",
            "loss_cls: 0.0696,   \n",
            "loss_aug: 0.0699,   \n",
            "loss_ori_tea: 0.6162,   \n",
            "loss_aug_tea: 0.8311,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0790,   \n",
            "loss_ori_tea: 0.6948,   \n",
            "loss_aug_tea: 0.7494,   \n",
            "loss_cls: 0.0590,   \n",
            "loss_aug: 0.0592,   \n",
            "loss_ori_tea: 1.1959,   \n",
            "loss_aug_tea: 0.9299,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.9440,   \n",
            "loss_aug_tea: 0.9962,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 0.8756,   \n",
            "loss_aug_tea: 0.8672,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0797,   \n",
            "loss_ori_tea: 0.8392,   \n",
            "loss_aug_tea: 0.7879,   \n",
            "loss_cls: 0.0803,   \n",
            "loss_aug: 0.0805,   \n",
            "loss_ori_tea: 0.7747,   \n",
            "loss_aug_tea: 0.7781,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.9200,   \n",
            "loss_aug_tea: 0.9554,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 1.0872,   \n",
            "loss_aug_tea: 0.9101,   \n",
            "loss_cls: 0.0489,   \n",
            "loss_aug: 0.0492,   \n",
            "loss_ori_tea: 0.7001,   \n",
            "loss_aug_tea: 0.6784,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0761,   \n",
            "loss_ori_tea: 0.6608,   \n",
            "loss_aug_tea: 0.9298,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0629,   \n",
            "loss_ori_tea: 0.7116,   \n",
            "loss_aug_tea: 0.8371,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0645,   \n",
            "loss_ori_tea: 0.7423,   \n",
            "loss_aug_tea: 0.6899,   \n",
            "loss_cls: 0.0709,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.9357,   \n",
            "loss_aug_tea: 0.7843,   \n",
            "loss_cls: 0.0850,   \n",
            "loss_aug: 0.0853,   \n",
            "loss_ori_tea: 0.7133,   \n",
            "loss_aug_tea: 0.7956,   \n",
            "loss_cls: 0.0733,   \n",
            "loss_aug: 0.0735,   \n",
            "loss_ori_tea: 0.7543,   \n",
            "loss_aug_tea: 0.7980,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0659,   \n",
            "loss_ori_tea: 0.6599,   \n",
            "loss_aug_tea: 1.0396,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0730,   \n",
            "loss_ori_tea: 0.8359,   \n",
            "loss_aug_tea: 1.0623,   \n",
            "loss_cls: 0.0588,   \n",
            "loss_aug: 0.0591,   \n",
            "loss_ori_tea: 0.8258,   \n",
            "loss_aug_tea: 0.9074,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 0.6776,   \n",
            "loss_aug_tea: 0.6922,   \n",
            "loss_cls: 0.0642,   \n",
            "loss_aug: 0.0644,   \n",
            "loss_ori_tea: 0.6283,   \n",
            "loss_aug_tea: 0.8796,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.6678,   \n",
            "loss_aug_tea: 0.6994,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 1.2448,   \n",
            "loss_aug_tea: 1.3365,   \n",
            "loss_cls: 0.0898,   \n",
            "loss_aug: 0.0900,   \n",
            "loss_ori_tea: 0.7136,   \n",
            "loss_aug_tea: 0.7524,   \n",
            "loss_cls: 0.0663,   \n",
            "loss_aug: 0.0664,   \n",
            "loss_ori_tea: 0.9133,   \n",
            "loss_aug_tea: 0.6804,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0700,   \n",
            "loss_ori_tea: 0.6685,   \n",
            "loss_aug_tea: 0.8049,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.9181,   \n",
            "loss_aug_tea: 1.2270,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.7191,   \n",
            "loss_aug_tea: 0.9390,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 0.9067,   \n",
            "loss_aug_tea: 1.1219,   \n",
            "loss_cls: 0.0784,   \n",
            "loss_aug: 0.0785,   \n",
            "loss_ori_tea: 0.6965,   \n",
            "loss_aug_tea: 0.7197,   \n",
            "loss_cls: 0.0890,   \n",
            "loss_aug: 0.0892,   \n",
            "loss_ori_tea: 0.6410,   \n",
            "loss_aug_tea: 0.7944,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 1.0311,   \n",
            "loss_aug_tea: 0.6414,   \n",
            "loss_cls: 0.0843,   \n",
            "loss_aug: 0.0845,   \n",
            "loss_ori_tea: 0.6254,   \n",
            "loss_aug_tea: 0.8983,   \n",
            "loss_cls: 0.0620,   \n",
            "loss_aug: 0.0620,   \n",
            "loss_ori_tea: 0.7833,   \n",
            "loss_aug_tea: 0.8182,   \n",
            "loss_cls: 0.0596,   \n",
            "loss_aug: 0.0596,   \n",
            "loss_ori_tea: 1.1609,   \n",
            "loss_aug_tea: 0.9423,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0637,   \n",
            "loss_ori_tea: 0.7196,   \n",
            "loss_aug_tea: 0.8813,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 1.0526,   \n",
            "loss_aug_tea: 0.9752,   \n",
            "loss_cls: 0.0790,   \n",
            "loss_aug: 0.0792,   \n",
            "loss_ori_tea: 0.8689,   \n",
            "loss_aug_tea: 1.1126,   \n",
            "loss_cls: 0.0755,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.7330,   \n",
            "loss_aug_tea: 0.8899,   \n",
            "loss_cls: 0.0606,   \n",
            "loss_aug: 0.0609,   \n",
            "loss_ori_tea: 0.7693,   \n",
            "loss_aug_tea: 0.8511,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 0.7103,   \n",
            "loss_aug_tea: 0.8829,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.6897,   \n",
            "loss_aug_tea: 0.7009,   \n",
            "loss_cls: 0.0863,   \n",
            "loss_aug: 0.0865,   \n",
            "loss_ori_tea: 0.6871,   \n",
            "loss_aug_tea: 0.9340,   \n",
            "loss_cls: 0.0715,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.8792,   \n",
            "loss_aug_tea: 0.8199,   \n",
            "loss_cls: 0.0873,   \n",
            "loss_aug: 0.0874,   \n",
            "loss_ori_tea: 0.8371,   \n",
            "loss_aug_tea: 0.8460,   \n",
            "Epoch [07/15],   Mean Train Loss: 3.0836,   Mean Train Dice: 0.9283,   Mean Val Dice: 0.9618,   Mean Val ASD: 0.0019,   \n",
            "loss_cls: 0.0586,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.7097,   \n",
            "loss_aug_tea: 0.8795,   \n",
            "loss_cls: 0.0790,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 1.1243,   \n",
            "loss_aug_tea: 0.9707,   \n",
            "loss_cls: 0.0731,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.9678,   \n",
            "loss_aug_tea: 0.8183,   \n",
            "loss_cls: 0.0761,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 0.9374,   \n",
            "loss_aug_tea: 0.7168,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.8586,   \n",
            "loss_aug_tea: 0.9121,   \n",
            "loss_cls: 0.0572,   \n",
            "loss_aug: 0.0573,   \n",
            "loss_ori_tea: 0.7385,   \n",
            "loss_aug_tea: 0.9277,   \n",
            "loss_cls: 0.0652,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 0.9918,   \n",
            "loss_aug_tea: 0.8640,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.6263,   \n",
            "loss_aug_tea: 1.0810,   \n",
            "loss_cls: 0.0835,   \n",
            "loss_aug: 0.0836,   \n",
            "loss_ori_tea: 0.8705,   \n",
            "loss_aug_tea: 0.8577,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0724,   \n",
            "loss_ori_tea: 0.6431,   \n",
            "loss_aug_tea: 0.8868,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0768,   \n",
            "loss_ori_tea: 1.0496,   \n",
            "loss_aug_tea: 0.9667,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.7789,   \n",
            "loss_aug_tea: 0.7424,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 0.7174,   \n",
            "loss_aug_tea: 0.9031,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0666,   \n",
            "loss_ori_tea: 0.6773,   \n",
            "loss_aug_tea: 0.7778,   \n",
            "loss_cls: 0.0709,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.8350,   \n",
            "loss_aug_tea: 0.8576,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0807,   \n",
            "loss_ori_tea: 1.0250,   \n",
            "loss_aug_tea: 0.9808,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0693,   \n",
            "loss_ori_tea: 0.6562,   \n",
            "loss_aug_tea: 0.7434,   \n",
            "loss_cls: 0.0819,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 0.7158,   \n",
            "loss_aug_tea: 0.9665,   \n",
            "loss_cls: 0.0494,   \n",
            "loss_aug: 0.0495,   \n",
            "loss_ori_tea: 1.1400,   \n",
            "loss_aug_tea: 0.7505,   \n",
            "loss_cls: 0.0716,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 0.7421,   \n",
            "loss_aug_tea: 0.8554,   \n",
            "loss_cls: 0.0651,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 1.0146,   \n",
            "loss_aug_tea: 0.9749,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.8169,   \n",
            "loss_aug_tea: 0.8486,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0731,   \n",
            "loss_ori_tea: 0.9473,   \n",
            "loss_aug_tea: 0.7596,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.7106,   \n",
            "loss_aug_tea: 0.9798,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0648,   \n",
            "loss_ori_tea: 0.8426,   \n",
            "loss_aug_tea: 0.7379,   \n",
            "loss_cls: 0.0845,   \n",
            "loss_aug: 0.0848,   \n",
            "loss_ori_tea: 1.0049,   \n",
            "loss_aug_tea: 0.8391,   \n",
            "loss_cls: 0.0951,   \n",
            "loss_aug: 0.0953,   \n",
            "loss_ori_tea: 0.7304,   \n",
            "loss_aug_tea: 0.8965,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0866,   \n",
            "loss_ori_tea: 0.7317,   \n",
            "loss_aug_tea: 0.8404,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0707,   \n",
            "loss_ori_tea: 0.7725,   \n",
            "loss_aug_tea: 0.7103,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 1.0721,   \n",
            "loss_aug_tea: 1.0263,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0771,   \n",
            "loss_ori_tea: 0.9766,   \n",
            "loss_aug_tea: 1.3267,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 0.8538,   \n",
            "loss_aug_tea: 0.7994,   \n",
            "loss_cls: 0.0562,   \n",
            "loss_aug: 0.0565,   \n",
            "loss_ori_tea: 0.8456,   \n",
            "loss_aug_tea: 0.7485,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 1.2221,   \n",
            "loss_aug_tea: 0.6745,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 0.6866,   \n",
            "loss_aug_tea: 0.9189,   \n",
            "loss_cls: 0.0801,   \n",
            "loss_aug: 0.0803,   \n",
            "loss_ori_tea: 0.7278,   \n",
            "loss_aug_tea: 0.7663,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.8679,   \n",
            "loss_aug_tea: 0.7296,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.8916,   \n",
            "loss_aug_tea: 0.8065,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.6359,   \n",
            "loss_aug_tea: 1.0759,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.8247,   \n",
            "loss_aug_tea: 0.7512,   \n",
            "loss_cls: 0.0746,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.9332,   \n",
            "loss_aug_tea: 0.9435,   \n",
            "loss_cls: 0.0852,   \n",
            "loss_aug: 0.0854,   \n",
            "loss_ori_tea: 1.2204,   \n",
            "loss_aug_tea: 0.9528,   \n",
            "loss_cls: 0.0732,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.6838,   \n",
            "loss_aug_tea: 0.8506,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.7604,   \n",
            "loss_aug_tea: 0.9023,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 1.0570,   \n",
            "loss_aug_tea: 1.0579,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0806,   \n",
            "loss_ori_tea: 0.6434,   \n",
            "loss_aug_tea: 0.8464,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0751,   \n",
            "loss_ori_tea: 0.7351,   \n",
            "loss_aug_tea: 0.8761,   \n",
            "loss_cls: 0.0803,   \n",
            "loss_aug: 0.0805,   \n",
            "loss_ori_tea: 0.7927,   \n",
            "loss_aug_tea: 0.7729,   \n",
            "loss_cls: 0.0705,   \n",
            "loss_aug: 0.0706,   \n",
            "loss_ori_tea: 1.2437,   \n",
            "loss_aug_tea: 0.5994,   \n",
            "loss_cls: 0.0756,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.9527,   \n",
            "loss_aug_tea: 1.0038,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 0.7671,   \n",
            "loss_aug_tea: 0.8068,   \n",
            "loss_cls: 0.0937,   \n",
            "loss_aug: 0.0938,   \n",
            "loss_ori_tea: 0.6917,   \n",
            "loss_aug_tea: 0.8615,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.6946,   \n",
            "loss_aug_tea: 0.7987,   \n",
            "loss_cls: 0.0809,   \n",
            "loss_aug: 0.0811,   \n",
            "loss_ori_tea: 0.6712,   \n",
            "loss_aug_tea: 0.8856,   \n",
            "loss_cls: 0.0723,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 0.6826,   \n",
            "loss_aug_tea: 0.9504,   \n",
            "loss_cls: 0.0662,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.8085,   \n",
            "loss_aug_tea: 0.8889,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0795,   \n",
            "loss_ori_tea: 0.7133,   \n",
            "loss_aug_tea: 0.8261,   \n",
            "loss_cls: 0.0730,   \n",
            "loss_aug: 0.0733,   \n",
            "loss_ori_tea: 0.8648,   \n",
            "loss_aug_tea: 0.7163,   \n",
            "loss_cls: 0.0831,   \n",
            "loss_aug: 0.0834,   \n",
            "loss_ori_tea: 0.9258,   \n",
            "loss_aug_tea: 0.8461,   \n",
            "loss_cls: 0.0805,   \n",
            "loss_aug: 0.0807,   \n",
            "loss_ori_tea: 1.0914,   \n",
            "loss_aug_tea: 0.8550,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0846,   \n",
            "loss_ori_tea: 1.1687,   \n",
            "loss_aug_tea: 0.6845,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0785,   \n",
            "loss_ori_tea: 0.7689,   \n",
            "loss_aug_tea: 0.7799,   \n",
            "loss_cls: 0.0694,   \n",
            "loss_aug: 0.0698,   \n",
            "loss_ori_tea: 0.7466,   \n",
            "loss_aug_tea: 0.8235,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.7111,   \n",
            "loss_aug_tea: 0.8590,   \n",
            "loss_cls: 0.0581,   \n",
            "loss_aug: 0.0583,   \n",
            "loss_ori_tea: 0.7725,   \n",
            "loss_aug_tea: 0.8317,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.7901,   \n",
            "loss_aug_tea: 1.0153,   \n",
            "Epoch [08/15],   Mean Train Loss: 3.4907,   Mean Train Dice: 0.9268,   Mean Val Dice: 0.9618,   Mean Val ASD: 0.0032,   \n",
            "loss_cls: 0.0655,   \n",
            "loss_aug: 0.0656,   \n",
            "loss_ori_tea: 0.6451,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0852,   \n",
            "loss_aug: 0.0856,   \n",
            "loss_ori_tea: 0.7989,   \n",
            "loss_aug_tea: 0.8315,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.6727,   \n",
            "loss_aug_tea: 0.8547,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 0.6933,   \n",
            "loss_aug_tea: 0.8128,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.8351,   \n",
            "loss_aug_tea: 0.7817,   \n",
            "loss_cls: 0.0604,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.6655,   \n",
            "loss_aug_tea: 0.7620,   \n",
            "loss_cls: 0.0835,   \n",
            "loss_aug: 0.0838,   \n",
            "loss_ori_tea: 0.7267,   \n",
            "loss_aug_tea: 0.7619,   \n",
            "loss_cls: 0.0577,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.8745,   \n",
            "loss_aug_tea: 0.9897,   \n",
            "loss_cls: 0.0689,   \n",
            "loss_aug: 0.0691,   \n",
            "loss_ori_tea: 0.9383,   \n",
            "loss_aug_tea: 0.7347,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.8873,   \n",
            "loss_aug_tea: 0.9655,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.9758,   \n",
            "loss_aug_tea: 1.0247,   \n",
            "loss_cls: 0.0793,   \n",
            "loss_aug: 0.0794,   \n",
            "loss_ori_tea: 0.8230,   \n",
            "loss_aug_tea: 1.0221,   \n",
            "loss_cls: 0.0658,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.7398,   \n",
            "loss_aug_tea: 0.9522,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.8089,   \n",
            "loss_aug_tea: 0.7570,   \n",
            "loss_cls: 0.0856,   \n",
            "loss_aug: 0.0859,   \n",
            "loss_ori_tea: 0.8432,   \n",
            "loss_aug_tea: 0.8372,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0730,   \n",
            "loss_ori_tea: 0.7464,   \n",
            "loss_aug_tea: 0.9214,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 0.6870,   \n",
            "loss_aug_tea: 0.9769,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 1.2080,   \n",
            "loss_aug_tea: 1.2455,   \n",
            "loss_cls: 0.0660,   \n",
            "loss_aug: 0.0662,   \n",
            "loss_ori_tea: 0.7049,   \n",
            "loss_aug_tea: 0.8687,   \n",
            "loss_cls: 0.0733,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.6812,   \n",
            "loss_aug_tea: 0.9415,   \n",
            "loss_cls: 0.0595,   \n",
            "loss_aug: 0.0600,   \n",
            "loss_ori_tea: 1.0222,   \n",
            "loss_aug_tea: 0.7308,   \n",
            "loss_cls: 0.0702,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.7685,   \n",
            "loss_aug_tea: 1.0597,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.7073,   \n",
            "loss_aug_tea: 0.8510,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 0.9635,   \n",
            "loss_aug_tea: 0.9001,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 1.3123,   \n",
            "loss_aug_tea: 1.0495,   \n",
            "loss_cls: 0.0817,   \n",
            "loss_aug: 0.0818,   \n",
            "loss_ori_tea: 0.9623,   \n",
            "loss_aug_tea: 0.9882,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0848,   \n",
            "loss_ori_tea: 0.7284,   \n",
            "loss_aug_tea: 0.8130,   \n",
            "loss_cls: 0.0839,   \n",
            "loss_aug: 0.0841,   \n",
            "loss_ori_tea: 0.7778,   \n",
            "loss_aug_tea: 0.8999,   \n",
            "loss_cls: 0.0802,   \n",
            "loss_aug: 0.0806,   \n",
            "loss_ori_tea: 0.9487,   \n",
            "loss_aug_tea: 0.8071,   \n",
            "loss_cls: 0.0647,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 0.6680,   \n",
            "loss_aug_tea: 0.8161,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0666,   \n",
            "loss_ori_tea: 1.1420,   \n",
            "loss_aug_tea: 0.7820,   \n",
            "loss_cls: 0.0547,   \n",
            "loss_aug: 0.0550,   \n",
            "loss_ori_tea: 0.8528,   \n",
            "loss_aug_tea: 0.9178,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0769,   \n",
            "loss_ori_tea: 0.6966,   \n",
            "loss_aug_tea: 1.0523,   \n",
            "loss_cls: 0.0880,   \n",
            "loss_aug: 0.0882,   \n",
            "loss_ori_tea: 1.1824,   \n",
            "loss_aug_tea: 0.7344,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0693,   \n",
            "loss_ori_tea: 0.8098,   \n",
            "loss_aug_tea: 0.8579,   \n",
            "loss_cls: 0.0581,   \n",
            "loss_aug: 0.0582,   \n",
            "loss_ori_tea: 0.8480,   \n",
            "loss_aug_tea: 1.0287,   \n",
            "loss_cls: 0.0628,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 0.8206,   \n",
            "loss_aug_tea: 1.0680,   \n",
            "loss_cls: 0.0836,   \n",
            "loss_aug: 0.0839,   \n",
            "loss_ori_tea: 0.8029,   \n",
            "loss_aug_tea: 0.9438,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0609,   \n",
            "loss_ori_tea: 0.8544,   \n",
            "loss_aug_tea: 0.7459,   \n",
            "loss_cls: 0.0714,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.9773,   \n",
            "loss_aug_tea: 0.7210,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0741,   \n",
            "loss_ori_tea: 0.9754,   \n",
            "loss_aug_tea: 0.7383,   \n",
            "loss_cls: 0.0798,   \n",
            "loss_aug: 0.0799,   \n",
            "loss_ori_tea: 0.8361,   \n",
            "loss_aug_tea: 0.7158,   \n",
            "loss_cls: 0.0859,   \n",
            "loss_aug: 0.0861,   \n",
            "loss_ori_tea: 0.8879,   \n",
            "loss_aug_tea: 1.0198,   \n",
            "loss_cls: 0.0655,   \n",
            "loss_aug: 0.0658,   \n",
            "loss_ori_tea: 1.2484,   \n",
            "loss_aug_tea: 0.8343,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0638,   \n",
            "loss_ori_tea: 0.7948,   \n",
            "loss_aug_tea: 0.9001,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0821,   \n",
            "loss_ori_tea: 0.9537,   \n",
            "loss_aug_tea: 0.9862,   \n",
            "loss_cls: 0.0663,   \n",
            "loss_aug: 0.0666,   \n",
            "loss_ori_tea: 0.8499,   \n",
            "loss_aug_tea: 0.9989,   \n",
            "loss_cls: 0.0624,   \n",
            "loss_aug: 0.0626,   \n",
            "loss_ori_tea: 0.9341,   \n",
            "loss_aug_tea: 0.8881,   \n",
            "loss_cls: 0.0633,   \n",
            "loss_aug: 0.0636,   \n",
            "loss_ori_tea: 1.1718,   \n",
            "loss_aug_tea: 1.1592,   \n",
            "loss_cls: 0.0616,   \n",
            "loss_aug: 0.0618,   \n",
            "loss_ori_tea: 0.9391,   \n",
            "loss_aug_tea: 0.8399,   \n",
            "loss_cls: 0.0934,   \n",
            "loss_aug: 0.0936,   \n",
            "loss_ori_tea: 0.7929,   \n",
            "loss_aug_tea: 0.8734,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 0.7588,   \n",
            "loss_aug_tea: 0.9784,   \n",
            "loss_cls: 0.0665,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.8100,   \n",
            "loss_aug_tea: 0.8215,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.8842,   \n",
            "loss_aug_tea: 0.8676,   \n",
            "loss_cls: 0.0661,   \n",
            "loss_aug: 0.0665,   \n",
            "loss_ori_tea: 0.8001,   \n",
            "loss_aug_tea: 0.9712,   \n",
            "loss_cls: 0.0693,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 0.7711,   \n",
            "loss_aug_tea: 0.8800,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0771,   \n",
            "loss_ori_tea: 0.8541,   \n",
            "loss_aug_tea: 1.0938,   \n",
            "loss_cls: 0.0562,   \n",
            "loss_aug: 0.0563,   \n",
            "loss_ori_tea: 1.0415,   \n",
            "loss_aug_tea: 0.9299,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0677,   \n",
            "loss_ori_tea: 1.1624,   \n",
            "loss_aug_tea: 0.7702,   \n",
            "loss_cls: 0.0687,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.6745,   \n",
            "loss_aug_tea: 0.9955,   \n",
            "loss_cls: 0.0758,   \n",
            "loss_aug: 0.0760,   \n",
            "loss_ori_tea: 0.7406,   \n",
            "loss_aug_tea: 0.8806,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.6952,   \n",
            "loss_aug_tea: 0.9634,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 0.7422,   \n",
            "loss_aug_tea: 0.9082,   \n",
            "loss_cls: 0.0700,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.7840,   \n",
            "loss_aug_tea: 0.9802,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0708,   \n",
            "loss_ori_tea: 0.8548,   \n",
            "loss_aug_tea: 0.8985,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0629,   \n",
            "loss_ori_tea: 0.7363,   \n",
            "loss_aug_tea: 0.7827,   \n",
            "Epoch [09/15],   Mean Train Loss: 3.5850,   Mean Train Dice: 0.9293,   Mean Val Dice: 0.9616,   Mean Val ASD: 0.0028,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 0.6812,   \n",
            "loss_aug_tea: 0.9096,   \n",
            "loss_cls: 0.0702,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.8645,   \n",
            "loss_aug_tea: 0.8967,   \n",
            "loss_cls: 0.0880,   \n",
            "loss_aug: 0.0884,   \n",
            "loss_ori_tea: 0.7518,   \n",
            "loss_aug_tea: 0.8479,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0724,   \n",
            "loss_ori_tea: 0.9892,   \n",
            "loss_aug_tea: 0.7091,   \n",
            "loss_cls: 0.0827,   \n",
            "loss_aug: 0.0835,   \n",
            "loss_ori_tea: 0.8874,   \n",
            "loss_aug_tea: 1.0257,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 0.9711,   \n",
            "loss_aug_tea: 0.7995,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8106,   \n",
            "loss_aug_tea: 0.9688,   \n",
            "loss_cls: 0.0644,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.8751,   \n",
            "loss_aug_tea: 1.0608,   \n",
            "loss_cls: 0.0763,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 1.1173,   \n",
            "loss_aug_tea: 0.6881,   \n",
            "loss_cls: 0.0566,   \n",
            "loss_aug: 0.0569,   \n",
            "loss_ori_tea: 0.7604,   \n",
            "loss_aug_tea: 0.7930,   \n",
            "loss_cls: 0.0706,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 0.9708,   \n",
            "loss_aug_tea: 1.0221,   \n",
            "loss_cls: 0.0892,   \n",
            "loss_aug: 0.0894,   \n",
            "loss_ori_tea: 0.8754,   \n",
            "loss_aug_tea: 0.8896,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 1.0736,   \n",
            "loss_aug_tea: 1.1723,   \n",
            "loss_cls: 0.0812,   \n",
            "loss_aug: 0.0815,   \n",
            "loss_ori_tea: 1.1734,   \n",
            "loss_aug_tea: 1.0786,   \n",
            "loss_cls: 0.0666,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.9080,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0675,   \n",
            "loss_aug: 0.0679,   \n",
            "loss_ori_tea: 1.0789,   \n",
            "loss_aug_tea: 0.9007,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.7158,   \n",
            "loss_aug_tea: 1.0172,   \n",
            "loss_cls: 0.0813,   \n",
            "loss_aug: 0.0816,   \n",
            "loss_ori_tea: 0.9377,   \n",
            "loss_aug_tea: 0.9745,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0633,   \n",
            "loss_ori_tea: 0.9185,   \n",
            "loss_aug_tea: 0.8355,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 1.0176,   \n",
            "loss_aug_tea: 0.9455,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.8934,   \n",
            "loss_aug_tea: 0.9132,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 0.9218,   \n",
            "loss_aug_tea: 1.0762,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0768,   \n",
            "loss_ori_tea: 0.8299,   \n",
            "loss_aug_tea: 0.8350,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 1.2404,   \n",
            "loss_aug_tea: 0.9092,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 0.9558,   \n",
            "loss_aug_tea: 0.9720,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.9417,   \n",
            "loss_aug_tea: 0.8609,   \n",
            "loss_cls: 0.0720,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 1.2168,   \n",
            "loss_aug_tea: 0.8693,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 1.0296,   \n",
            "loss_aug_tea: 1.0039,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 1.2118,   \n",
            "loss_aug_tea: 1.1540,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.9666,   \n",
            "loss_aug_tea: 0.8791,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 0.9160,   \n",
            "loss_aug_tea: 0.8814,   \n",
            "loss_cls: 0.0610,   \n",
            "loss_aug: 0.0610,   \n",
            "loss_ori_tea: 1.2152,   \n",
            "loss_aug_tea: 0.8519,   \n",
            "loss_cls: 0.0883,   \n",
            "loss_aug: 0.0885,   \n",
            "loss_ori_tea: 0.8612,   \n",
            "loss_aug_tea: 0.9288,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.8820,   \n",
            "loss_aug_tea: 0.8533,   \n",
            "loss_cls: 0.0742,   \n",
            "loss_aug: 0.0744,   \n",
            "loss_ori_tea: 1.0681,   \n",
            "loss_aug_tea: 0.9060,   \n",
            "loss_cls: 0.0864,   \n",
            "loss_aug: 0.0866,   \n",
            "loss_ori_tea: 0.7257,   \n",
            "loss_aug_tea: 0.9976,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.8697,   \n",
            "loss_aug_tea: 1.1288,   \n",
            "loss_cls: 0.0930,   \n",
            "loss_aug: 0.0932,   \n",
            "loss_ori_tea: 0.8072,   \n",
            "loss_aug_tea: 0.8269,   \n",
            "loss_cls: 0.0715,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 0.9563,   \n",
            "loss_aug_tea: 1.0183,   \n",
            "loss_cls: 0.0617,   \n",
            "loss_aug: 0.0619,   \n",
            "loss_ori_tea: 1.2153,   \n",
            "loss_aug_tea: 0.9075,   \n",
            "loss_cls: 0.0812,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 0.8218,   \n",
            "loss_aug_tea: 0.7643,   \n",
            "loss_cls: 0.0809,   \n",
            "loss_aug: 0.0813,   \n",
            "loss_ori_tea: 0.9779,   \n",
            "loss_aug_tea: 0.9490,   \n",
            "loss_cls: 0.0785,   \n",
            "loss_aug: 0.0791,   \n",
            "loss_ori_tea: 0.7351,   \n",
            "loss_aug_tea: 0.9780,   \n",
            "loss_cls: 0.0620,   \n",
            "loss_aug: 0.0627,   \n",
            "loss_ori_tea: 0.8890,   \n",
            "loss_aug_tea: 0.8863,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 1.1716,   \n",
            "loss_aug_tea: 1.0173,   \n",
            "loss_cls: 0.0797,   \n",
            "loss_aug: 0.0800,   \n",
            "loss_ori_tea: 1.0676,   \n",
            "loss_aug_tea: 0.9200,   \n",
            "loss_cls: 0.0836,   \n",
            "loss_aug: 0.0838,   \n",
            "loss_ori_tea: 0.9206,   \n",
            "loss_aug_tea: 0.7728,   \n",
            "loss_cls: 0.0753,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.8503,   \n",
            "loss_aug_tea: 0.9917,   \n",
            "loss_cls: 0.1019,   \n",
            "loss_aug: 0.1024,   \n",
            "loss_ori_tea: 0.8293,   \n",
            "loss_aug_tea: 0.8984,   \n",
            "loss_cls: 0.0678,   \n",
            "loss_aug: 0.0680,   \n",
            "loss_ori_tea: 1.2470,   \n",
            "loss_aug_tea: 0.8070,   \n",
            "loss_cls: 0.0717,   \n",
            "loss_aug: 0.0720,   \n",
            "loss_ori_tea: 0.8911,   \n",
            "loss_aug_tea: 0.8704,   \n",
            "loss_cls: 0.0656,   \n",
            "loss_aug: 0.0659,   \n",
            "loss_ori_tea: 0.8691,   \n",
            "loss_aug_tea: 0.8458,   \n",
            "loss_cls: 0.0738,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.7487,   \n",
            "loss_aug_tea: 0.9228,   \n",
            "loss_cls: 0.0819,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 1.0574,   \n",
            "loss_aug_tea: 0.9480,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0717,   \n",
            "loss_ori_tea: 0.8134,   \n",
            "loss_aug_tea: 0.9571,   \n",
            "loss_cls: 0.0734,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 1.0208,   \n",
            "loss_aug_tea: 0.8653,   \n",
            "loss_cls: 0.0691,   \n",
            "loss_aug: 0.0694,   \n",
            "loss_ori_tea: 1.0962,   \n",
            "loss_aug_tea: 0.7975,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0739,   \n",
            "loss_ori_tea: 1.0243,   \n",
            "loss_aug_tea: 0.7753,   \n",
            "loss_cls: 0.0779,   \n",
            "loss_aug: 0.0782,   \n",
            "loss_ori_tea: 1.0253,   \n",
            "loss_aug_tea: 1.0227,   \n",
            "loss_cls: 0.0579,   \n",
            "loss_aug: 0.0583,   \n",
            "loss_ori_tea: 1.0605,   \n",
            "loss_aug_tea: 1.0207,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0675,   \n",
            "loss_ori_tea: 1.0746,   \n",
            "loss_aug_tea: 0.9612,   \n",
            "loss_cls: 0.0789,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 1.1516,   \n",
            "loss_aug_tea: 0.7944,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 0.9223,   \n",
            "loss_aug_tea: 0.9510,   \n",
            "loss_cls: 0.0746,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.8328,   \n",
            "loss_aug_tea: 0.8914,   \n",
            "loss_cls: 0.0714,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 0.8901,   \n",
            "loss_aug_tea: 0.9377,   \n",
            "loss_cls: 0.0724,   \n",
            "loss_aug: 0.0727,   \n",
            "loss_ori_tea: 0.6668,   \n",
            "loss_aug_tea: 0.9004,   \n",
            "Epoch [10/15],   Mean Train Loss: 3.8162,   Mean Train Dice: 0.9264,   Mean Val Dice: 0.9616,   Mean Val ASD: 0.0026,   \n",
            "loss_cls: 0.0739,   \n",
            "loss_aug: 0.0742,   \n",
            "loss_ori_tea: 1.3850,   \n",
            "loss_aug_tea: 0.9499,   \n",
            "loss_cls: 0.0826,   \n",
            "loss_aug: 0.0831,   \n",
            "loss_ori_tea: 0.8138,   \n",
            "loss_aug_tea: 0.9390,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 0.9870,   \n",
            "loss_aug_tea: 0.9257,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.8970,   \n",
            "loss_aug_tea: 1.0137,   \n",
            "loss_cls: 0.0811,   \n",
            "loss_aug: 0.0813,   \n",
            "loss_ori_tea: 0.9323,   \n",
            "loss_aug_tea: 0.8066,   \n",
            "loss_cls: 0.0846,   \n",
            "loss_aug: 0.0847,   \n",
            "loss_ori_tea: 0.9118,   \n",
            "loss_aug_tea: 0.9733,   \n",
            "loss_cls: 0.0897,   \n",
            "loss_aug: 0.0899,   \n",
            "loss_ori_tea: 0.9367,   \n",
            "loss_aug_tea: 0.8525,   \n",
            "loss_cls: 0.0624,   \n",
            "loss_aug: 0.0626,   \n",
            "loss_ori_tea: 1.0246,   \n",
            "loss_aug_tea: 1.3298,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0786,   \n",
            "loss_ori_tea: 0.8384,   \n",
            "loss_aug_tea: 0.8806,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.8390,   \n",
            "loss_aug_tea: 0.9002,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0720,   \n",
            "loss_ori_tea: 0.8881,   \n",
            "loss_aug_tea: 0.9693,   \n",
            "loss_cls: 0.0658,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 0.9038,   \n",
            "loss_aug_tea: 0.8169,   \n",
            "loss_cls: 0.0925,   \n",
            "loss_aug: 0.0928,   \n",
            "loss_ori_tea: 1.2793,   \n",
            "loss_aug_tea: 1.2901,   \n",
            "loss_cls: 0.0756,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.8704,   \n",
            "loss_aug_tea: 0.9550,   \n",
            "loss_cls: 0.0521,   \n",
            "loss_aug: 0.0524,   \n",
            "loss_ori_tea: 1.0222,   \n",
            "loss_aug_tea: 0.8105,   \n",
            "loss_cls: 0.0849,   \n",
            "loss_aug: 0.0850,   \n",
            "loss_ori_tea: 0.9016,   \n",
            "loss_aug_tea: 0.9284,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 0.9526,   \n",
            "loss_aug_tea: 0.9750,   \n",
            "loss_cls: 0.0527,   \n",
            "loss_aug: 0.0532,   \n",
            "loss_ori_tea: 1.0373,   \n",
            "loss_aug_tea: 0.9852,   \n",
            "loss_cls: 0.0690,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 1.3595,   \n",
            "loss_aug_tea: 1.1623,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.7984,   \n",
            "loss_aug_tea: 0.9796,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 1.0239,   \n",
            "loss_aug_tea: 1.0159,   \n",
            "loss_cls: 0.0600,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 1.0318,   \n",
            "loss_aug_tea: 0.8207,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0742,   \n",
            "loss_ori_tea: 1.2906,   \n",
            "loss_aug_tea: 0.7533,   \n",
            "loss_cls: 0.0847,   \n",
            "loss_aug: 0.0849,   \n",
            "loss_ori_tea: 0.7266,   \n",
            "loss_aug_tea: 0.9600,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0681,   \n",
            "loss_ori_tea: 1.0687,   \n",
            "loss_aug_tea: 1.2163,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0714,   \n",
            "loss_ori_tea: 0.7994,   \n",
            "loss_aug_tea: 0.9088,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.0276,   \n",
            "loss_aug_tea: 0.8947,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.8135,   \n",
            "loss_aug_tea: 0.9906,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 1.1689,   \n",
            "loss_aug_tea: 0.6428,   \n",
            "loss_cls: 0.0751,   \n",
            "loss_aug: 0.0756,   \n",
            "loss_ori_tea: 1.0409,   \n",
            "loss_aug_tea: 0.8754,   \n",
            "loss_cls: 0.0720,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 1.0597,   \n",
            "loss_aug_tea: 0.7699,   \n",
            "loss_cls: 0.0740,   \n",
            "loss_aug: 0.0743,   \n",
            "loss_ori_tea: 1.5030,   \n",
            "loss_aug_tea: 1.3681,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 0.9261,   \n",
            "loss_aug_tea: 0.8517,   \n",
            "loss_cls: 0.0778,   \n",
            "loss_aug: 0.0781,   \n",
            "loss_ori_tea: 0.8355,   \n",
            "loss_aug_tea: 0.8322,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.8268,   \n",
            "loss_aug_tea: 0.8924,   \n",
            "loss_cls: 0.0830,   \n",
            "loss_aug: 0.0831,   \n",
            "loss_ori_tea: 1.0728,   \n",
            "loss_aug_tea: 1.0609,   \n",
            "loss_cls: 0.0708,   \n",
            "loss_aug: 0.0711,   \n",
            "loss_ori_tea: 1.0661,   \n",
            "loss_aug_tea: 1.0207,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0710,   \n",
            "loss_ori_tea: 0.9154,   \n",
            "loss_aug_tea: 1.2050,   \n",
            "loss_cls: 0.0677,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.4356,   \n",
            "loss_aug_tea: 0.7794,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0745,   \n",
            "loss_ori_tea: 1.0050,   \n",
            "loss_aug_tea: 0.9659,   \n",
            "loss_cls: 0.0610,   \n",
            "loss_aug: 0.0612,   \n",
            "loss_ori_tea: 0.8626,   \n",
            "loss_aug_tea: 0.9009,   \n",
            "loss_cls: 0.0772,   \n",
            "loss_aug: 0.0774,   \n",
            "loss_ori_tea: 0.9225,   \n",
            "loss_aug_tea: 0.9270,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0783,   \n",
            "loss_ori_tea: 1.1683,   \n",
            "loss_aug_tea: 1.2403,   \n",
            "loss_cls: 0.0646,   \n",
            "loss_aug: 0.0648,   \n",
            "loss_ori_tea: 1.0660,   \n",
            "loss_aug_tea: 0.9646,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0690,   \n",
            "loss_ori_tea: 0.7919,   \n",
            "loss_aug_tea: 1.0237,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 0.7733,   \n",
            "loss_aug_tea: 1.0362,   \n",
            "loss_cls: 0.0858,   \n",
            "loss_aug: 0.0861,   \n",
            "loss_ori_tea: 1.2178,   \n",
            "loss_aug_tea: 0.7570,   \n",
            "loss_cls: 0.0715,   \n",
            "loss_aug: 0.0716,   \n",
            "loss_ori_tea: 1.3469,   \n",
            "loss_aug_tea: 1.1140,   \n",
            "loss_cls: 0.0791,   \n",
            "loss_aug: 0.0792,   \n",
            "loss_ori_tea: 0.8554,   \n",
            "loss_aug_tea: 0.9132,   \n",
            "loss_cls: 0.0833,   \n",
            "loss_aug: 0.0836,   \n",
            "loss_ori_tea: 1.0467,   \n",
            "loss_aug_tea: 0.9844,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.7324,   \n",
            "loss_aug_tea: 1.0812,   \n",
            "loss_cls: 0.0609,   \n",
            "loss_aug: 0.0613,   \n",
            "loss_ori_tea: 0.9604,   \n",
            "loss_aug_tea: 0.9752,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0654,   \n",
            "loss_ori_tea: 1.0363,   \n",
            "loss_aug_tea: 1.0113,   \n",
            "loss_cls: 0.0683,   \n",
            "loss_aug: 0.0685,   \n",
            "loss_ori_tea: 1.0015,   \n",
            "loss_aug_tea: 0.9853,   \n",
            "loss_cls: 0.0586,   \n",
            "loss_aug: 0.0589,   \n",
            "loss_ori_tea: 0.8180,   \n",
            "loss_aug_tea: 0.8591,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 1.0027,   \n",
            "loss_aug_tea: 1.0607,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.9582,   \n",
            "loss_aug_tea: 0.9948,   \n",
            "loss_cls: 0.0754,   \n",
            "loss_aug: 0.0756,   \n",
            "loss_ori_tea: 0.8860,   \n",
            "loss_aug_tea: 0.8183,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 0.8760,   \n",
            "loss_aug_tea: 0.8811,   \n",
            "loss_cls: 0.0835,   \n",
            "loss_aug: 0.0840,   \n",
            "loss_ori_tea: 1.1454,   \n",
            "loss_aug_tea: 1.1595,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0670,   \n",
            "loss_ori_tea: 0.9541,   \n",
            "loss_aug_tea: 0.9181,   \n",
            "loss_cls: 0.0859,   \n",
            "loss_aug: 0.0862,   \n",
            "loss_ori_tea: 1.0722,   \n",
            "loss_aug_tea: 0.8418,   \n",
            "loss_cls: 0.0830,   \n",
            "loss_aug: 0.0833,   \n",
            "loss_ori_tea: 1.1159,   \n",
            "loss_aug_tea: 1.1201,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.9706,   \n",
            "loss_aug_tea: 0.9533,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 0.9436,   \n",
            "loss_aug_tea: 0.9404,   \n",
            "loss_cls: 0.0781,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 0.8216,   \n",
            "loss_aug_tea: 1.0181,   \n",
            "Epoch [11/15],   Mean Train Loss: 3.9921,   Mean Train Dice: 0.9264,   Mean Val Dice: 0.9630,   Mean Val ASD: 0.0026,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0637,   \n",
            "loss_ori_tea: 1.3524,   \n",
            "loss_aug_tea: 1.0667,   \n",
            "loss_cls: 0.0751,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 1.0839,   \n",
            "loss_aug_tea: 1.0382,   \n",
            "loss_cls: 0.0774,   \n",
            "loss_aug: 0.0778,   \n",
            "loss_ori_tea: 1.0090,   \n",
            "loss_aug_tea: 1.0419,   \n",
            "loss_cls: 0.0723,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.8444,   \n",
            "loss_aug_tea: 0.9067,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 1.0605,   \n",
            "loss_aug_tea: 0.8491,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0703,   \n",
            "loss_ori_tea: 0.8710,   \n",
            "loss_aug_tea: 0.8767,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0699,   \n",
            "loss_ori_tea: 1.1142,   \n",
            "loss_aug_tea: 0.8106,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0635,   \n",
            "loss_ori_tea: 1.2009,   \n",
            "loss_aug_tea: 0.8518,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0679,   \n",
            "loss_ori_tea: 0.9102,   \n",
            "loss_aug_tea: 1.0399,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 0.7811,   \n",
            "loss_aug_tea: 0.9867,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0751,   \n",
            "loss_ori_tea: 1.1475,   \n",
            "loss_aug_tea: 0.7174,   \n",
            "loss_cls: 0.0551,   \n",
            "loss_aug: 0.0556,   \n",
            "loss_ori_tea: 0.8359,   \n",
            "loss_aug_tea: 0.9479,   \n",
            "loss_cls: 0.0767,   \n",
            "loss_aug: 0.0770,   \n",
            "loss_ori_tea: 0.8883,   \n",
            "loss_aug_tea: 0.9466,   \n",
            "loss_cls: 0.0605,   \n",
            "loss_aug: 0.0607,   \n",
            "loss_ori_tea: 0.8409,   \n",
            "loss_aug_tea: 1.3057,   \n",
            "loss_cls: 0.0905,   \n",
            "loss_aug: 0.0907,   \n",
            "loss_ori_tea: 1.0194,   \n",
            "loss_aug_tea: 1.0960,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0703,   \n",
            "loss_ori_tea: 0.8576,   \n",
            "loss_aug_tea: 0.9390,   \n",
            "loss_cls: 0.0753,   \n",
            "loss_aug: 0.0755,   \n",
            "loss_ori_tea: 0.9552,   \n",
            "loss_aug_tea: 0.9483,   \n",
            "loss_cls: 0.0659,   \n",
            "loss_aug: 0.0661,   \n",
            "loss_ori_tea: 0.7520,   \n",
            "loss_aug_tea: 0.9981,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0753,   \n",
            "loss_ori_tea: 0.9725,   \n",
            "loss_aug_tea: 0.9645,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.9264,   \n",
            "loss_aug_tea: 1.0209,   \n",
            "loss_cls: 0.0790,   \n",
            "loss_aug: 0.0793,   \n",
            "loss_ori_tea: 1.1568,   \n",
            "loss_aug_tea: 0.9112,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 0.7786,   \n",
            "loss_aug_tea: 1.3423,   \n",
            "loss_cls: 0.0810,   \n",
            "loss_aug: 0.0811,   \n",
            "loss_ori_tea: 1.3943,   \n",
            "loss_aug_tea: 1.3525,   \n",
            "loss_cls: 0.0743,   \n",
            "loss_aug: 0.0747,   \n",
            "loss_ori_tea: 0.9935,   \n",
            "loss_aug_tea: 1.0057,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.1200,   \n",
            "loss_aug_tea: 1.0914,   \n",
            "loss_cls: 0.0651,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 0.8399,   \n",
            "loss_aug_tea: 1.0666,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 0.9547,   \n",
            "loss_aug_tea: 0.9952,   \n",
            "loss_cls: 0.0805,   \n",
            "loss_aug: 0.0810,   \n",
            "loss_ori_tea: 0.9842,   \n",
            "loss_aug_tea: 0.9663,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.8713,   \n",
            "loss_aug_tea: 0.9448,   \n",
            "loss_cls: 0.0725,   \n",
            "loss_aug: 0.0726,   \n",
            "loss_ori_tea: 0.9953,   \n",
            "loss_aug_tea: 0.8233,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0753,   \n",
            "loss_ori_tea: 0.9511,   \n",
            "loss_aug_tea: 1.1047,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 1.1768,   \n",
            "loss_aug_tea: 0.8854,   \n",
            "loss_cls: 0.0657,   \n",
            "loss_aug: 0.0663,   \n",
            "loss_ori_tea: 1.1356,   \n",
            "loss_aug_tea: 0.9177,   \n",
            "loss_cls: 0.0885,   \n",
            "loss_aug: 0.0888,   \n",
            "loss_ori_tea: 0.8633,   \n",
            "loss_aug_tea: 1.0060,   \n",
            "loss_cls: 0.0639,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 0.9470,   \n",
            "loss_aug_tea: 0.9628,   \n",
            "loss_cls: 0.0694,   \n",
            "loss_aug: 0.0697,   \n",
            "loss_ori_tea: 1.2178,   \n",
            "loss_aug_tea: 1.1291,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0724,   \n",
            "loss_ori_tea: 0.8196,   \n",
            "loss_aug_tea: 0.9738,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.9210,   \n",
            "loss_aug_tea: 1.0392,   \n",
            "loss_cls: 0.0594,   \n",
            "loss_aug: 0.0597,   \n",
            "loss_ori_tea: 1.3447,   \n",
            "loss_aug_tea: 0.8896,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 0.8680,   \n",
            "loss_aug_tea: 1.1061,   \n",
            "loss_cls: 0.0555,   \n",
            "loss_aug: 0.0557,   \n",
            "loss_ori_tea: 0.8271,   \n",
            "loss_aug_tea: 1.1698,   \n",
            "loss_cls: 0.0570,   \n",
            "loss_aug: 0.0573,   \n",
            "loss_ori_tea: 0.8224,   \n",
            "loss_aug_tea: 0.8438,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 1.3603,   \n",
            "loss_aug_tea: 0.9867,   \n",
            "loss_cls: 0.0811,   \n",
            "loss_aug: 0.0815,   \n",
            "loss_ori_tea: 1.1177,   \n",
            "loss_aug_tea: 0.9931,   \n",
            "loss_cls: 0.0612,   \n",
            "loss_aug: 0.0615,   \n",
            "loss_ori_tea: 1.0462,   \n",
            "loss_aug_tea: 0.9370,   \n",
            "loss_cls: 0.0655,   \n",
            "loss_aug: 0.0660,   \n",
            "loss_ori_tea: 1.1442,   \n",
            "loss_aug_tea: 1.0818,   \n",
            "loss_cls: 0.0870,   \n",
            "loss_aug: 0.0871,   \n",
            "loss_ori_tea: 1.1807,   \n",
            "loss_aug_tea: 0.8244,   \n",
            "loss_cls: 0.0636,   \n",
            "loss_aug: 0.0638,   \n",
            "loss_ori_tea: 0.9087,   \n",
            "loss_aug_tea: 0.7401,   \n",
            "loss_cls: 0.0823,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 1.0939,   \n",
            "loss_aug_tea: 0.9090,   \n",
            "loss_cls: 0.0866,   \n",
            "loss_aug: 0.0867,   \n",
            "loss_ori_tea: 0.9845,   \n",
            "loss_aug_tea: 0.8619,   \n",
            "loss_cls: 0.0666,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 1.1271,   \n",
            "loss_aug_tea: 0.9570,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0601,   \n",
            "loss_ori_tea: 0.8097,   \n",
            "loss_aug_tea: 0.9491,   \n",
            "loss_cls: 0.0763,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 0.8346,   \n",
            "loss_aug_tea: 1.0478,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.9238,   \n",
            "loss_aug_tea: 0.8911,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0822,   \n",
            "loss_ori_tea: 1.1673,   \n",
            "loss_aug_tea: 0.7785,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0653,   \n",
            "loss_ori_tea: 1.2087,   \n",
            "loss_aug_tea: 0.8226,   \n",
            "loss_cls: 0.0887,   \n",
            "loss_aug: 0.0890,   \n",
            "loss_ori_tea: 0.9983,   \n",
            "loss_aug_tea: 0.8630,   \n",
            "loss_cls: 0.0813,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 1.1845,   \n",
            "loss_aug_tea: 0.9976,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 0.7926,   \n",
            "loss_aug_tea: 1.0516,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0670,   \n",
            "loss_ori_tea: 1.0150,   \n",
            "loss_aug_tea: 1.0546,   \n",
            "loss_cls: 0.0615,   \n",
            "loss_aug: 0.0617,   \n",
            "loss_ori_tea: 1.0916,   \n",
            "loss_aug_tea: 0.8631,   \n",
            "loss_cls: 0.0729,   \n",
            "loss_aug: 0.0732,   \n",
            "loss_ori_tea: 0.9162,   \n",
            "loss_aug_tea: 0.9716,   \n",
            "loss_cls: 0.0801,   \n",
            "loss_aug: 0.0804,   \n",
            "loss_ori_tea: 1.1489,   \n",
            "loss_aug_tea: 0.9835,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0640,   \n",
            "loss_ori_tea: 0.9029,   \n",
            "loss_aug_tea: 1.0081,   \n",
            "loss_cls: 0.0649,   \n",
            "loss_aug: 0.0652,   \n",
            "loss_ori_tea: 0.9393,   \n",
            "loss_aug_tea: 0.8854,   \n",
            "loss_cls: 0.0640,   \n",
            "loss_aug: 0.0643,   \n",
            "loss_ori_tea: 1.0316,   \n",
            "loss_aug_tea: 0.9208,   \n",
            "Epoch [12/15],   Mean Train Loss: 4.0290,   Mean Train Dice: 0.9284,   Mean Val Dice: 0.9623,   Mean Val ASD: 0.0020,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0669,   \n",
            "loss_ori_tea: 0.9066,   \n",
            "loss_aug_tea: 1.0840,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0635,   \n",
            "loss_ori_tea: 1.1813,   \n",
            "loss_aug_tea: 0.9089,   \n",
            "loss_cls: 0.0782,   \n",
            "loss_aug: 0.0788,   \n",
            "loss_ori_tea: 0.8365,   \n",
            "loss_aug_tea: 1.0047,   \n",
            "loss_cls: 0.0589,   \n",
            "loss_aug: 0.0593,   \n",
            "loss_ori_tea: 0.8464,   \n",
            "loss_aug_tea: 1.0466,   \n",
            "loss_cls: 0.0803,   \n",
            "loss_aug: 0.0805,   \n",
            "loss_ori_tea: 1.1570,   \n",
            "loss_aug_tea: 1.0814,   \n",
            "loss_cls: 0.0770,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.8706,   \n",
            "loss_aug_tea: 1.0296,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0790,   \n",
            "loss_ori_tea: 0.7397,   \n",
            "loss_aug_tea: 1.0038,   \n",
            "loss_cls: 0.0773,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 0.8647,   \n",
            "loss_aug_tea: 0.9230,   \n",
            "loss_cls: 0.0826,   \n",
            "loss_aug: 0.0828,   \n",
            "loss_ori_tea: 0.9043,   \n",
            "loss_aug_tea: 0.8989,   \n",
            "loss_cls: 0.0594,   \n",
            "loss_aug: 0.0597,   \n",
            "loss_ori_tea: 0.9442,   \n",
            "loss_aug_tea: 0.8787,   \n",
            "loss_cls: 0.0697,   \n",
            "loss_aug: 0.0700,   \n",
            "loss_ori_tea: 1.0463,   \n",
            "loss_aug_tea: 1.0017,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0682,   \n",
            "loss_ori_tea: 1.0757,   \n",
            "loss_aug_tea: 0.9844,   \n",
            "loss_cls: 0.0782,   \n",
            "loss_aug: 0.0785,   \n",
            "loss_ori_tea: 1.4741,   \n",
            "loss_aug_tea: 0.9820,   \n",
            "loss_cls: 0.0627,   \n",
            "loss_aug: 0.0629,   \n",
            "loss_ori_tea: 1.0847,   \n",
            "loss_aug_tea: 1.0272,   \n",
            "loss_cls: 0.0585,   \n",
            "loss_aug: 0.0588,   \n",
            "loss_ori_tea: 0.9254,   \n",
            "loss_aug_tea: 0.9548,   \n",
            "loss_cls: 0.0779,   \n",
            "loss_aug: 0.0783,   \n",
            "loss_ori_tea: 1.0359,   \n",
            "loss_aug_tea: 1.0315,   \n",
            "loss_cls: 0.0848,   \n",
            "loss_aug: 0.0852,   \n",
            "loss_ori_tea: 0.8804,   \n",
            "loss_aug_tea: 1.0006,   \n",
            "loss_cls: 0.0733,   \n",
            "loss_aug: 0.0734,   \n",
            "loss_ori_tea: 0.9633,   \n",
            "loss_aug_tea: 0.9547,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 0.8680,   \n",
            "loss_aug_tea: 0.9968,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 1.2046,   \n",
            "loss_aug_tea: 0.9170,   \n",
            "loss_cls: 0.0847,   \n",
            "loss_aug: 0.0851,   \n",
            "loss_ori_tea: 0.7905,   \n",
            "loss_aug_tea: 1.1611,   \n",
            "loss_cls: 0.0844,   \n",
            "loss_aug: 0.0846,   \n",
            "loss_ori_tea: 0.7707,   \n",
            "loss_aug_tea: 1.0484,   \n",
            "loss_cls: 0.0731,   \n",
            "loss_aug: 0.0732,   \n",
            "loss_ori_tea: 1.4248,   \n",
            "loss_aug_tea: 1.0490,   \n",
            "loss_cls: 0.0590,   \n",
            "loss_aug: 0.0592,   \n",
            "loss_ori_tea: 0.7416,   \n",
            "loss_aug_tea: 0.9919,   \n",
            "loss_cls: 0.0685,   \n",
            "loss_aug: 0.0686,   \n",
            "loss_ori_tea: 1.0082,   \n",
            "loss_aug_tea: 0.9912,   \n",
            "loss_cls: 0.0630,   \n",
            "loss_aug: 0.0634,   \n",
            "loss_ori_tea: 0.9682,   \n",
            "loss_aug_tea: 1.0416,   \n",
            "loss_cls: 0.0816,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 1.0330,   \n",
            "loss_aug_tea: 0.9049,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.9183,   \n",
            "loss_aug_tea: 0.9914,   \n",
            "loss_cls: 0.0707,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 1.0227,   \n",
            "loss_aug_tea: 1.1436,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 1.0554,   \n",
            "loss_aug_tea: 0.9334,   \n",
            "loss_cls: 0.0758,   \n",
            "loss_aug: 0.0762,   \n",
            "loss_ori_tea: 1.0773,   \n",
            "loss_aug_tea: 0.9892,   \n",
            "loss_cls: 0.0719,   \n",
            "loss_aug: 0.0722,   \n",
            "loss_ori_tea: 0.8710,   \n",
            "loss_aug_tea: 1.0056,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 1.0741,   \n",
            "loss_aug_tea: 0.9215,   \n",
            "loss_cls: 0.0828,   \n",
            "loss_aug: 0.0829,   \n",
            "loss_ori_tea: 1.5017,   \n",
            "loss_aug_tea: 0.7993,   \n",
            "loss_cls: 0.0652,   \n",
            "loss_aug: 0.0653,   \n",
            "loss_ori_tea: 0.8311,   \n",
            "loss_aug_tea: 1.0866,   \n",
            "loss_cls: 0.0908,   \n",
            "loss_aug: 0.0912,   \n",
            "loss_ori_tea: 1.0073,   \n",
            "loss_aug_tea: 0.9651,   \n",
            "loss_cls: 0.0740,   \n",
            "loss_aug: 0.0741,   \n",
            "loss_ori_tea: 0.8356,   \n",
            "loss_aug_tea: 0.9303,   \n",
            "loss_cls: 0.0785,   \n",
            "loss_aug: 0.0790,   \n",
            "loss_ori_tea: 0.9536,   \n",
            "loss_aug_tea: 0.9097,   \n",
            "loss_cls: 0.0732,   \n",
            "loss_aug: 0.0737,   \n",
            "loss_ori_tea: 0.9295,   \n",
            "loss_aug_tea: 1.0291,   \n",
            "loss_cls: 0.0668,   \n",
            "loss_aug: 0.0668,   \n",
            "loss_ori_tea: 0.9917,   \n",
            "loss_aug_tea: 0.8870,   \n",
            "loss_cls: 0.0736,   \n",
            "loss_aug: 0.0740,   \n",
            "loss_ori_tea: 1.0250,   \n",
            "loss_aug_tea: 0.9210,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 0.8947,   \n",
            "loss_aug_tea: 0.8782,   \n",
            "loss_cls: 0.0635,   \n",
            "loss_aug: 0.0639,   \n",
            "loss_ori_tea: 0.9461,   \n",
            "loss_aug_tea: 1.0010,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 0.8306,   \n",
            "loss_aug_tea: 1.0520,   \n",
            "loss_cls: 0.0769,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.9815,   \n",
            "loss_aug_tea: 0.9403,   \n",
            "loss_cls: 0.0619,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 0.9733,   \n",
            "loss_aug_tea: 1.0173,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 1.0542,   \n",
            "loss_aug_tea: 1.1159,   \n",
            "loss_cls: 0.0717,   \n",
            "loss_aug: 0.0718,   \n",
            "loss_ori_tea: 1.0187,   \n",
            "loss_aug_tea: 1.3984,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0801,   \n",
            "loss_ori_tea: 1.3564,   \n",
            "loss_aug_tea: 1.2500,   \n",
            "loss_cls: 0.0640,   \n",
            "loss_aug: 0.0642,   \n",
            "loss_ori_tea: 0.9224,   \n",
            "loss_aug_tea: 0.9053,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0685,   \n",
            "loss_ori_tea: 0.9887,   \n",
            "loss_aug_tea: 1.0503,   \n",
            "loss_cls: 0.0673,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 1.1253,   \n",
            "loss_aug_tea: 1.0269,   \n",
            "loss_cls: 0.0796,   \n",
            "loss_aug: 0.0798,   \n",
            "loss_ori_tea: 1.1718,   \n",
            "loss_aug_tea: 0.8742,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.8797,   \n",
            "loss_aug_tea: 1.1108,   \n",
            "loss_cls: 0.0634,   \n",
            "loss_aug: 0.0638,   \n",
            "loss_ori_tea: 0.9705,   \n",
            "loss_aug_tea: 1.2099,   \n",
            "loss_cls: 0.0561,   \n",
            "loss_aug: 0.0562,   \n",
            "loss_ori_tea: 1.0148,   \n",
            "loss_aug_tea: 0.9842,   \n",
            "loss_cls: 0.0499,   \n",
            "loss_aug: 0.0504,   \n",
            "loss_ori_tea: 1.0619,   \n",
            "loss_aug_tea: 1.1488,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0761,   \n",
            "loss_ori_tea: 1.1458,   \n",
            "loss_aug_tea: 1.1382,   \n",
            "loss_cls: 0.0760,   \n",
            "loss_aug: 0.0765,   \n",
            "loss_ori_tea: 1.1433,   \n",
            "loss_aug_tea: 1.2013,   \n",
            "loss_cls: 0.0786,   \n",
            "loss_aug: 0.0788,   \n",
            "loss_ori_tea: 0.9889,   \n",
            "loss_aug_tea: 1.0181,   \n",
            "loss_cls: 0.0752,   \n",
            "loss_aug: 0.0757,   \n",
            "loss_ori_tea: 0.9555,   \n",
            "loss_aug_tea: 0.9084,   \n",
            "loss_cls: 0.0749,   \n",
            "loss_aug: 0.0752,   \n",
            "loss_ori_tea: 1.0976,   \n",
            "loss_aug_tea: 1.0297,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 1.3667,   \n",
            "loss_aug_tea: 1.2665,   \n",
            "loss_cls: 0.0700,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.9494,   \n",
            "loss_aug_tea: 0.8540,   \n",
            "loss_cls: 0.0771,   \n",
            "loss_aug: 0.0773,   \n",
            "loss_ori_tea: 0.9964,   \n",
            "loss_aug_tea: 0.9763,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0692,   \n",
            "loss_ori_tea: 1.2656,   \n",
            "loss_aug_tea: 0.9248,   \n",
            "Epoch [13/15],   Mean Train Loss: 4.1156,   Mean Train Dice: 0.9278,   Mean Val Dice: 0.9627,   Mean Val ASD: 0.0020,   \n",
            "loss_cls: 0.0619,   \n",
            "loss_aug: 0.0622,   \n",
            "loss_ori_tea: 1.1713,   \n",
            "loss_aug_tea: 1.1202,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0746,   \n",
            "loss_ori_tea: 0.9867,   \n",
            "loss_aug_tea: 0.9790,   \n",
            "loss_cls: 0.0774,   \n",
            "loss_aug: 0.0776,   \n",
            "loss_ori_tea: 0.7615,   \n",
            "loss_aug_tea: 1.2616,   \n",
            "loss_cls: 0.0741,   \n",
            "loss_aug: 0.0745,   \n",
            "loss_ori_tea: 0.9416,   \n",
            "loss_aug_tea: 0.9989,   \n",
            "loss_cls: 0.0685,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.9020,   \n",
            "loss_aug_tea: 0.9946,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0589,   \n",
            "loss_ori_tea: 0.9777,   \n",
            "loss_aug_tea: 0.9294,   \n",
            "loss_cls: 0.0603,   \n",
            "loss_aug: 0.0606,   \n",
            "loss_ori_tea: 0.9065,   \n",
            "loss_aug_tea: 1.1445,   \n",
            "loss_cls: 0.0899,   \n",
            "loss_aug: 0.0902,   \n",
            "loss_ori_tea: 1.1078,   \n",
            "loss_aug_tea: 0.7922,   \n",
            "loss_cls: 0.0654,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 1.0413,   \n",
            "loss_aug_tea: 1.0294,   \n",
            "loss_cls: 0.0602,   \n",
            "loss_aug: 0.0605,   \n",
            "loss_ori_tea: 1.1480,   \n",
            "loss_aug_tea: 1.3759,   \n",
            "loss_cls: 0.0764,   \n",
            "loss_aug: 0.0766,   \n",
            "loss_ori_tea: 0.9273,   \n",
            "loss_aug_tea: 0.9799,   \n",
            "loss_cls: 0.0805,   \n",
            "loss_aug: 0.0807,   \n",
            "loss_ori_tea: 0.8808,   \n",
            "loss_aug_tea: 0.9488,   \n",
            "loss_cls: 0.0730,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8654,   \n",
            "loss_aug_tea: 1.1089,   \n",
            "loss_cls: 0.0813,   \n",
            "loss_aug: 0.0817,   \n",
            "loss_ori_tea: 0.8062,   \n",
            "loss_aug_tea: 1.0778,   \n",
            "loss_cls: 0.0823,   \n",
            "loss_aug: 0.0826,   \n",
            "loss_ori_tea: 1.0210,   \n",
            "loss_aug_tea: 0.8634,   \n",
            "loss_cls: 0.0599,   \n",
            "loss_aug: 0.0602,   \n",
            "loss_ori_tea: 0.8517,   \n",
            "loss_aug_tea: 0.9175,   \n",
            "loss_cls: 0.0705,   \n",
            "loss_aug: 0.0707,   \n",
            "loss_ori_tea: 0.8124,   \n",
            "loss_aug_tea: 0.9822,   \n",
            "loss_cls: 0.0597,   \n",
            "loss_aug: 0.0600,   \n",
            "loss_ori_tea: 0.9408,   \n",
            "loss_aug_tea: 0.9600,   \n",
            "loss_cls: 0.0637,   \n",
            "loss_aug: 0.0642,   \n",
            "loss_ori_tea: 1.0497,   \n",
            "loss_aug_tea: 1.0076,   \n",
            "loss_cls: 0.0493,   \n",
            "loss_aug: 0.0495,   \n",
            "loss_ori_tea: 0.8872,   \n",
            "loss_aug_tea: 1.1587,   \n",
            "loss_cls: 0.0747,   \n",
            "loss_aug: 0.0750,   \n",
            "loss_ori_tea: 0.8913,   \n",
            "loss_aug_tea: 1.1151,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0706,   \n",
            "loss_ori_tea: 1.0472,   \n",
            "loss_aug_tea: 0.9606,   \n",
            "loss_cls: 0.0925,   \n",
            "loss_aug: 0.0929,   \n",
            "loss_ori_tea: 0.8788,   \n",
            "loss_aug_tea: 0.9541,   \n",
            "loss_cls: 0.0765,   \n",
            "loss_aug: 0.0767,   \n",
            "loss_ori_tea: 1.0125,   \n",
            "loss_aug_tea: 0.9239,   \n",
            "loss_cls: 0.0631,   \n",
            "loss_aug: 0.0633,   \n",
            "loss_ori_tea: 0.9584,   \n",
            "loss_aug_tea: 0.7658,   \n",
            "loss_cls: 0.0776,   \n",
            "loss_aug: 0.0779,   \n",
            "loss_ori_tea: 0.9053,   \n",
            "loss_aug_tea: 0.9562,   \n",
            "loss_cls: 0.0579,   \n",
            "loss_aug: 0.0580,   \n",
            "loss_ori_tea: 0.7689,   \n",
            "loss_aug_tea: 1.1792,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 1.1388,   \n",
            "loss_aug_tea: 1.2013,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 1.4601,   \n",
            "loss_aug_tea: 0.7778,   \n",
            "loss_cls: 0.0721,   \n",
            "loss_aug: 0.0725,   \n",
            "loss_ori_tea: 0.8615,   \n",
            "loss_aug_tea: 1.0689,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0685,   \n",
            "loss_ori_tea: 0.9968,   \n",
            "loss_aug_tea: 1.0036,   \n",
            "loss_cls: 0.0778,   \n",
            "loss_aug: 0.0779,   \n",
            "loss_ori_tea: 0.8698,   \n",
            "loss_aug_tea: 0.8543,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0732,   \n",
            "loss_ori_tea: 1.1592,   \n",
            "loss_aug_tea: 0.9035,   \n",
            "loss_cls: 0.0777,   \n",
            "loss_aug: 0.0781,   \n",
            "loss_ori_tea: 0.9358,   \n",
            "loss_aug_tea: 1.0898,   \n",
            "loss_cls: 0.0845,   \n",
            "loss_aug: 0.0847,   \n",
            "loss_ori_tea: 1.0010,   \n",
            "loss_aug_tea: 1.1357,   \n",
            "loss_cls: 0.0806,   \n",
            "loss_aug: 0.0809,   \n",
            "loss_ori_tea: 1.0870,   \n",
            "loss_aug_tea: 0.8998,   \n",
            "loss_cls: 0.0667,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.9935,   \n",
            "loss_aug_tea: 1.0924,   \n",
            "loss_cls: 0.0929,   \n",
            "loss_aug: 0.0933,   \n",
            "loss_ori_tea: 0.8427,   \n",
            "loss_aug_tea: 1.0280,   \n",
            "loss_cls: 0.0751,   \n",
            "loss_aug: 0.0753,   \n",
            "loss_ori_tea: 1.1630,   \n",
            "loss_aug_tea: 1.1287,   \n",
            "loss_cls: 0.0683,   \n",
            "loss_aug: 0.0689,   \n",
            "loss_ori_tea: 0.8829,   \n",
            "loss_aug_tea: 1.0678,   \n",
            "loss_cls: 0.0587,   \n",
            "loss_aug: 0.0590,   \n",
            "loss_ori_tea: 0.9700,   \n",
            "loss_aug_tea: 1.3899,   \n",
            "loss_cls: 0.0711,   \n",
            "loss_aug: 0.0712,   \n",
            "loss_ori_tea: 1.5047,   \n",
            "loss_aug_tea: 0.9121,   \n",
            "loss_cls: 0.0766,   \n",
            "loss_aug: 0.0768,   \n",
            "loss_ori_tea: 0.8448,   \n",
            "loss_aug_tea: 0.8987,   \n",
            "loss_cls: 0.0727,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.9832,   \n",
            "loss_aug_tea: 0.8720,   \n",
            "loss_cls: 0.0575,   \n",
            "loss_aug: 0.0577,   \n",
            "loss_ori_tea: 1.1546,   \n",
            "loss_aug_tea: 1.0530,   \n",
            "loss_cls: 0.0652,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 0.8236,   \n",
            "loss_aug_tea: 1.0324,   \n",
            "loss_cls: 0.0874,   \n",
            "loss_aug: 0.0879,   \n",
            "loss_ori_tea: 0.9999,   \n",
            "loss_aug_tea: 0.9081,   \n",
            "loss_cls: 0.0686,   \n",
            "loss_aug: 0.0688,   \n",
            "loss_ori_tea: 0.9974,   \n",
            "loss_aug_tea: 0.8371,   \n",
            "loss_cls: 0.0653,   \n",
            "loss_aug: 0.0655,   \n",
            "loss_ori_tea: 0.9851,   \n",
            "loss_aug_tea: 0.9380,   \n",
            "loss_cls: 0.0685,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.9270,   \n",
            "loss_aug_tea: 0.9674,   \n",
            "loss_cls: 0.0821,   \n",
            "loss_aug: 0.0824,   \n",
            "loss_ori_tea: 0.9181,   \n",
            "loss_aug_tea: 0.9724,   \n",
            "loss_cls: 0.0794,   \n",
            "loss_aug: 0.0796,   \n",
            "loss_ori_tea: 0.9864,   \n",
            "loss_aug_tea: 0.9937,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0632,   \n",
            "loss_ori_tea: 1.3231,   \n",
            "loss_aug_tea: 1.4280,   \n",
            "loss_cls: 0.0718,   \n",
            "loss_aug: 0.0720,   \n",
            "loss_ori_tea: 1.4543,   \n",
            "loss_aug_tea: 1.1717,   \n",
            "loss_cls: 0.0838,   \n",
            "loss_aug: 0.0841,   \n",
            "loss_ori_tea: 1.2686,   \n",
            "loss_aug_tea: 1.3450,   \n",
            "loss_cls: 0.0768,   \n",
            "loss_aug: 0.0772,   \n",
            "loss_ori_tea: 1.1188,   \n",
            "loss_aug_tea: 1.0988,   \n",
            "loss_cls: 0.0568,   \n",
            "loss_aug: 0.0571,   \n",
            "loss_ori_tea: 1.0113,   \n",
            "loss_aug_tea: 1.1068,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0784,   \n",
            "loss_ori_tea: 1.2680,   \n",
            "loss_aug_tea: 1.2871,   \n",
            "loss_cls: 0.0679,   \n",
            "loss_aug: 0.0684,   \n",
            "loss_ori_tea: 0.9259,   \n",
            "loss_aug_tea: 1.1489,   \n",
            "loss_cls: 0.0893,   \n",
            "loss_aug: 0.0897,   \n",
            "loss_ori_tea: 1.0909,   \n",
            "loss_aug_tea: 1.2337,   \n",
            "loss_cls: 0.0801,   \n",
            "loss_aug: 0.0806,   \n",
            "loss_ori_tea: 0.9706,   \n",
            "loss_aug_tea: 1.0451,   \n",
            "loss_cls: 0.0617,   \n",
            "loss_aug: 0.0623,   \n",
            "loss_ori_tea: 0.9161,   \n",
            "loss_aug_tea: 1.3133,   \n",
            "loss_cls: 0.0638,   \n",
            "loss_aug: 0.0641,   \n",
            "loss_ori_tea: 1.0853,   \n",
            "loss_aug_tea: 1.1337,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0823,   \n",
            "loss_ori_tea: 1.2034,   \n",
            "loss_aug_tea: 0.9009,   \n",
            "loss_cls: 0.0745,   \n",
            "loss_aug: 0.0749,   \n",
            "loss_ori_tea: 1.1006,   \n",
            "loss_aug_tea: 1.6098,   \n",
            "loss_cls: 0.0735,   \n",
            "loss_aug: 0.0736,   \n",
            "loss_ori_tea: 0.8165,   \n",
            "loss_aug_tea: 1.0364,   \n",
            "Epoch [14/15],   Mean Train Loss: 4.1893,   Mean Train Dice: 0.9277,   Mean Val Dice: 0.9625,   Mean Val ASD: 0.0019,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0709,   \n",
            "loss_ori_tea: 1.1062,   \n",
            "loss_aug_tea: 1.0240,   \n",
            "loss_cls: 0.0744,   \n",
            "loss_aug: 0.0748,   \n",
            "loss_ori_tea: 0.9857,   \n",
            "loss_aug_tea: 0.8804,   \n",
            "loss_cls: 0.0718,   \n",
            "loss_aug: 0.0721,   \n",
            "loss_ori_tea: 1.0281,   \n",
            "loss_aug_tea: 1.0355,   \n",
            "loss_cls: 0.0568,   \n",
            "loss_aug: 0.0571,   \n",
            "loss_ori_tea: 1.3947,   \n",
            "loss_aug_tea: 1.0073,   \n",
            "loss_cls: 0.0874,   \n",
            "loss_aug: 0.0881,   \n",
            "loss_ori_tea: 0.9368,   \n",
            "loss_aug_tea: 1.0553,   \n",
            "loss_cls: 0.0728,   \n",
            "loss_aug: 0.0729,   \n",
            "loss_ori_tea: 1.4780,   \n",
            "loss_aug_tea: 0.8876,   \n",
            "loss_cls: 0.0680,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 1.0371,   \n",
            "loss_aug_tea: 0.8190,   \n",
            "loss_cls: 0.0968,   \n",
            "loss_aug: 0.0971,   \n",
            "loss_ori_tea: 0.9621,   \n",
            "loss_aug_tea: 0.8740,   \n",
            "loss_cls: 0.0704,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 1.1725,   \n",
            "loss_aug_tea: 1.0565,   \n",
            "loss_cls: 0.0564,   \n",
            "loss_aug: 0.0566,   \n",
            "loss_ori_tea: 1.1256,   \n",
            "loss_aug_tea: 0.9384,   \n",
            "loss_cls: 0.0664,   \n",
            "loss_aug: 0.0667,   \n",
            "loss_ori_tea: 0.9173,   \n",
            "loss_aug_tea: 1.0738,   \n",
            "loss_cls: 0.0722,   \n",
            "loss_aug: 0.0723,   \n",
            "loss_ori_tea: 0.9110,   \n",
            "loss_aug_tea: 1.2311,   \n",
            "loss_cls: 0.0795,   \n",
            "loss_aug: 0.0794,   \n",
            "loss_ori_tea: 1.3519,   \n",
            "loss_aug_tea: 0.9898,   \n",
            "loss_cls: 0.0872,   \n",
            "loss_aug: 0.0876,   \n",
            "loss_ori_tea: 0.9726,   \n",
            "loss_aug_tea: 1.0408,   \n",
            "loss_cls: 0.0820,   \n",
            "loss_aug: 0.0825,   \n",
            "loss_ori_tea: 0.8891,   \n",
            "loss_aug_tea: 0.9989,   \n",
            "loss_cls: 0.0670,   \n",
            "loss_aug: 0.0673,   \n",
            "loss_ori_tea: 0.8428,   \n",
            "loss_aug_tea: 1.1003,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 1.0226,   \n",
            "loss_aug_tea: 1.0197,   \n",
            "loss_cls: 0.0597,   \n",
            "loss_aug: 0.0598,   \n",
            "loss_ori_tea: 1.4233,   \n",
            "loss_aug_tea: 0.6649,   \n",
            "loss_cls: 0.0683,   \n",
            "loss_aug: 0.0686,   \n",
            "loss_ori_tea: 0.9650,   \n",
            "loss_aug_tea: 1.0078,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0630,   \n",
            "loss_ori_tea: 1.2255,   \n",
            "loss_aug_tea: 1.1349,   \n",
            "loss_cls: 0.0611,   \n",
            "loss_aug: 0.0613,   \n",
            "loss_ori_tea: 0.8619,   \n",
            "loss_aug_tea: 1.0191,   \n",
            "loss_cls: 0.0783,   \n",
            "loss_aug: 0.0786,   \n",
            "loss_ori_tea: 1.0787,   \n",
            "loss_aug_tea: 1.1576,   \n",
            "loss_cls: 0.0759,   \n",
            "loss_aug: 0.0763,   \n",
            "loss_ori_tea: 0.7696,   \n",
            "loss_aug_tea: 1.0777,   \n",
            "loss_cls: 0.0633,   \n",
            "loss_aug: 0.0635,   \n",
            "loss_ori_tea: 0.8814,   \n",
            "loss_aug_tea: 0.8926,   \n",
            "loss_cls: 0.0830,   \n",
            "loss_aug: 0.0831,   \n",
            "loss_ori_tea: 1.1012,   \n",
            "loss_aug_tea: 0.8676,   \n",
            "loss_cls: 0.0699,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 1.0054,   \n",
            "loss_aug_tea: 0.8893,   \n",
            "loss_cls: 0.0732,   \n",
            "loss_aug: 0.0733,   \n",
            "loss_ori_tea: 1.0538,   \n",
            "loss_aug_tea: 1.0587,   \n",
            "loss_cls: 0.0669,   \n",
            "loss_aug: 0.0671,   \n",
            "loss_ori_tea: 0.9235,   \n",
            "loss_aug_tea: 1.2713,   \n",
            "loss_cls: 0.0789,   \n",
            "loss_aug: 0.0792,   \n",
            "loss_ori_tea: 0.8372,   \n",
            "loss_aug_tea: 0.9918,   \n",
            "loss_cls: 0.0712,   \n",
            "loss_aug: 0.0715,   \n",
            "loss_ori_tea: 0.8720,   \n",
            "loss_aug_tea: 0.9763,   \n",
            "loss_cls: 0.0676,   \n",
            "loss_aug: 0.0678,   \n",
            "loss_ori_tea: 0.8270,   \n",
            "loss_aug_tea: 1.1009,   \n",
            "loss_cls: 0.0682,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.9418,   \n",
            "loss_aug_tea: 0.9327,   \n",
            "loss_cls: 0.0777,   \n",
            "loss_aug: 0.0779,   \n",
            "loss_ori_tea: 0.8937,   \n",
            "loss_aug_tea: 0.9915,   \n",
            "loss_cls: 0.0858,   \n",
            "loss_aug: 0.0860,   \n",
            "loss_ori_tea: 1.1496,   \n",
            "loss_aug_tea: 0.8444,   \n",
            "loss_cls: 0.0780,   \n",
            "loss_aug: 0.0782,   \n",
            "loss_ori_tea: 1.1508,   \n",
            "loss_aug_tea: 1.0179,   \n",
            "loss_cls: 0.0726,   \n",
            "loss_aug: 0.0728,   \n",
            "loss_ori_tea: 0.8308,   \n",
            "loss_aug_tea: 0.8876,   \n",
            "loss_cls: 0.0703,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.9641,   \n",
            "loss_aug_tea: 1.6186,   \n",
            "loss_cls: 0.0525,   \n",
            "loss_aug: 0.0527,   \n",
            "loss_ori_tea: 1.2778,   \n",
            "loss_aug_tea: 1.0550,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0713,   \n",
            "loss_ori_tea: 1.0435,   \n",
            "loss_aug_tea: 1.0481,   \n",
            "loss_cls: 0.0713,   \n",
            "loss_aug: 0.0715,   \n",
            "loss_ori_tea: 0.9490,   \n",
            "loss_aug_tea: 1.0036,   \n",
            "loss_cls: 0.0702,   \n",
            "loss_aug: 0.0705,   \n",
            "loss_ori_tea: 0.9562,   \n",
            "loss_aug_tea: 1.1021,   \n",
            "loss_cls: 0.0623,   \n",
            "loss_aug: 0.0624,   \n",
            "loss_ori_tea: 1.0987,   \n",
            "loss_aug_tea: 0.9786,   \n",
            "loss_cls: 0.0712,   \n",
            "loss_aug: 0.0716,   \n",
            "loss_ori_tea: 0.9438,   \n",
            "loss_aug_tea: 1.0427,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0633,   \n",
            "loss_ori_tea: 0.7323,   \n",
            "loss_aug_tea: 1.0247,   \n",
            "loss_cls: 0.0750,   \n",
            "loss_aug: 0.0750,   \n",
            "loss_ori_tea: 1.0170,   \n",
            "loss_aug_tea: 1.1440,   \n",
            "loss_cls: 0.0772,   \n",
            "loss_aug: 0.0775,   \n",
            "loss_ori_tea: 0.9799,   \n",
            "loss_aug_tea: 0.9647,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0695,   \n",
            "loss_ori_tea: 1.1016,   \n",
            "loss_aug_tea: 0.8098,   \n",
            "loss_cls: 0.0671,   \n",
            "loss_aug: 0.0674,   \n",
            "loss_ori_tea: 0.9195,   \n",
            "loss_aug_tea: 0.9781,   \n",
            "loss_cls: 0.0692,   \n",
            "loss_aug: 0.0696,   \n",
            "loss_ori_tea: 1.1012,   \n",
            "loss_aug_tea: 1.2771,   \n",
            "loss_cls: 0.0753,   \n",
            "loss_aug: 0.0758,   \n",
            "loss_ori_tea: 0.9122,   \n",
            "loss_aug_tea: 1.0239,   \n",
            "loss_cls: 0.0690,   \n",
            "loss_aug: 0.0695,   \n",
            "loss_ori_tea: 0.9285,   \n",
            "loss_aug_tea: 0.9779,   \n",
            "loss_cls: 0.0602,   \n",
            "loss_aug: 0.0603,   \n",
            "loss_ori_tea: 1.4669,   \n",
            "loss_aug_tea: 0.8945,   \n",
            "loss_cls: 0.0646,   \n",
            "loss_aug: 0.0650,   \n",
            "loss_ori_tea: 1.3334,   \n",
            "loss_aug_tea: 0.9800,   \n",
            "loss_cls: 0.0737,   \n",
            "loss_aug: 0.0738,   \n",
            "loss_ori_tea: 0.9180,   \n",
            "loss_aug_tea: 1.1282,   \n",
            "loss_cls: 0.0645,   \n",
            "loss_aug: 0.0647,   \n",
            "loss_ori_tea: 0.8382,   \n",
            "loss_aug_tea: 1.1884,   \n",
            "loss_cls: 0.0688,   \n",
            "loss_aug: 0.0691,   \n",
            "loss_ori_tea: 0.9587,   \n",
            "loss_aug_tea: 0.7822,   \n",
            "loss_cls: 0.0700,   \n",
            "loss_aug: 0.0702,   \n",
            "loss_ori_tea: 0.9110,   \n",
            "loss_aug_tea: 0.8630,   \n",
            "loss_cls: 0.0812,   \n",
            "loss_aug: 0.0814,   \n",
            "loss_ori_tea: 0.9033,   \n",
            "loss_aug_tea: 0.9000,   \n",
            "loss_cls: 0.0684,   \n",
            "loss_aug: 0.0687,   \n",
            "loss_ori_tea: 0.6988,   \n",
            "loss_aug_tea: 1.1524,   \n",
            "loss_cls: 0.0681,   \n",
            "loss_aug: 0.0683,   \n",
            "loss_ori_tea: 1.1221,   \n",
            "loss_aug_tea: 0.9152,   \n",
            "loss_cls: 0.0787,   \n",
            "loss_aug: 0.0788,   \n",
            "loss_ori_tea: 1.0731,   \n",
            "loss_aug_tea: 0.9570,   \n",
            "loss_cls: 0.0782,   \n",
            "loss_aug: 0.0787,   \n",
            "loss_ori_tea: 1.3580,   \n",
            "loss_aug_tea: 1.1422,   \n",
            "loss_cls: 0.0701,   \n",
            "loss_aug: 0.0704,   \n",
            "loss_ori_tea: 0.8016,   \n",
            "loss_aug_tea: 1.2176,   \n",
            "loss_cls: 0.0629,   \n",
            "loss_aug: 0.0634,   \n",
            "loss_ori_tea: 0.8931,   \n",
            "loss_aug_tea: 1.0114,   \n",
            "loss_cls: 0.0814,   \n",
            "loss_aug: 0.0816,   \n",
            "loss_ori_tea: 0.9682,   \n",
            "loss_aug_tea: 1.4791,   \n",
            "loss_cls: 0.0816,   \n",
            "loss_aug: 0.0820,   \n",
            "loss_ori_tea: 1.0712,   \n",
            "loss_aug_tea: 0.8778,   \n",
            "Epoch [15/15],   Mean Train Loss: 4.1481,   Mean Train Dice: 0.9283,   Mean Val Dice: 0.9621,   Mean Val ASD: 0.0019,   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss curve\n",
        "fig = plt.figure()\n",
        "plt.plot(loss_total_list, label='Total Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot accuracy curve\n",
        "fig = plt.figure()\n",
        "plt.plot(dice_train_list, label='Training DICE')\n",
        "plt.plot(dice_val_list, label='Validation DICE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('DICE')\n",
        "plt.title('DICE Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot AUC curve\n",
        "fig = plt.figure()\n",
        "plt.plot(asd_val_list, label='Validation ASD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('ASD')\n",
        "plt.title('ASD Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AR-8B9aJH3gT",
        "outputId": "30338538-ec87-434e-a2f5-26bd1947a12e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for it, (batch) in enumerate(val_loader):\n",
        "    if it > 0 and it < 2:\n",
        "        data, labels = batch[0].to(device), batch[1].to(device)\n",
        "        outputs = mean_teacher_model.student_model(data)\n",
        "        outputs = F.softmax(outputs,dim=1)\n",
        "      \n",
        "        plt.figure()\n",
        "        plt.title(\"Prediction \"+str(it))\n",
        "        image = outputs.cpu().detach().numpy()[0].reshape(3,64,64)\n",
        "        imgplot1 = plt.imshow(image[0])\n",
        "        plt.show(imgplot1)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title(\"Prediction \"+str(it))\n",
        "        image = outputs.cpu().detach().numpy()[1].reshape(3,64,64)\n",
        "        imgplot1 = plt.imshow(image[0])\n",
        "        plt.show(imgplot1)\n",
        "        plt.figure()\n",
        "        plt.title(\"Prediction \"+str(it))\n",
        "        image = outputs.cpu().detach().numpy()[2].reshape(3,64,64)\n",
        "        imgplot1 = plt.imshow(image[0])\n",
        "        plt.show(imgplot1)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title(\"Label \"+str(it))\n",
        "        image = labels.cpu().detach().numpy()[0].reshape(3,64,64)\n",
        "        imgplot2 = plt.imshow(image[0])\n",
        "        plt.show(imgplot2)\n"
      ],
      "metadata": {
        "id": "Sppl_mn5pF3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}