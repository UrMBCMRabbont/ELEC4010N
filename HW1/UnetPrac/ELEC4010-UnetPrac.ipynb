{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkzyBLUwIM1T48ZvCml++P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ILUE3AAagvv9","executionInfo":{"status":"ok","timestamp":1678172072366,"user_tz":-480,"elapsed":6359,"user":{"displayName":"BonKong Lai","userId":"14934257860777894140"}}},"outputs":[],"source":["import torch.nn as nn\n","from torch.nn.modules.loss import _Loss\n","\n","\n","class SoftDiceLoss(_Loss):\n","    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n","        super(SoftDiceLoss, self).__init__(size_average, reduce, reduction)\n","\n","    def forward(self, y_pred, y_gt):\n","        numerator = torch.sum(y_pred*y_gt)\n","        denominator = torch.sum(y_pred*y_pred + y_gt*y_gt)\n","        return numerator/denominator\n","\n","\n","class First2D(nn.Module):\n","    def __init__(self, in_channels, middle_channels, out_channels, dropout=False):\n","        super(First2D, self).__init__()\n","\n","        layers = [\n","            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(middle_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        ]\n","\n","        if dropout:\n","            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n","            layers.append(nn.Dropout2d(p=dropout))\n","\n","        self.first = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        print(\"Before First2D: \", x.shape)\n","        return self.first(x)\n","\n","\n","class Encoder2D(nn.Module):\n","    def __init__(\n","            self, in_channels, middle_channels, out_channels,\n","            dropout=False, downsample_kernel=2\n","    ):\n","        super(Encoder2D, self).__init__()\n","\n","        layers = [\n","            nn.MaxPool2d(kernel_size=downsample_kernel),\n","            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(middle_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        ]\n","\n","        if dropout:\n","            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n","            layers.append(nn.Dropout2d(p=dropout))\n","\n","        self.encoder = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        print(\"Before Encoder2D: \", x.shape)\n","        return self.encoder(x)\n","\n","\n","class Center2D(nn.Module):\n","    def __init__(self, in_channels, middle_channels, out_channels, deconv_channels, dropout=False):\n","        super(Center2D, self).__init__()\n","\n","        layers = [\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(middle_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(out_channels, deconv_channels, kernel_size=2, stride=2)\n","        ]\n","\n","        if dropout:\n","            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n","            layers.append(nn.Dropout2d(p=dropout))\n","\n","        self.center = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        print(\"Before Center2D: \", x.shape)\n","        return self.center(x)\n","\n","\n","class Decoder2D(nn.Module):\n","    def __init__(self, in_channels, middle_channels, out_channels, deconv_channels, dropout=False):\n","        super(Decoder2D, self).__init__()\n","\n","        layers = [\n","            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(middle_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(out_channels, deconv_channels, kernel_size=2, stride=2)\n","        ]\n","\n","        if dropout:\n","            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n","            layers.append(nn.Dropout2d(p=dropout))\n","\n","        self.decoder = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        print(\"Before Decoder2D: \", x.shape)\n","        return self.decoder(x)\n","\n","\n","class Last2D(nn.Module):\n","    def __init__(self, in_channels, middle_channels, out_channels, softmax=False):\n","        super(Last2D, self).__init__()\n","\n","        layers = [\n","            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(middle_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(middle_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(middle_channels, out_channels, kernel_size=1),\n","            nn.Softmax(dim=1)\n","        ]\n","\n","        self.first = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        print(\"Before Last2D: \", x.shape)\n","        return self.first(x)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class UNet2D(nn.Module):\n","    def __init__(self, in_channels, out_channels, conv_depths=(64, 128, 256, 512, 1024)):\n","        assert len(conv_depths) > 2, 'conv_depths must have at least 3 members'\n","\n","        super(UNet2D, self).__init__()\n","\n","        # defining encoder layers\n","        encoder_layers = []\n","        encoder_layers.append(First2D(in_channels, conv_depths[0], conv_depths[0]))\n","        encoder_layers.extend([Encoder2D(conv_depths[i], conv_depths[i + 1], conv_depths[i + 1])\n","                               for i in range(len(conv_depths)-2)])\n","\n","        # defining decoder layers\n","        decoder_layers = []\n","        decoder_layers.extend([Decoder2D(2 * conv_depths[i + 1], 2 * conv_depths[i], 2 * conv_depths[i], conv_depths[i])\n","                               for i in reversed(range(len(conv_depths)-2))])\n","        decoder_layers.append(Last2D(conv_depths[1], conv_depths[0], out_channels))\n","\n","        # encoder, center and decoder layers\n","        self.encoder_layers = nn.Sequential(*encoder_layers)\n","        self.center = Center2D(conv_depths[-2], conv_depths[-1], conv_depths[-1], conv_depths[-2])\n","        self.decoder_layers = nn.Sequential(*decoder_layers)\n","\n","    def forward(self, x, return_all=False):\n","        x_enc = [x]\n","        for enc_layer in self.encoder_layers:\n","            x_enc.append(enc_layer(x_enc[-1]))\n","\n","        x_dec = [self.center(x_enc[-1])]\n","        for dec_layer_idx, dec_layer in enumerate(self.decoder_layers):\n","            x_opposite = x_enc[-1-dec_layer_idx]\n","            x_cat = torch.cat(\n","                [pad_to_shape(x_dec[-1], x_opposite.shape), x_opposite],\n","                dim=1\n","            )\n","            x_dec.append(dec_layer(x_cat))\n","\n","        if not return_all:\n","            return x_dec[-1]\n","        else:\n","            return x_enc + x_dec\n","\n","\n","def pad_to_shape(this, shp):\n","    \"\"\"\n","    Pads this image with zeroes to shp.\n","    Args:\n","        this: image tensor to pad\n","        shp: desired output shape\n","\n","    Returns:\n","        Zero-padded tensor of shape shp.\n","    \"\"\"\n","    if len(shp) == 4:\n","        pad = (0, shp[3] - this.shape[3], 0, shp[2] - this.shape[2])\n","    elif len(shp) == 5:\n","        pad = (0, shp[4] - this.shape[4], 0, shp[3] - this.shape[3], 0, shp[2] - this.shape[2])\n","    return F.pad(this, pad)"],"metadata":{"id":"FCOgOD-qionG","executionInfo":{"status":"ok","timestamp":1678172072367,"user_tz":-480,"elapsed":6,"user":{"displayName":"BonKong Lai","userId":"14934257860777894140"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Author: Yonglong Tian (yonglong@mit.edu)\n","Date: May 07, 2020\n","\"\"\"\n","from __future__ import print_function\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class SupConLoss(nn.Module):\n","    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n","    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n","    def __init__(self, temperature=0.07, contrast_mode='all',\n","                 base_temperature=0.07):\n","        super(SupConLoss, self).__init__()\n","        self.temperature = temperature\n","        self.contrast_mode = contrast_mode\n","        self.base_temperature = base_temperature\n","\n","    def forward(self, features, labels=None, mask=None):\n","        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n","        it degenerates to SimCLR unsupervised loss:\n","        https://arxiv.org/pdf/2002.05709.pdf\n","\n","        Args:\n","            features: hidden vector of shape [bsz, n_views, ...].\n","            labels: ground truth of shape [bsz].\n","            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n","                has the same class as sample i. Can be asymmetric.\n","        Returns:\n","            A loss scalar.\n","        \"\"\"\n","        device = (torch.device('cuda')\n","                  if features.is_cuda\n","                  else torch.device('cpu'))\n","\n","        if len(features.shape) < 3:\n","            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n","                             'at least 3 dimensions are required')\n","        if len(features.shape) > 3:\n","            features = features.view(features.shape[0], features.shape[1], -1)\n","\n","        batch_size = features.shape[0]\n","        if labels is not None and mask is not None:\n","            raise ValueError('Cannot define both `labels` and `mask`')\n","        elif labels is None and mask is None:\n","            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n","        elif labels is not None:\n","            labels = labels.contiguous().view(-1, 1)\n","            if labels.shape[0] != batch_size:\n","                raise ValueError('Num of labels does not match num of features')\n","            mask = torch.eq(labels, labels.T).float().to(device)\n","        else:\n","            mask = mask.float().to(device)\n","\n","        contrast_count = features.shape[1]\n","        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n","        if self.contrast_mode == 'one':\n","            anchor_feature = features[:, 0]\n","            anchor_count = 1\n","        elif self.contrast_mode == 'all':\n","            anchor_feature = contrast_feature\n","            anchor_count = contrast_count\n","        else:\n","            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n","\n","        # compute logits\n","        anchor_dot_contrast = torch.div(\n","            torch.matmul(anchor_feature, contrast_feature.T),\n","            self.temperature)\n","        # for numerical stability\n","        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n","        logits = anchor_dot_contrast - logits_max.detach()\n","\n","        # tile mask\n","        mask = mask.repeat(anchor_count, contrast_count)\n","        # mask-out self-contrast cases\n","        logits_mask = torch.scatter(\n","            torch.ones_like(mask),\n","            1,\n","            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n","            0\n","        )\n","        mask = mask * logits_mask\n","\n","        # compute log_prob\n","        exp_logits = torch.exp(logits) * logits_mask\n","        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n","\n","        # compute mean of log-likelihood over positive\n","        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n","\n","        # loss\n","        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n","        loss = loss.view(anchor_count, batch_size).mean()\n","\n","        return loss"],"metadata":{"id":"ltKEa4092XTv","executionInfo":{"status":"ok","timestamp":1678172076459,"user_tz":-480,"elapsed":1029,"user":{"displayName":"BonKong Lai","userId":"14934257860777894140"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["img = torch.rand(64, 3, 28, 28)\n","model = UNet2D(3, 23)\n","output = model.forward(img, return_all=False)\n","print(\"Output: \", output.shape)\n","labels = torch.rand(64)\n","\n","criterion = SupConLoss()\n","loss = criterion(output, labels)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRIYv2Y3jcJW","executionInfo":{"status":"ok","timestamp":1678172519900,"user_tz":-480,"elapsed":2273,"user":{"displayName":"BonKong Lai","userId":"14934257860777894140"}},"outputId":"79a1f0c7-8767-4560-f2b1-5c3c343462dc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Before First2D:  torch.Size([64, 3, 28, 28])\n","Before Encoder2D:  torch.Size([64, 64, 28, 28])\n","Before Encoder2D:  torch.Size([64, 128, 14, 14])\n","Before Encoder2D:  torch.Size([64, 256, 7, 7])\n","Before Center2D:  torch.Size([64, 512, 3, 3])\n","Before Decoder2D:  torch.Size([64, 1024, 3, 3])\n","Before Decoder2D:  torch.Size([64, 512, 7, 7])\n","Before Decoder2D:  torch.Size([64, 256, 14, 14])\n","Before Last2D:  torch.Size([64, 128, 28, 28])\n","Output:  torch.Size([64, 23, 28, 28])\n","tensor(12.7993, grad_fn=<MeanBackward0>)\n"]}]}]}