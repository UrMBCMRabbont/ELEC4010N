{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jLPJjp76lZMY"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "\n",
        "Baseline code: https://github.com/emma-sjwang/Dofe"
      ],
      "metadata": {
        "id": "z7rWTqmmi34W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm Implementation\n",
        "\n",
        "1. Implement the naive baseline model for comparison. This is typically a simple model with basic feature extraction and classification steps.\n",
        "\n",
        "2. Choose at least one Domain Generalization (DG) method to implement and compare against the naive baseline. This could be FACT, Dofe, or another method of your choice.\n",
        "\n",
        "3. Report the segmentation performance in terms of dice coefficient and average surface distance."
      ],
      "metadata": {
        "id": "ikoXfMF_lOJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and unzip data\n",
        "# Download from https://drive.google.com/u/0/uc?id=1p33nsWQaiZMAgsruDoJLyatoq5XAH-TH&export=download\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2\"\n",
        "!unzip \"/content/gdrive/MyDrive/Colab Notebooks/ELEC4010N/Final Project/Project2/Fundus-doFE.zip\" -d \"/content/\"\n",
        "\n",
        "# Install additional libraries\n",
        "!pip install -U albumentations\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install torchmetrics\n",
        "!git clone https://github.com/deepmind/surface-distance.git\n",
        "!pip install surface-distance/\n",
        "!pip install MedPy"
      ],
      "metadata": {
        "id": "IBA8GHAbJ-Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Baseline Model"
      ],
      "metadata": {
        "id": "jLPJjp76lZMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just for reference\n",
        "# UNet implementation\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.conv_down1 = double_conv(3, 64)      # Number of channel\n",
        "        self.conv_down2 = double_conv(64, 128)\n",
        "        self.conv_down3 = double_conv(128, 256)\n",
        "        self.conv_down4 = double_conv(256, 512)        \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        self.conv_up3 = double_conv(256 + 512, 256)\n",
        "        self.conv_up2 = double_conv(128 + 256, 128)\n",
        "        self.conv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.last_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        conv2 = self.conv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        conv3 = self.conv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "        x = self.conv_down4(x)\n",
        "        x = self.upsample(x)\n",
        "        \n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "        \n",
        "        x = self.conv_up1(x)\n",
        "        out = self.last_conv(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "2cHXXW2wj-Cj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "da2cf772-f401-4e18-c1e7-432036a0e30e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6ff8d284c3c9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         nn.ReLU(inplace=True))\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model = UNet(num_classes=1).to(device)\n",
        "output = model(torch.randn(1,3,256,256).to(device))\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "1LQxOa7tkAA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FACT\n",
        "\n",
        "https://github.com/MediaBrain-SJTU/FACT"
      ],
      "metadata": {
        "id": "gEwWV9bcle8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import math\n",
        "import copy\n",
        "import gc\n",
        "import random as randpy\n",
        "from copy import deepcopy\n",
        "from math import sqrt\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageFilter\n",
        "from pylab import *\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from ast import In\n",
        "import albumentations as Augm\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# PyTorch imports for deep learning\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50 as _resnet50\n",
        "from torchsummary import summary\n",
        "\n",
        "# Utilities for image data handling\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# For segmentation models\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Metrics\n",
        "from medpy import metric\n",
        "from segmentation_models_pytorch.losses import DiceLoss\n",
        "import surface_distance as surfdist\n",
        "from torchmetrics.classification import BinaryJaccardIndex\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "w_qdq2stHBS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e713df53-7c5b-404f-8297-79803e96b384"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset info\n",
        "[cup]Domian1: Drishti-GS dataset [101] including training[50] and testing[51]\n",
        "\n",
        "[cup]Domain2: RIM-ONE_r3 dataset [159] including training and[99] testing[60]. \n",
        "\n",
        "[cup]Domain3: REFUGE training [400]  MICCAI 2018 workshop including training and[320] testing[80]. \n",
        "\n",
        "[cup]Domian4: REFUGE val [400]  including training and[320] testing[80]. \n",
        "Domain5: ISBI [81]  IDRID chanllenge\n",
        "\n"
      ],
      "metadata": {
        "id": "KB_yBwBhTu45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmXdlqNFhHCN",
        "outputId": "bc979d82-50e6-4ecd-b902-7d1d46c51e3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to move the data\n",
        "train_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "\n",
        "# Define the path to load the data\n",
        "D_mask_dir = [[\"./Fundus/Domain1/train/ROIs/mask\", \"./Fundus/Domain1/test/ROIs/mask\"],\n",
        "              [\"./Fundus/Domain2/train/ROIs/mask\", \"./Fundus/Domain2/test/ROIs/mask\"],\n",
        "              [\"./Fundus/Domain3/train/ROIs/mask\", \"./Fundus/Domain3/test/ROIs/mask\"],\n",
        "              [\"./Fundus/Domain4/train/ROIs/mask\", \"./Fundus/Domain4/test/ROIs/mask\"]]\n",
        "\n",
        "# Define the fundus dataset class to load the data by functions\n",
        "class FundusDataset(Dataset):\n",
        "    # Define the constructor\n",
        "    def __init__(self, path, transform, transform_mask, split_idx):\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        self.transform_mask = transform_mask\n",
        "        self.split_idx = split_idx\n",
        "        self.path_df = self.create_path_df()\n",
        "    \n",
        "    # Creates a DataFrame with image and mask paths\n",
        "    def create_path_df(self):\n",
        "        dirsImg, images, dirsMask, masks = [], [], [], []\n",
        "        \n",
        "        for mask_dir in self.path:\n",
        "            for img_path in mask_dir:\n",
        "                for _, _, files in os.walk(img_path):\n",
        "                    for file in files:\n",
        "                        check_mask = cv2.imread(os.path.join(img_path, file))\n",
        "                        \n",
        "                        if check_mask.sum() > 0:\n",
        "                            domain = re.search(r'Domain\\d', img_path).group() # Extract domain\n",
        "                            images.append(file)\n",
        "                            masks.append(file)\n",
        "                            dirsMask.append(img_path+'/')\n",
        "                            dirsImg.append(img_path.replace('mask','image')+'/')\n",
        "\n",
        "        return pd.DataFrame({'direcImg':dirsImg, 'images':images, 'direcMask':dirsMask, 'masks':masks})\n",
        "    \n",
        "    # Splits the train dataset into train and validation based on the split index\n",
        "    def split_traindataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1] + '/' + self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1] + '/' + self.path_df.loc[i]['masks']\n",
        "            domain = re.search(r'Domain\\d', image_direc).group() if image_direc else None\n",
        "            \n",
        "            # Split the dataset based on the project description\n",
        "            if domain:\n",
        "                if domain != \"Domain4\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain3\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain2\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "                elif domain != \"Domain1\":\n",
        "                    destination_path = train_path if i < int(0.8 * len(self.path_df)) else val_path\n",
        "                    shutil.copy(image_direc, destination_path + \"/image/\")\n",
        "                    shutil.copy(mask_direc, destination_path + \"/mask/\")\n",
        "\n",
        "    # Moves the test dataset to the test path\n",
        "    def split_testdataset(self):\n",
        "        for i in range(len(self.path_df)):\n",
        "            image_direc = self.path_df.loc[i]['direcImg'][:-1]+'/'+self.path_df.loc[i]['images']\n",
        "            mask_direc = self.path_df.loc[i]['direcMask'][:-1]+'/'+self.path_df.loc[i]['masks']\n",
        "            shutil.copy(image_direc, test_path+\"/image/\")\n",
        "            shutil.copy(mask_direc, test_path+\"/mask/\")\n",
        "\n",
        "    # Returns length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.path_df)\n",
        "\n",
        "    #  Creates a list of image and mask paths\n",
        "    def direc(self):\n",
        "        FundusImg_direc = []\n",
        "        Mask_direc = []\n",
        "        train_folders = os.listdir(train_path)\n",
        "        \n",
        "        for folder in train_folders:\n",
        "            files = os.listdir(os.path.join(train_path, folder))\n",
        "            \n",
        "            for file in files:\n",
        "                direc = os.path.join(train_path, folder, file)\n",
        "                \n",
        "                if 'mask' in folder:\n",
        "                    Mask_direc.append(direc)\n",
        "                else:\n",
        "                    FundusImg_direc.append(direc)\n",
        "        \n",
        "        return FundusImg_direc, Mask_direc"
      ],
      "metadata": {
        "id": "d9xTyMmkHiT6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the train, validation, and test folders\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path)\n",
        "    os.makedirs(os.path.join(path, \"image\"))\n",
        "    os.makedirs(os.path.join(path, \"mask\"))\n",
        "\n",
        "# Normalization\n",
        "normalization = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(64),\n",
        "    transforms.RandomCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "# train = 1,2,3 test= 4\n",
        "train_class = FundusDataset(D_mask_dir[:3], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "test_class = FundusDataset(D_mask_dir[3:], transform=normalization, transform_mask=normalization, split_idx=1)\n",
        "\n",
        "# Split files of system\n",
        "train_class.split_traindataset()\n",
        "test_class.split_testdataset()"
      ],
      "metadata": {
        "id": "4p8xLZdGxAqZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data_utils.py"
      ],
      "metadata": {
        "id": "ASCCtcZzsZbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_info(filepath):\n",
        "    img_direcs = []\n",
        "    mask_direcs = []\n",
        "    for folder in os.listdir(filepath):\n",
        "        files = os.listdir(filepath+'/'+folder)\n",
        "        for file in files:\n",
        "            direc = filepath+'/'+folder+'/'+file\n",
        "            if 'mask' in direc: mask_direcs.append(direc)\n",
        "            else: img_direcs.append(direc)\n",
        "    return img_direcs, mask_direcs\n",
        "\n",
        "\n",
        "def get_img_transform(train=False, image_size=224, crop=False, jitter=0):\n",
        "    mean = [0.5]\n",
        "    std = [0.5]\n",
        "    if train:\n",
        "        if crop:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "        else:\n",
        "            img_transform = [transforms.ToPILImage(),transforms.Resize((image_size, image_size))]\n",
        "        if jitter > 0:\n",
        "            img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                        contrast=jitter,\n",
        "                                                        saturation=jitter,\n",
        "                                                        hue=min(0.5, jitter)))\n",
        "        img_transform += [transforms.RandomHorizontalFlip(),\n",
        "                          transforms.Normalize(mean, std)]\n",
        "        img_transform = transforms.Compose(img_transform)\n",
        "    else:\n",
        "        img_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "    return img_transform\n",
        "\n",
        "def get_label_transform(train=False, image_size=224, crop=False, jitter=0):\n",
        "    mean = [0.5]\n",
        "    std = [0.5]\n",
        "    if train:\n",
        "        if crop:\n",
        "            label_transform = [transforms.ToPILImage(),transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0], interpolation=Image.NEAREST)]\n",
        "        else:\n",
        "            label_transform = [transforms.ToPILImage(),transforms.Resize((image_size, image_size), interpolation=Image.NEAREST)]\n",
        "        label_transform += [transforms.RandomHorizontalFlip(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize(mean, std)]\n",
        "        label_transform = transforms.Compose(label_transform)\n",
        "    else:\n",
        "        label_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((image_size, image_size), interpolation=Image.NEAREST),\n",
        "        ])\n",
        "    return label_transform\n",
        "    \n",
        "\n",
        "def get_pre_transform(image_size=224, crop=False, jitter=0):\n",
        "    if crop:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.RandomResizedCrop(image_size, scale=[0.8, 1.0])]\n",
        "    else:\n",
        "        img_transform = [transforms.ToPILImage(), transforms.Resize((image_size, image_size))]\n",
        "    if jitter > 0:\n",
        "        img_transform.append(transforms.ColorJitter(brightness=jitter,\n",
        "                                                    contrast=jitter,\n",
        "                                                    saturation=jitter,\n",
        "                                                    hue=min(0.5, jitter)))\n",
        "    img_transform += [transforms.RandomHorizontalFlip(), lambda x: np.asarray(x)]\n",
        "    img_transform = transforms.Compose(img_transform)\n",
        "    return img_transform\n",
        "\n",
        "def get_post_transform(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    img_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    return img_transform\n",
        "\n",
        "def colorful_spectrum_mix(img1, img2, alpha, ratio=1.0):\n",
        "    \"\"\"Input image size: ndarray of [H, W, C]\"\"\"\n",
        "    lam = np.random.uniform(0, alpha)\n",
        "\n",
        "    assert img1.shape == img2.shape\n",
        "    h, w, c = img1.shape\n",
        "    h_crop = int(h * sqrt(ratio))\n",
        "    w_crop = int(w * sqrt(ratio))\n",
        "    h_start = h // 2 - h_crop // 2\n",
        "    w_start = w // 2 - w_crop // 2\n",
        "\n",
        "    img1_fft = np.fft.fft2(img1, axes=(0, 1))\n",
        "    img2_fft = np.fft.fft2(img2, axes=(0, 1))\n",
        "    img1_abs, img1_pha = np.abs(img1_fft), np.angle(img1_fft)\n",
        "    img2_abs, img2_pha = np.abs(img2_fft), np.angle(img2_fft)\n",
        "\n",
        "    img1_abs = np.fft.fftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.fftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img1_abs_ = np.copy(img1_abs)\n",
        "    img2_abs_ = np.copy(img2_abs)\n",
        "    img1_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img2_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img1_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "    img2_abs[h_start:h_start + h_crop, w_start:w_start + w_crop] = \\\n",
        "        lam * img1_abs_[h_start:h_start + h_crop, w_start:w_start + w_crop] + (1 - lam) * img2_abs_[\n",
        "                                                                                          h_start:h_start + h_crop,\n",
        "                                                                                          w_start:w_start + w_crop]\n",
        "\n",
        "    img1_abs = np.fft.ifftshift(img1_abs, axes=(0, 1))\n",
        "    img2_abs = np.fft.ifftshift(img2_abs, axes=(0, 1))\n",
        "\n",
        "    img21 = img1_abs * (np.e ** (1j * img1_pha))\n",
        "    img12 = img2_abs * (np.e ** (1j * img2_pha))\n",
        "    img21 = np.real(np.fft.ifft2(img21, axes=(0, 1)))\n",
        "    img12 = np.real(np.fft.ifft2(img12, axes=(0, 1)))\n",
        "    img21 = np.uint8(np.clip(img21, 0, 255))\n",
        "    img12 = np.uint8(np.clip(img12, 0, 255))\n",
        "\n",
        "    return img21, img12"
      ],
      "metadata": {
        "id": "Kyu2yCvnsAnj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1.1 Data Read(Fourier) & Load "
      ],
      "metadata": {
        "id": "dBgSpAIZIh-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, lbl_transformer=None):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.lbl_transformer = lbl_transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.names[index]\n",
        "        mask_name = self.labels[index]\n",
        "        img = cv2.imread(img_name).copy()\n",
        "        mask = cv2.imread(mask_name).copy()\n",
        "        if self.transformer is not None:\n",
        "            img = self.transformer(img)\n",
        "            mask = self.lbl_transformer(mask)\n",
        "        return img, mask\n",
        "\n",
        "class FourierDGDataset(Dataset):\n",
        "    def __init__(self, names, labels, transformer=None, lbl_transformer=None, from_domain=None, alpha=1.0):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "        self.lbl_transformer = lbl_transformer\n",
        "        self.post_transform = get_post_transform()\n",
        "        self.from_domain = from_domain\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.flat_names = []\n",
        "        self.flat_labels = []\n",
        "        self.flat_domains = []\n",
        "        for i in range(len(names)):\n",
        "            self.flat_domains += [i] * len(names[i])\n",
        "            self.flat_names += names[i]\n",
        "            self.flat_labels += labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.names[index]\n",
        "        mask_name = self.labels[index]\n",
        "        img = cv2.imread(img_name).copy()\n",
        "        mask = cv2.imread(mask_name).copy()\n",
        "        img_o = self.transformer(img)\n",
        "        mask_o = self.lbl_transformer(mask)\n",
        "\n",
        "        img_s, mask_s, _ = self.sample_image()  ## random pick image\n",
        "        img_s2o, img_o2s = colorful_spectrum_mix(img_o, img_s, alpha=self.alpha)  ## mix their amplitude\n",
        "        img_o, img_s = self.post_transform(img_o), self.post_transform(img_s)\n",
        "        img_s2o, img_o2s = self.post_transform(img_s2o), self.post_transform(img_o2s)\n",
        "        img = [img_o, img_s, img_s2o, img_o2s]  ## [original, img2, img1 x img2, img2 x img1] \n",
        "        mask = [mask_o, mask_s, mask_o, mask_s]\n",
        "        # domain = [domain, domain_s, domain, domain_s]\n",
        "        return img, mask\n",
        "\n",
        "    def sample_image(self, domain=None):\n",
        "        if self.from_domain == 'all':\n",
        "            domain_idx = randpy.randint(0, len(self.names)-1)\n",
        "        elif self.from_domain == 'inter':\n",
        "            domains = list(range(len(self.names)))\n",
        "            # domains.remove(domain)\n",
        "            domain_idx = randpy.sample(domains, 1)[0]\n",
        "        elif self.from_domain == 'intra':\n",
        "            domain_idx = domain\n",
        "        else:\n",
        "            raise ValueError(\"Not implemented\")\n",
        "        img_idx = randpy.randint(0, len(self.names[domain_idx])-1)\n",
        "        imgn_ame_sampled = self.names[img_idx]\n",
        "        img_sampled = cv2.imread(imgn_ame_sampled)\n",
        "        label_ame_sampled = self.labels[img_idx]\n",
        "        label_sampled = cv2.imread(label_ame_sampled)\n",
        "        label_sampled = self.lbl_transformer(label_sampled)\n",
        "        return self.transformer(img_sampled), label_sampled, domain_idx\n",
        "\n",
        "\n",
        "def get_dataset(path, train=False, image_size=128, crop=False, jitter=0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_img_transform(train, image_size, crop, jitter)\n",
        "    lbl_transform = get_label_transform(train, image_size=image_size, crop=crop, jitter=jitter)\n",
        "    return DGDataset(names, labels, img_transform, lbl_transform)\n",
        "\n",
        "def get_fourier_dataset(path, image_size=128, crop=False, jitter=0, from_domain='all', alpha=1.0):\n",
        "    names, labels = dataset_info(path)\n",
        "    img_transform = get_pre_transform(image_size, crop, jitter)\n",
        "    lbl_transform = get_label_transform(train=True, image_size=image_size, crop=crop, jitter=jitter)\n",
        "    return FourierDGDataset(names, labels, img_transform, lbl_transform, from_domain, alpha)\n",
        "\n",
        "\n",
        "\n",
        "trian_path = \"./train\"\n",
        "val_path = \"./val\"\n",
        "test_path = \"./test\"\n",
        "\n",
        "train_fourier_dataset = get_fourier_dataset(path=train_path,crop=True,jitter=0.0)\n",
        "val_dataset = get_dataset(path=val_path, train=False)\n",
        "test_dataset = get_dataset(path=test_path, train=False)\n",
        "\n",
        "train_fourier_loader = torch.utils.data.DataLoader(train_fourier_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "print(f'Shape of image: {train_fourier_loader.dataset[0][0][0].numpy().shape}')\n",
        "print(f'Number of training batches: {len(train_fourier_loader)}')\n",
        "print(f'Number of validation batches: {len(val_loader)}')\n",
        "print(f'Number of test batches: {len(test_loader)}')"
      ],
      "metadata": {
        "id": "lpwjv5xjlfPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35981c04-f4d7-428f-d4a2-bc1cfa96998e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image: (3, 128, 128)\n",
            "Number of training batches: 33\n",
            "Number of validation batches: 9\n",
            "Number of test batches: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Show image from loader"
      ],
      "metadata": {
        "id": "tsIWaqTTRdtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for it, (batch, label) in enumerate(train_fourier_loader):\n",
        "    batch = torch.cat(batch, dim=0).cuda()\n",
        "    label = torch.cat(label, dim=0).cuda()\n",
        "\n",
        "    split_idx = int(batch.size(0) / 2)\n",
        "    ori_img, aug_img = torch.split(batch, split_idx)\n",
        "    labels_ori, labels_aug = torch.split(label, split_idx)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Ori1 \"+str(it))\n",
        "    image = ori_img.cpu().detach().numpy()[0]\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "    imgplot1 = plt.imshow(image)\n",
        "    plt.show(imgplot1)\n",
        "    plt.figure()\n",
        "    plt.title(\"Aug1 \"+str(it))\n",
        "    image = aug_img.cpu().detach().numpy()[0]\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "    imgplot2 = plt.imshow(image)\n",
        "    plt.show(imgplot2)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Ori2 \"+str(it))\n",
        "    image = ori_img.cpu().detach().numpy()[8]\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "    imgplot1 = plt.imshow(image)\n",
        "    plt.show(imgplot1)\n",
        "    plt.figure()\n",
        "    plt.title(\"Aug2 \"+str(it))\n",
        "    image = aug_img.cpu().detach().numpy()[8]\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "    imgplot2 = plt.imshow(image)\n",
        "    plt.show(imgplot2)"
      ],
      "metadata": {
        "id": "hj8Z1tjN66ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for it, (batch) in enumerate(val_loader):\n",
        "    data, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Prediction \"+str(it))\n",
        "    image = data.cpu().detach().numpy()[0]\n",
        "    imgplot1 = plt.imshow(image[0])\n",
        "    plt.show(imgplot1)\n",
        "    plt.figure()\n",
        "    plt.title(\"Label \"+str(it))\n",
        "    image = labels.cpu().detach().numpy()[0] * 2\n",
        "    image = image.astype(np.int32)\n",
        "    print(np.unique(image))\n",
        "    imgplot2 = plt.imshow(image[0])\n",
        "    plt.show(imgplot2)"
      ],
      "metadata": {
        "id": "8c2r0q3dRkZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Build Unet2D Model"
      ],
      "metadata": {
        "id": "3SL8XcjmAnQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Teacher Model"
      ],
      "metadata": {
        "id": "O38A5I8dBnrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Teacher Model\n",
        "# Student model would be ResNet50 model\n",
        "class MeanTeacherModel(nn.Module):\n",
        "    # Core\n",
        "    def __init__(self, student_model, ema_decay):\n",
        "        super().__init__()\n",
        "        self.student_model = student_model\n",
        "        self.teacher_model = deepcopy(student_model)\n",
        "        self.ema_decay = ema_decay\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.student_model(x)\n",
        "\n",
        "    def update_teacher_model(self, current_epoch, momentum=0.9995):\n",
        "        # The momentum increases from 0 to ema_decay\n",
        "        # Useful for improving quickly at the beginning\n",
        "        momentum = min(1 - 1 / (current_epoch + 1), self.ema_decay)\n",
        "        with torch.no_grad():\n",
        "            for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):\n",
        "                teacher_params.data.mul_(momentum).add_((1 - momentum) * student_params.data)\n",
        "\n",
        "    # Adjust the weight of the consistency loss to rely on teacher's prediction\n",
        "    # The weight factor decreases from 1 to 0 during the first 5 epochs\n",
        "    def sigmoid_rampup(self, current_epoch, rampup_len=2):\n",
        "        current_epoch = np.clip(current_epoch, 0.0, rampup_len)\n",
        "        phase = 1.0 - current_epoch / rampup_len\n",
        "        return np.exp(-5.0 * phase * phase).astype(np.float32)\n",
        "\n",
        "    # The weight decreases from 2\n",
        "    def get_consistency_weight(self, epoch):\n",
        "        return 0.5*self.sigmoid_rampup(epoch)"
      ],
      "metadata": {
        "id": "zkXkYCFGBdJj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medical Image Metrics"
      ],
      "metadata": {
        "id": "wPIglb1nI4d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement ASD\n",
        "def compute_asd(outputs, labels, idx):\n",
        "  outputs = F.softmax(outputs,dim=1)\n",
        "  outputs_cpy1, outputs_cpy2 = outputs.clone(), outputs.clone()\n",
        "  labels_cpy1, labels_cpy2 = labels.clone(), labels.clone()\n",
        "  ## [-1, 0.xx..., 1]\n",
        "  OC, OD, BG = torch.unique(labels_cpy2)[0].item(), torch.unique(labels_cpy2)[1].item(), torch.unique(labels_cpy2)[2].item()\n",
        "  mean, std, max, min = outputs.mean(), outputs.std(), torch.max(outputs), torch.min(outputs)\n",
        "\n",
        "  ## OC asd\n",
        "  labels_cpy1[labels_cpy1 == 0.0] = 2.0\n",
        "  labels_cpy1[labels_cpy1 != 2.0] = 0.0\n",
        "  outputs_cpy1[outputs_cpy1 > (mean-std)] = 0.0\n",
        "  labels_np = labels.cpu().detach().numpy().astype(np.bool_)\n",
        "  outputs_np = outputs_cpy1.cpu().detach().numpy().astype(np.bool_)\n",
        "  surface_dist = surfdist.compute_surface_distances(labels_np.reshape((1,-1,labels_np.shape[3])),\n",
        "                                                    outputs_np.reshape((1,-1,outputs_np.shape[3])),spacing_mm=(1.0,1.0,1.0))\n",
        "  oc_avg_asd = surfdist.compute_average_surface_distance(surface_dist)[1]\n",
        "\n",
        "  ## OD asd\n",
        "  labels_cpy2[labels_cpy2 == 1.0] = 0.0\n",
        "  outputs_cpy2[outputs_cpy2 < (mean-std)] = 0.0\n",
        "  outputs_cpy2[outputs_cpy2 > (mean+std)] = 0.0\n",
        "  labels_np = labels.cpu().detach().numpy().astype(np.bool_)\n",
        "  outputs_np = outputs_cpy2.cpu().detach().numpy().astype(np.bool_)\n",
        "  surface_dist = surfdist.compute_surface_distances(labels_np.reshape((1,-1,labels_np.shape[3])),\n",
        "                                                    outputs_np.reshape((1,-1,outputs_np.shape[3])),spacing_mm=(1.0,1.0,1.0))\n",
        "  od_avg_asd = surfdist.compute_average_surface_distance(surface_dist)[1]\n",
        "\n",
        "  if(idx == 16):\n",
        "      labels_img = outputs_cpy1.cpu().detach().numpy()\n",
        "      plt.figure(), plt.title(\"OC pred \")\n",
        "      imgplot1 = plt.imshow(labels_img[0][0])\n",
        "      plt.show(imgplot1)\n",
        "\n",
        "      labels_img = outputs_cpy2.cpu().detach().numpy()\n",
        "      plt.figure(), plt.title(\"OD pred \")\n",
        "      imgplot1 = plt.imshow(labels_img[0][0])\n",
        "      plt.show(imgplot1)\n",
        "\n",
        "  return oc_avg_asd, od_avg_asd"
      ],
      "metadata": {
        "id": "TmR0BSOXI26V"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Training"
      ],
      "metadata": {
        "id": "j3_18EFUCceP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load model\n",
        "Unet2D_model = smp.Unet(\n",
        "    encoder_name=\"resnet50\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=3,\n",
        ")\n",
        "\n",
        "\n",
        "def train_mean_teacher(model, train_loader, val_loader, optimizer, scheduler,\n",
        "                       consistency_criterion, supervised_criterion, device, epochs):\n",
        "    # Clear GPU cache\n",
        "    if torch.cuda.is_available():\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    loss_total_list, loss_sup_list, loss_con_list = [], [], []\n",
        "    dice_train_list = []\n",
        "    dice_val_list, oc_val_list, od_val_list = [], [], []\n",
        "\n",
        "    model.student_model.train()\n",
        "    model.teacher_model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        loss_total_train, dice_total_train, loss_sup_train, loss_con_train = 0,0,0,0\n",
        "        for it, (batch, label) in enumerate(train_loader):\n",
        "            batch = torch.cat(batch, dim=0).to(device)\n",
        "            label = torch.cat(label, dim=0).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get predictions\n",
        "            scores = model.student_model(batch)\n",
        "            with torch.no_grad():\n",
        "                scores_teacher = model.teacher_model(batch)\n",
        "\n",
        "            # Split data and predictions into original and augmented parts\n",
        "            assert batch.size(0) % 2 == 0\n",
        "            split_idx = int(batch.size(0) / 2)\n",
        "            scores_ori, scores_aug = torch.split(scores, split_idx)\n",
        "            scores_ori_tea, scores_aug_tea = torch.split(scores_teacher, split_idx)\n",
        "            scores_ori_tea, scores_aug_tea = scores_ori_tea.detach(), scores_aug_tea.detach()\n",
        "            labels_ori, labels_aug = torch.split(label, split_idx)\n",
        "            assert scores_ori.size(0) == scores_aug.size(0)\n",
        "\n",
        "            # Compute supervised losses for original and augmented data\n",
        "            # Use KL Divergence for consistency loss\n",
        "            labels_ori *= 2\n",
        "            labels_aug *= 2\n",
        "            labels_img = labels_ori.cpu().detach().numpy()\n",
        "            plt.figure(), plt.title(\"Temp Label \")\n",
        "            imgplot1 = plt.imshow(labels_img[0][0])\n",
        "            plt.show(imgplot1)\n",
        "\n",
        "            print(torch.unique(labels_ori.long()))\n",
        "            print(torch.unique(labels_aug.long()))\n",
        "            loss_cls = supervised_criterion(scores_ori, labels_ori[:,:1,:,:].long())\n",
        "            loss_aug = supervised_criterion(scores_aug, labels_aug[:,:1,:,:].long())\n",
        "\n",
        "            # KL divergence\n",
        "            p_ori, p_aug = F.softmax(scores_ori / 10.0, dim=1), F.softmax(scores_aug / 10.0, dim=1)\n",
        "            p_ori_tea, p_aug_tea = F.softmax(scores_ori_tea / 10.0, dim=1), F.softmax(scores_aug_tea / 10.0, dim=1)\n",
        "            loss_ori_tea = consistency_criterion(p_aug.log(), p_ori_tea, reduction='batchmean')\n",
        "            loss_aug_tea = consistency_criterion(p_ori.log(), p_aug_tea, reduction='batchmean')\n",
        "\n",
        "            const_weight = model.get_consistency_weight(epoch)\n",
        "\n",
        "            # Compute total loss\n",
        "            train_dice_coeff = (-0.5) * (loss_cls-1) + (-0.5) * (loss_aug-1)\n",
        "            total_loss = 0.5 * loss_cls + 0.5 * loss_aug + const_weight * loss_ori_tea + const_weight * loss_aug_tea\n",
        "\n",
        "            # Backward pass and update weights\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update teacher model parameters\n",
        "            model.update_teacher_model(current_epoch=epoch)\n",
        "            loss_total_train += total_loss.item()\n",
        "            loss_sup_train += 0.5*loss_cls.item() + 0.5*loss_aug.item()\n",
        "            loss_con_train += const_weight*loss_aug_tea.item() + const_weight*loss_ori_tea.item()\n",
        "            dice_total_train += train_dice_coeff.item()\n",
        "        loss_total_list.append(loss_total_train/len(train_loader))\n",
        "        loss_sup_list.append(loss_sup_train/len(train_loader))\n",
        "        loss_con_list.append(loss_con_train/len(train_loader))\n",
        "        dice_train_list.append(dice_total_train/len(train_loader))\n",
        "\n",
        "\n",
        "        ## Validation\n",
        "        model.student_model.eval()\n",
        "        dice_total_val, oc_asd_val, od_asd_val = 0,0,0\n",
        "        for it, (batch) in enumerate(val_loader):\n",
        "            data, labels = batch[0].to(device), batch[1].to(device)\n",
        "            scores = model.student_model(data)\n",
        "\n",
        "            loss = supervised_criterion(scores, labels)\n",
        "            oc_asd, od_asd = compute_asd(scores, labels, it)\n",
        "            asd_ori = 0.5*oc_asd + 0.5*od_asd\n",
        "            # Compute dice\n",
        "            val_dice_coeff = (-1) * (loss-1)\n",
        "            dice_total_val += val_dice_coeff.item()\n",
        "            oc_asd_val += oc_asd\n",
        "            od_asd_val += od_asd\n",
        "        dice_val_list.append(dice_total_val/len(val_loader))\n",
        "        oc_val_list.append(oc_asd_val/len(val_loader))\n",
        "        od_val_list.append(od_asd_val/len(val_loader))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Epoch summary\n",
        "        print(f\"Epoch [{epoch+1:02}/{epochs}],   \"\n",
        "              f\"Total Train Loss: {loss_total_list[-1]:.4f},   \"\n",
        "              f\"Supervised Train Loss: {loss_sup_list[-1]:.4f},   \"\n",
        "              f\"Const Train Loss: {loss_con_list[-1]:.4f},   \"\n",
        "              f\"Mean Train Dice: {dice_train_list[-1]:.4f},   \"\n",
        "              f\"Mean Val Dice: {dice_val_list[-1]:.4f},   \"\n",
        "              f\"OC Val ASD: {oc_val_list[-1]:.4f}   \"\n",
        "              f\"OD Val ASD: {od_val_list[-1]:.4f}   \")\n",
        "\n",
        "        # Clear GPU cache\n",
        "        if torch.cuda.is_available():\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return model, loss_total_list, loss_sup_list, loss_con_list, dice_train_list, dice_val_list, (oc_val_list,od_val_list)\n",
        "\n",
        "# Load Unet2D as Student model and Mean Teacher model\n",
        "base_model = Unet2D_model.to(device)\n",
        "mean_teacher_model = MeanTeacherModel(base_model, ema_decay=0.99).to(device)\n",
        "\n",
        "# Optimizer, loss functions and scheduler\n",
        "optimizer = Adam(mean_teacher_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "consistency_criterion = F.kl_div\n",
        "supervised_criterion = DiceLoss(mode='multiclass', classes=3)\n",
        "\n",
        "epochs = 25\n",
        "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=epochs)\n",
        "\n",
        "# Run training\n",
        "mean_teacher_model, loss_total_list, loss_sup_list, loss_con_list, dice_train_list, dice_val_list, asd_val_list = train_mean_teacher(\n",
        "    mean_teacher_model, train_fourier_loader, val_loader, optimizer, scheduler,\n",
        "    consistency_criterion, supervised_criterion, device, epochs=epochs)\n",
        "\n",
        "MODEL_PATH = \"./content\"\n",
        "torch.save(mean_teacher_model.state_dict(), MODEL_PATH)"
      ],
      "metadata": {
        "id": "0f1zLnoeCWYv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "outputId": "b6727415-48c4-48c9-f3bd-a17421ab64ad"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGzCAYAAACVYeimAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA880lEQVR4nO3de3gU5d038O/M7DHZzeYA2WRJQgIGOQoIEiI+HmoqeKZiebD4FK1XsQpW5KkCtkCxYqx9L6uoFe37PLZWFLWv2EorFgOC1ggYBMUDBBMgEDYHQnaTTfY0c79/RFcXgpw22dnN93Nde13uPfdOfjfCfjP33DMjCSEEiIiIdEiOdwFEREQnwpAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHSLIUVERLrFkCLqI379619DkiQ0NzfHbJ+33HILCgsLY7Y/omMxpCjpSJJ0Sq933nkn3qWe1KWXXoqRI0fGuwyiuDHEuwCiWPvLX/4S9f7555/H+vXrj2sfNmxYb5ZFRGeAIUVJ5+abb456/8EHH2D9+vXHtROR/nG6j/okTdPw2GOPYcSIEbBYLHA6nbj99ttx9OjRqH6FhYW45ppr8M4772D8+PGwWq0YNWpUZKrwtddew6hRo2CxWDBu3Dh89NFHUZ+/5ZZbYLPZUFNTg8mTJyM1NRUulwsPPPAAYvUAgo8//hi33HILBg0aBIvFgpycHPzkJz/BkSNHuu3f3NyM6dOnIy0tDVlZWbj77rvh9/uP6/fCCy9g3LhxsFqtyMzMxIwZM1BXVxeTmolOFUOK+qTbb78d9957LyZNmoTHH38ct956K1atWoXJkycjFApF9d27dy9+9KMf4dprr0V5eTmOHj2Ka6+9FqtWrcI999yDm2++GcuWLcOXX36J6dOnQ9O0qM+rqoopU6bA6XTikUcewbhx47B06VIsXbo0JmNZv349ampqcOutt+KJJ57AjBkzsHr1alx11VXdBuH06dPh9/tRXl6Oq666CitWrMDs2bOj+ixfvhw//vGPUVxcjEcffRTz5s1DRUUFLr74YrS2tsakbqJTIoiS3Jw5c8S3/6q/++67AoBYtWpVVL9169Yd1z5w4EABQLz//vuRtrfeeksAEFarVezfvz/S/swzzwgAYuPGjZG2WbNmCQDirrvuirRpmiauvvpqYTKZRFNT03fWfskll4gRI0Z8Z5+Ojo7j2l566SUBQGzevDnStnTpUgFAXHfddVF977zzTgFA7Ny5UwghxL59+4SiKGL58uVR/T755BNhMBii2mfNmiUGDhz4nfURnQ0eSVGf8+qrr8LhcOD73/8+mpubI69x48bBZrNh48aNUf2HDx+O0tLSyPuSkhIAwPe+9z0UFBQc115TU3Pcz5w7d27kvyVJwty5cxEMBvH222+f9XisVmvkv/1+P5qbmzFx4kQAwPbt24/rP2fOnKj3d911FwDgn//8J4CuKUxN0zB9+vSoP5+cnBwUFxcf9+dD1JO4cIL6nOrqang8HmRnZ3e7vbGxMer9t4MIABwOBwAgPz+/2/Zjz2vJsoxBgwZFtQ0ZMgQAsG/fvtMrvhstLS1YtmwZVq9efVztHo/nuP7FxcVR7wcPHgxZliO1VFdXQwhxXL+vGY3Gs66Z6FQxpKjP0TQN2dnZWLVqVbfb+/fvH/VeUZRu+52oXcRoQcSpmj59Ot5//33ce++9GDNmDGw2GzRNw5QpU447P9YdSZKi3muaBkmS8Oabb3Y7RpvNFrPaiU6GIUV9zuDBg/H2229j0qRJUVNlPUXTNNTU1ESOngBgz549AHDWd2s4evQoKioqsGzZMixZsiTSXl1dfcLPVFdXo6ioKPJ+79690DQtUsvgwYMhhEBRUVFUzUTxwHNS1OdMnz4dqqriN7/5zXHbwuFwj6xee/LJJyP/LYTAk08+CaPRiMsvv/ys9vv1kc6xR2+PPfbYCT/z1FNPRb1/4oknAABXXnklAOCGG26AoihYtmzZcfsVQpxwaTtRT+CRFPU5l1xyCW6//XaUl5djx44duOKKK2A0GlFdXY1XX30Vjz/+OG688caY/TyLxYJ169Zh1qxZKCkpwZtvvol//OMfuP/++4+bWuxOU1MTHnzwwePai4qKMHPmTFx88cV45JFHEAqFMGDAAPzrX/9CbW3tCfdXW1uL6667DlOmTEFlZSVeeOEF/OhHP8Lo0aMBdB1JPfjgg1i0aBH27duHqVOnwm63o7a2FmvWrMHs2bPxi1/84sz/QIhOA0OK+qSVK1di3LhxeOaZZ3D//ffDYDCgsLAQN998MyZNmhTTn6UoCtatW4c77rgD9957L+x2O5YuXRo1PfddGhsbsXjx4uPaL7/8csycORMvvvgi7rrrLjz11FMQQuCKK67Am2++CZfL1e3+Xn75ZSxZsgQLFy6EwWDA3Llz8bvf/S6qz8KFCzFkyBD8/ve/x7JlywB0LRS54oorcN11153mnwDRmZNEb5/lJepDbrnlFvz1r39Fe3t7vEshSkg8J0VERLrFkCIiIt1iSBERkW7xnBQREelW3I6knnrqKRQWFsJisaCkpARbt26NVylERKRTcQmpl19+GfPnz8fSpUuxfft2jB49GpMnTz7uvmNERNS3xWW6r6SkBBdccEHkKnxN05Cfn4+77roLCxcuPOnnNU1DfX097Hb7cfcdIyIi/RNCoK2tDS6XC7J84uOlXr+YNxgMoqqqCosWLYq0ybKMsrIyVFZWdvuZQCCAQCAQeX/o0CEMHz68x2slIqKeVVdXh7y8vBNu7/WQam5uhqqqcDqdUe1OpxNffPFFt58pLy+PXPX+bfu3FyLNxgWKRESJxtuuYeD5+2C327+zX0LcFmnRokWYP39+5L3X60V+fj7SbDLS7AwpIqJEdbJTNr0eUv369YOiKGhoaIhqb2hoQE5OTrefMZvNMJvNvVEeERHpSK8fhphMJowbNw4VFRWRNk3TUFFREfWIbiIiorhM982fPx+zZs3C+PHjMWHCBDz22GPw+Xy49dZb41EOERHpVFxC6j//8z/R1NSEJUuWwO12Y8yYMVi3bt1xiymIiKhvS8jbInm9XjgcDhzdM4gLJ4iIEpC3TUPGkBp4PB6kpaWdsB+/4YmISLcYUkREpFsMKSIi0i2GFBER6RZDioiIdIshRUREusWQIiIi3WJIERGRbjGkiIhItxhSRESkWwwpIiLSLYYUERHpFkOKiIh0iyFFRES6xZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHSLIUVERLrFkCIiIt1iSBERkW4xpIiISLcYUkREpFsMKSIi0i2GFBER6RZDioiIdIshRUREusWQIiIi3WJIERGRbjGkiIhItxhSRESkWwwpIiLSLYYUERHpFkOKiIh0iyFFRES6xZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHSLIUVERLrFkCIiIt2KeUiVl5fjggsugN1uR3Z2NqZOnYrdu3dH9fH7/ZgzZw6ysrJgs9kwbdo0NDQ0xLoUIiJKcDEPqU2bNmHOnDn44IMPsH79eoRCIVxxxRXw+XyRPvfccw/eeOMNvPrqq9i0aRPq6+txww03xLoUIiJKcJIQQvTkD2hqakJ2djY2bdqEiy++GB6PB/3798eLL76IG2+8EQDwxRdfYNiwYaisrMTEiRNPuk+v1wuHw4GjewYhzc4ZSyKiRONt05AxpAYejwdpaWkn7Nfj3/AejwcAkJmZCQCoqqpCKBRCWVlZpM/QoUNRUFCAysrKbvcRCATg9XqjXkRElPx6NKQ0TcO8efMwadIkjBw5EgDgdrthMpmQnp4e1dfpdMLtdne7n/LycjgcjsgrPz+/J8smIiKd6NGQmjNnDnbt2oXVq1ef1X4WLVoEj8cTedXV1cWoQiIi0jNDT+147ty5WLt2LTZv3oy8vLxIe05ODoLBIFpbW6OOphoaGpCTk9PtvsxmM8xmc0+VSkREOhXzIykhBObOnYs1a9Zgw4YNKCoqito+btw4GI1GVFRURNp2796NAwcOoLS0NNblEBFRAov5kdScOXPw4osv4m9/+xvsdnvkPJPD4YDVaoXD4cBtt92G+fPnIzMzE2lpabjrrrtQWlp6Siv7iIio74h5SD399NMAgEsvvTSq/bnnnsMtt9wCAPj9738PWZYxbdo0BAIBTJ48GX/4wx9iXQoRESW4Hr9OqifwOikiosSmm+ukiIiIzhRDioiIdIshRUREusWQIiIi3WJIERGRbjGkiIhItxhSRESkWwwpIiLSLYYUERHpFkOKiIh0iyFFRES6xZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHSLIUVERLrFkCIiIt1iSBERkW4xpIiISLcYUkREpFsMKSIi0i2GFBER6RZDioiIdIshRUREusWQIiIi3WJIERGRbjGkiIhItxhSRESkWwwpIiLSLYYUERHpliHeBRD1NSGh4rDaCbdq7rWfaZFU5CsaMpSUXvuZRLHAkCLqZUc1P55tKUXF4SFQtd6ZzMhObcfsAZtwXWpHr/w8olhhSBH1Mp8m8GFLARqq+0FSpV75mc2Zduzulws1pRqKxFl+ShwMKaIYOxhuR03YBr9m7HZ7XegcHOlIBUTvBBQAiLCMz9pdWG85CBlat32ylXacYxSwyZZeq4voZBhSRDEUEir+6RuC5/aVot3f/TmncFiB32OGLHqvLqlTwea952BbfQEkqfsfPCr7MBYOeBPnmXqvLqKTYUgRxZAGDV/6s+E+mAm5XTlhv96ecJNCEtBkRidOvFjjc1lDS04KcIIjLaJ4YEgRnaHD4XbsDGahVf1mxVxQKPjUkwv00rmmWPJ1mvAv70i4w3WRNlnSMNjYhJEmCWap++lLop7EkCI6Q9sC2fg/NZPR0GqPtAkBhDpMkIKJF1Khoxa8/Ok4/D/jmEibLAtcVliNxc63kWtgSFHvY0gRnSF3OB31zekQjdFTaIm6dk72y4DfjPC32oQi8GVmP3Rkx60s6uMYUkSn4XC4HVsCOTgUysCmliFQA0rChtIpERIOt9nxfGsJso3eSLNFCmGcZT9GmYxc0k49iiFFdBo+CznwaM33cagxHVpQgdRx4sURyUDSgDa3HX/xlEBSvlkVaDSF8Z/nbsc5WVWwSVyyTj2HIUV0Aqo4fpXbEdWBRo8NaDYn9xHUt8idMtAZvS49YDWi3p+OkNCgCo1HU9RjGFJE3TiqdmCTPxufdQ6Ahm8WQXzsHYBghwmJtywitqSwhE+O5OJ3ponINnnxHyl7MMZkYFhRzDGkiLpRr0r4v4f+A5/WuqLvDBGWIAX4RSyFJTTsz8RLhyfAmBpEYIQR52V+geSe/KR46PF/bQ8//DAkScK8efMibX6/H3PmzEFWVhZsNhumTZuGhoaGni6F6KRCQkVAhNCqmdHckQrZY4TsMXzz8imQwn39OAqA6FoNKHsNCLWZ0Ri0o10LICBC3U6TEp2pHj2S2rZtG5555hmcd955Ue333HMP/vGPf+DVV1+Fw+HA3LlzccMNN+Df//53T5ZD9J08Wife6sjBtvZBqOvMQHOrDejFWxclrJCEd92DsVQoyDa14Ur7xxhn5r2VKDZ6LKTa29sxc+ZM/PGPf8SDDz4Yafd4PPif//kfvPjii/je974HAHjuuecwbNgwfPDBB5g4cWJPlUT0nVpUFa80XICq3YVAWIYUkvr8uadTIQdlNO3PwBsH02FKDyBjlA9jTPt5fopiosf+Fs2ZMwdXX301ysrKotqrqqoQCoWi2ocOHYqCggJUVlZ2u69AIACv1xv1IoqVgAjBo3WiSTPjiD8Vks8AuVPmtN6pEoAckCF3KAh2GNEYSsNRrRPtmh8hoca7OkpwPXIktXr1amzfvh3btm07bpvb7YbJZEJ6enpUu9PphNvt7nZ/5eXlWLZsWU+USn1cu+bHWl8uKlqHo8lvw8GmDJzgJuF0KgIK3jw4HLUdWciztGJ6+jaMMXM5BZ25mIdUXV0d7r77bqxfvx4WS2wu8lu0aBHmz58fee/1epGfnx+TfVPf1iFUrD0yGv/eVdy1ci8sMaTOguyX0VybiXf3Z8DcrxNDz6vHGHNzvMuiBBbzkKqqqkJjYyPOP//8SJuqqti8eTOefPJJvPXWWwgGg2htbY06mmpoaEBOTk63+zSbzTCbT/yIAaLT1a750aaFUa+a0OxPhRTg9F5MCHx1Lk9CKGDA4VAGDob3wSJJcMgWGCUeVdHpiXlIXX755fjkk0+i2m699VYMHToUCxYsQH5+PoxGIyoqKjBt2jQAwO7du3HgwAGUlpbGuhyi4wRECGt9ufh78xi0BFJQ09AP0BhQsab5jPjr/rHYklaIwbZm3JL5PkaYrPEuixJMzEPKbrdj5MiRUW2pqanIysqKtN92222YP38+MjMzkZaWhrvuugulpaVc2Ue9wi/C2OgZispPz+m6MFeTIPHSnpiT/TKO1GTgiJSBPQOyMclejRGm9niXRQkmLnec+P3vfw9ZljFt2jQEAgFMnjwZf/jDH+JRCvUhHq0TzaqKI5oZTX5b5BwU9RABSF89/DEYMGBfsB/2mBqQIgn0V8x8iCKdEkkIkXCnib1eLxwOB47uGYQ0O6/FoJMLCRVrfJl48fBEtAasONScDrXZHPkSpZ6lpahIz2lDZmoHRmXU485+mzDEmBrvsiiOvG0aMobUwOPxIC0t7YT9eO8+6hM0aKjyFWFndT4knwGSAFfx9SK5Q4G3Nh0eKR0thSmYmr4dQ4ycY6WTY0hRUjuqdqBOldGqWVDXkdF1Jwl+N8bHV78Y+INGfOLPh12uRqYchMvAqT86MYYUJS1VaNjQmYP/OXQRmnw2HPWmQApxei/e/EcteGb3RVhluQATnftwd793UGRkSFH3GFKU1Hb7c/HF/lxIrcau3+TjXRBB9ino9NnRKdmxRRLwZDKg6MQYUpR0mlUf9oYsaNVS8IXPCRGSef5Jp9r9ZrzbMQRHtINwKW0YZDRy6o+iMKQo6WwJZGHF/jIcbrPD127hQwr1SgDtTan4Q/hiWEwhfC9vD37RfzNyDQwp+gZDipJOXTALew/3B5q6bqXFKT79kn0Kgr5UBGRgl82Fjn7xroj0hr9iEhGRbjGkiIhItxhSRKQLGiSonJylY/CcFCWFw+F2bAnkwB1y4J2j50ILKPwNLME0tNmwqnUCCkxHMMpSh9EmcKUfMaQoOXwSzMCjNd/HoaZ0aAEFUgefW5RIJA1oc9vxF28JDEYVU4o/wyDnOzArDKm+jiFFCU0VXfc4alFtaGi1A01mHkElKLlTBjpNCBsF6vMcCCXeva+pBzCkKGEdDrdjU2c+agLZ2OHNQ6jTyDMaREmGIUUJqyacgpX7L8H+Q1ldN4718xiKKNkwpChh+TQzWjqskFt53iLZhDUFfgEERAgGKFAk/gLSVzGkiEhfNGDvkX54yDIZTrMXl9k/w8WWIIwSF8P0RQwpItIVSZXQUW/D203DIVlVtI8wo8T5HkOqj2JIEZHuSCEJUkiBpkloC1mggiv9+ipO9BIRkW4xpIiISLc43UcJRRUa2kUAHZqKJnUgVJW/ZyU7n2pCfVggoPhgk4xIkU3xLol6EUOKEsphtQOrPGOxrXUg3L40dHisnA5IYlJYwo5DA7AgfAOyzD7c2G8brrD6uIiiD2FIUUJpUE34R/1IHKzOBgQgq7zHRDKTwhKCh1OxqzEFIkVF7lgPyqxVABhSfQVDihJOUFUghSUk1IIvAShBCXIQgIhNsGomAc0kIJL8UFLSAGgSRFhCSDCc+hqGFFEvkEMS7LWA48sgZPXs01XIEjyFJniLJajmREprotPDkCLqBZIK2A6pML//OTR/4Oz3ZzQgTRmFtkIjYI5BgUQ6xZAi3QsJFYfVTjSpJnwSyENn0JgYU30CUPwSDJ0SlE7A6AtDhMKApp79rkOAoT0ES4sJIf+3pg9lIGwVUC0CvCU8JQOGFOles9qJlUcuxKaGc9DuN8PbnJoQK/okVYLtAJD5hR+G9iAU91GEw6HY7FxoMNa4kRvoB830zT9jzaygZZgZ3kEShCERkpzouzGkSPfahIQtRwpxeE9/SJoEOUG+eyUNSGnSYKyqhtbWhnAsdy4Ewu4GwN0QdcBktNth7T8cbYVyQhxsEp0MQ4oSh9Dvij7FL8HYJkH61kyeHAJMnjAQitHR06lQVZg9KqwNClSzhLBNIGxNkqk/TUKtLwsbO21IlzswyOhHtpIa76qohzGkiM6WAKwNErI/6oTB861FEZoGuakV4WDvhZTmD8D6aT0GuNOg2i1oGpOCtkEiVqve40oKSvhoXz7uO3IDslI7cMfAdzAt9SifNZXkGFJEMWBsEzDtru+agvsWrbcL0VSED9UDh+phyMqEadC5uj36PF2SKgHNZviazWhLT0FtTjaQejTeZVEPY0gRnSGlU4K5VYIcAFIbwxC9Oa13KkJhpDSFEThghGoGgg6BcGqSJBb1GQwpojNkaZbg3NYJY4MXUpsPqrc93iVF0To6kLLjAKw1NoSzbGi8IBVthUlyfor6DIYU0RkydAqY6o4gvO9AvEvplgiHIysAjbk5MAwvindJRKeNIUW6FBAh7AwCn/jzcSCYhcY2GyQdzFQpnRIsRyQYOgTsB1WITn+8SzolIhiErT4MzWSAapHgz+LUHyUGhhTpkkcL4oUjl2Ld3mEIBw0QHQZdXB9lbpXg3NoJU90RiE4/tJbWeJd0SjSPFykf7kPq51aEcjPgLk1lSFFCYEiRLgWFwMGOdISbrJDCUvxPo3z1fS4HAGNjm26n+E5EhMNQm5qAJsAoSVACvL6IEgNDiugkFL8Ea4MEY5tAamMYUntHvEsi6jMYUkQnYWzrulDXtLseIhTS3So+omTGkCLqjkDXQg0ByEHA0Oo/7kLdhCUEJFV0PThS+uqhiXGfTyXqHkOKqBtKQEJKvQRLiwZLawjyEW/v3z2ih4i2dmRUB2FqMyJol+AbAIRtXERB+sSQIuqGoUNC1qcBmHfWAqEw1I7kOQ+lHvXA9MEXMJtM0M7JwyG7nSFFusWQIuqOBiidYahHWuJdSexpKjSfD/D5oLT3g6Ta410R0Qnx9sFERKRbDCkiItKtHgmpQ4cO4eabb0ZWVhasVitGjRqFDz/8MLJdCIElS5YgNzcXVqsVZWVlqK6u7olSKMF0aEE0qj40qCZ0hE3xLif5qRoUf9ftnuSABClRVocICS3hVBxWO9Cs+hAQOrsDPcVMzM9JHT16FJMmTcJll12GN998E/3790d1dTUyMjIifR555BGsWLECf/7zn1FUVITFixdj8uTJ+Oyzz2CxWGJdEiWIgAjh7z4n1h4ZjZZACmoa+gEa10b3qCNH0X+HDY59JnRkG+AdJCFkT4BFFAEZ/zowFF+290NeSit+lFmJCeZ4F0U9IeYh9dvf/hb5+fl47rnnIm1FRd/cfVkIgcceewy/+tWvcP311wMAnn/+eTidTrz++uuYMWPGcfsMBAIIBL554qnX64112aQDfhHGRs9Q/HtXMaSgDKgJ9Jt9glJbjkLe6oVFUWAePQS+XHtChJQckOHZ78D2Ogc+7teJUaMPYoLZHe+yqAfEfLrv73//O8aPH48f/vCHyM7OxtixY/HHP/4xsr22thZutxtlZWWRNofDgZKSElRWVna7z/LycjgcjsgrPz8/1mWTDmhCIKwpXeEU6v2AkrSuWyAZ2yQYfYAcDPduAfEgBEQ4DBEIQAppurjT/CkRXU/qlUISNFVBSCjxroh6SMxDqqamBk8//TSKi4vx1ltv4Y477sDPf/5z/PnPfwYAuN1dv+04nc6ozzmdzsi2Yy1atAgejyfyqquri3XZRJCDEtL2Aq7NQTi3+KAcao53SUR9Xsyn+zRNw/jx4/HQQw8BAMaOHYtdu3Zh5cqVmDVr1hnt02w2w2zmhDP1LDkowbEvAOPmTyBUFWFNjXdJRH1ezI+kcnNzMXz48Ki2YcOG4cCBrkcb5OTkAAAaGqLvg9bQ0BDZRhQvkiYgVBVgQBHpQsxDatKkSdi9e3dU2549ezBw4EAAXYsocnJyUFFREdnu9XqxZcsWlJaWxrocIiJKYDGf7rvnnntw4YUX4qGHHsL06dOxdetWPPvss3j22WcBAJIkYd68eXjwwQdRXFwcWYLucrkwderUWJdDREQJLOYhdcEFF2DNmjVYtGgRHnjgARQVFeGxxx7DzJkzI33uu+8++Hw+zJ49G62trbjooouwbt06XiNFRERReuQGs9dccw2uueaaE26XJAkPPPAAHnjggZ748URElCR47z4iItItPqqDiAAAss+P1Po0KAEZoRQg5BDQjIlydS8lK4YUEQEAxCE3+r0bhrCa0XZuBprGKAwpijuGFBEBADSfD1qtDwBgTR8NOZwa54qIeE6KiIh0jCFFRES6xZAiIiLdYkgREZFuMaSIiEi3uLqPiBKSkAFIAvJXL0pODCkiSjiaWUNqjg956a3IT23FCPNBcGIoOTGkiCjxmDVMHvg57uy3GWYJyJRNAEzxrop6AEOK6GuSgGaUYbRaIIIhiHAIEJxG0iVZINvUhkJDChSJR1DJjCFF9BXNBLQOMsJuHgFjexjm6gaEDx6Kd1lEfRpDiugrqkmgrUhC+0AF5hYDBrRnAgwporhiSBF9TQI0U9f0nmqSoBlkSHEuqVfJCmSLGZLBgKBZgehTgye9YkgREQDAkOtE5wgXAg4Fnf1kqFaej6P4Y0gREQBA6+dA8ygTOp0CQgKEwpCi+GNIEREAQMgyNCP4DCnSFa7dJCIi3WJIERGRbjGkiLohFCCUZoRhgAtK//6QzOZ4l0TUJ/GcFFE3wikCR0aa0JY/EJajGhxVhxHedyDeZRH1OQwpom5oZgHfAAHfACDFrcC+NzXeJRH1SQwp0g2jJCPPehTW/h0I+I3QfAbInQoQr8VmX13MKiQAUnJe2SqZzVAyMwCLGR3ZVmj8RiCd4V9J0g2zZMQ0RxWGjHKjPpSOV/aNw5GaDEhqcgaEHijZ/dE6cQA6shWEU4GwjcvPSV8YUqQbRknBeSYF55mO4GB4PyrTBuGIlBHvspKasFnRlq/Al6/FuxSibjGkiE5CMwKdrlSkBIohdfihNR2B1tER77L6HKEICHsY5tQgMuwdyDO1xLsk6gUMKaKTCNkFmsYYoQztj9TDGjLfB7T9DKneJkwCowYdws25lchS2nGu0QNFssW7LOphDCmik9BMAv7+XedqJE1GhoXXTMWFLHCuvQHXpzbDLBkBMKD6AoYUUR8jp6RAcjmh2VPQ4UqBaol3RUQnxpAi6mPkjHS0XOBE28CuG8qGuKKPdIwhRXQ6JABygi+JNxnhz5TR6UygFX1S10uWGKh9DUOK6DSEUoC2oZmwZoyGobUTYv8haD5fvMs6KdligVQwAOF+NrRnmhGyx7uiUyMUATkriH4ZbUi3dOK8lDrIvOVon8KQIjoNIYdA0xgFcigVafusyPK0J0RISXY7Wsf2R2uxDGEAwgny1F1hEBiedxi/yF+HLLkTTkWDUeItqvoShhTpkoKuqR2hCECTIAnE7/ZI36IZReShgIEWGTCbAFmJ7iQ0QMSxWEkCpOijDcliRtAuIZCpRW73lBAkoL+5HaNNnXDI1nhXQ3HAkCJdSpEVXJJZDf9wI1r9VtQ3pkNqMekiqL4WdADeMU6YCrMibZIQMNV7oe2rgwgEer0myWCAXFSA4IB0iG+dO+u0KwhkStDVHyDRKWBIkS7ZJDNuTvsc19g+RXUoA8twLQ4fzYaezpsHMgUaz5chaaZIm6RKyNplRFpDE9R4hJTZDN+5/dA01hD1GHghd13vlVBHUURgSJFOKZKMDCUFGQoQQisshnC8SzqOMAiEj/kXJKkCoRQZksUCyeyP7h8KA5raM8XICiSjAZLVglCqjJBNdIUSUYJjSBHFkgR0OCV4JhXC4P9mibccErDUtkD9cn/sg0pWoAweiM5BmQinKPC5ZAglgZaXE30HhhRRDAkZ6HQKBLLkrjdfMXRKcEqZMO87CBHjkJKMBnQOyoS7xATVKqAZBIRy8s8RJQKGFFGMCYOAeuy/LAkIpyiwptkg/LE9VyVZLQinKFCtAqolOab4hEFAGARg1pBq6P1ze6QfDCmiXiAUwDtQgXb5EEhabINEUyT4XDI0Q5IElCJgzunAWNdBZFva8H3HLlgkflX1Vfw/T9QLNKOAL0+gI/er+/vEmFC05Jnik4Hi7Cb8Om8tXIoCs2SEUTLGuyqKE4YU6Z4RAg5TJ4QtDBGWIAVlSOHEW0stlK6jBOqeMAoIowaYNPQ3tyNdBmwyb9He1zGkSPcyFQUzcrZiqL0BBzoz8cG+QogGfnklFQkwZXdgUkEtci0elNqqYePREwGxv1OjqqpYvHgxioqKYLVaMXjwYPzmN7+B+NZtYoQQWLJkCXJzc2G1WlFWVobq6upYl0JJwiFbMS31KJZmV+GOnA3IdPh4UWqSERIwINOD+Tnrsbj/dkyxdiBFNp38g5T0Yh5Sv/3tb/H000/jySefxOeff47f/va3eOSRR/DEE09E+jzyyCNYsWIFVq5ciS1btiA1NRWTJ0+G3+//jj1TX6ZIMsySERYpzMc1JCmDpMEiqTBLRigS73ROXWI+3ff+++/j+uuvx9VXXw0AKCwsxEsvvYStW7cC6DqKeuyxx/CrX/0K119/PQDg+eefh9PpxOuvv44ZM2bEuiQiIkpQMf915cILL0RFRQX27NkDANi5cyfee+89XHnllQCA2tpauN1ulJWVRT7jcDhQUlKCysrKbvcZCATg9XqjXkRElPxifiS1cOFCeL1eDB06FIqiQFVVLF++HDNnzgQAuN1uAIDT6Yz6nNPpjGw7Vnl5OZYtWxbrUikBWSQVefZWNGfboAUVwGeAFOQJqkSlWTRIqWEoBg15qa0w8n8lHSPmIfXKK69g1apVePHFFzFixAjs2LED8+bNg8vlwqxZs85on4sWLcL8+fMj771eL/Lz82NVMiWQPAMw17UBX/bLxk5fPv65ZwRX+iUqCbD068QPztmJIVY3ik1u9ONiCTpGzEPq3nvvxcKFCyPnlkaNGoX9+/ejvLwcs2bNQk5ODgCgoaEBubm5kc81NDRgzJgx3e7TbDbDbDbHulRKQA7ZikutGi61uvEv4xG8k3IOfGBIJap0Wwemp2/DGLMZXWcfGFIULebnpDo6OiDL0btVFAWa1nVX5qKiIuTk5KCioiKy3ev1YsuWLSgtLY11OUSkM5pFg+T0wzTAh+L0JqTI+nsMC+lHzI+krr32WixfvhwFBQUYMWIEPvroIzz66KP4yU9+AgCQJAnz5s3Dgw8+iOLiYhQVFWHx4sVwuVyYOnVqrMshIp0xZvgxfehHGJ9ai3xDC/IUXrRLJxbzkHriiSewePFi3HnnnWhsbITL5cLtt9+OJUuWRPrcd9998Pl8mD17NlpbW3HRRRdh3bp1sFg4bUOU7KyWEMrsn+JSqwZO79HJSOLbt4JIEF6vFw6HA0f3DEKanRf99VU7AgH8n8OT8UljLjr9RoSOWiD7+fdB72xFHjw+6uWvQor6Km+bhowhNfB4PEhLSzthP967jxLWOUaB+1zr0OJMwb+8I/HKZ+Mg/FxgQ5RM+GsnJSybbMF5JgsutWoYk7ofiiHGj2Wn2OqZp5RQkuORFBH1OM2qIaW/D2kpfpyXVY/+ig+ANd5lUQJgSBFRjzOkBTGz+ENcZf8YdjmEPAOnZenUMKQoKSgQkGUB8dUEtsRz8rrw9f8PoymM4dZDX120y4CiU8eQoqQw2NiEywqrUZ3ZHw1tNrS57ZA7eco1njSrBntOG5z2dhSnNaHQcAQMKDpdDClKCsNMMu53vo1ANvBC6wT82TsR6OQ1OPEkp4YwfdBHuMnxIcwS0F9hQNHpY0hRUjBLRuQZuu5ckGdqgcGoImwUgAZImgQk3NWACUoChCwAGTAYVeSZWjDYaIt3VZTAGFKUdEZZ6jCl+DPU5znwZUs/tNan8SLfXqKZNaS7vBic2Yy8lFaMMh8E7ypBZ4MhRUlntAkY5HwHfiHwhO0ivHJkPMCQ6h1mDVcWfIY7s96HRZJg56M36CwxpCjpmCUjzIoRqtDgNHqhWMJQAzIkVYIU5tRfzEmAMAgIRcBgCSPb5EWukgJF4i8GdPYYUpS0FElGScpeNAxLw2F/Gj5pdMFz0MEn+caYZtKQkefB6Ox6ZJvbMNH6JRRJiXdZlCQYUpTUJpgFRvf/AB1CxQOG7+Ef7vMgBfkFGlMGgYtcNfiV8x2kSArMEh+9QbHDkKKkZpSUrpcIob+pDYotDFUCpJAMKcSpv9MiAcIoIIxa1D34lNQQsk1tyJAtMPIIimKMIUV9ggEKyuy7gJFAU9COf9cXwVPn6AoqOiVCFkh1teHCAbVIM/gj7Q5DJ8psn0Lm3WOpBzCkqE9QJBkTzRrGm3fCowWxQJ2Cd+rTGFKnQwaKs5qw0LkermPuvWeAwoUS1CMYUtRnKJIMBTJSJBXZ5jYojiBUswEIyJADMqf+TkAza4BZg2xWkWNtg12WeN6Jeg1Divocs2TEDxxVyB3disPBdKzbPwxtB9IgqTyqOpaQgRSnD1MKP8cAcyvOt+7jtU/UqxhS1OcYJQUTLcAF5hocVjtQ3dYfOw6mAXxm4vEkgbx0D27v9y4GG6xfTenxKIp6D0OK+ixFkmGRJOSltGJXv05o6jcr07SwBNFh6FO3U9LMGqQUFbLxm+ecyJJAXmorUiTBc04UFwwp6tPssgk3Z72P0al1CIlvQupLfzbe2DsSoUOpcayudxkzAphS/BmGp9RH2mRJYIT5IDI5xUdxwpCiPs0sGTHBDEwwu6PaqwK1eM8+CI1Sap9ZUJGaEsANGVW41HrsEyNl8CaxFC8MKaJu2OUQBjuOoHVACkJBA1SvMWmm/oRRAGkhmCyhqPbBmc1IlzvBBxOSnjCkiLrhUhT8PHc96vplocpXiL/uHgP1cEq8y4oNWxiXn7sb38/YFWlSIJBjaEWR8dijKKL4YkgRdcMmW76aBvQiS9mFf1hGoD3eRcWIbFIxyVGN6TbPsVsAWONREtEJMaSITiJT6cCI/m7s+o4+vnYLRKsp7newEDKA9CBS7AHIcvdHRa40L1yGo71bGNEZYkgRncQgA7BwwJtoyul+pZ8qZLzcXIINu4ZCCsX3n5Qwahgx8DBmD9iEVDnQbR+77EexIQQgSaYvKakxpIhOwiZbcJ4JONHVvqoIYaftMDYazkXc77EqAwWpR/E9awtssuUEnYzgBbmUKBhSRGdJkWScaz6M4YX1aO6I73VVVmMIY2wH+MgMShoMKaIY+A9LMwYXvga/iG84GCUNOYoKs9R3LkKm5MaQIoqBDCUFGTx4IYq55Lg6kYiIkhJDioiIdIshRUREusWQIiIi3WJIERGRbjGkiIhItxhSRESkWwwpIiLSLYYUERHpFkOKiIh0iyFFRES6xZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItKt0w6pzZs349prr4XL5YIkSXj99dejtgshsGTJEuTm5sJqtaKsrAzV1dVRfVpaWjBz5kykpaUhPT0dt912G9rb289qIERElHxOO6R8Ph9Gjx6Np556qtvtjzzyCFasWIGVK1diy5YtSE1NxeTJk+H3+yN9Zs6ciU8//RTr16/H2rVrsXnzZsyePfvMR0FERElJEkKIM/6wJGHNmjWYOnUqgK6jKJfLhf/+7//GL37xCwCAx+OB0+nEn/70J8yYMQOff/45hg8fjm3btmH8+PEAgHXr1uGqq67CwYMH4XK5TvpzvV4vHA4Hju4ZhDQ7ZyyJiBKNt01DxpAaeDwepKWlnbBfTL/ha2tr4Xa7UVZWFmlzOBwoKSlBZWUlAKCyshLp6emRgAKAsrIyyLKMLVu2dLvfQCAAr9cb9SIiouQX05Byu90AAKfTGdXudDoj29xuN7Kzs6O2GwwGZGZmRvocq7y8HA6HI/LKz8+PZdlERKRTCTFXtmjRIng8nsirrq4u3iUREVEviGlI5eTkAAAaGhqi2hsaGiLbcnJy0NjYGLU9HA6jpaUl0udYZrMZaWlpUS8iIkp+MQ2poqIi5OTkoKKiItLm9XqxZcsWlJaWAgBKS0vR2tqKqqqqSJ8NGzZA0zSUlJTEshwiIkpwhtP9QHt7O/bu3Rt5X1tbix07diAzMxMFBQWYN28eHnzwQRQXF6OoqAiLFy+Gy+WKrAAcNmwYpkyZgp/+9KdYuXIlQqEQ5s6dixkzZpzSyj4iIuo7TjukPvzwQ1x22WWR9/PnzwcAzJo1C3/6059w3333wefzYfbs2WhtbcVFF12EdevWwWKxRD6zatUqzJ07F5dffjlkWca0adOwYsWKGAyHiIiSyVldJxUvvE6KiCixxeU6KSIiolhiSBERkW4xpIiISLcYUkREpFsMKSIi0i2GFBER6RZDioiIdIshRUREusWQIiIi3WJIERGRbjGkiIhItxhSRESkWwwpIiLSLYYUERHpFkOKiIh0iyFFRES6xZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHSLIUVERLrFkCIiIt1iSBERkW4xpIiISLcYUkREpFsMKSIi0i2GFBER6RZDioiIdIshRUREusWQIiIi3WJIERGRbjGkiIhItxhSRESkWwwpIiLSLYYUERHpFkOKiIh0iyFFRES6xZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHTrtENq8+bNuPbaa+FyuSBJEl5//fXItlAohAULFmDUqFFITU2Fy+XCj3/8Y9TX10fto6WlBTNnzkRaWhrS09Nx2223ob29/awHQ0REyeW0Q8rn82H06NF46qmnjtvW0dGB7du3Y/Hixdi+fTtee+017N69G9ddd11Uv5kzZ+LTTz/F+vXrsXbtWmzevBmzZ88+81EQEVFSkoQQ4ow/LElYs2YNpk6desI+27Ztw4QJE7B//34UFBTg888/x/Dhw7Ft2zaMHz8eALBu3TpcddVVOHjwIFwu10l/rtfrhcPhwNE9g5Bm54wlEVGi8bZpyBhSA4/Hg7S0tBP26/FveI/HA0mSkJ6eDgCorKxEenp6JKAAoKysDLIsY8uWLd3uIxAIwOv1Rr2IiCj59WhI+f1+LFiwADfddFMkKd1uN7Kzs6P6GQwGZGZmwu12d7uf8vJyOByOyCs/P78nyyYiIp3osZAKhUKYPn06hBB4+umnz2pfixYtgsfjibzq6upiVCUREemZoSd2+nVA7d+/Hxs2bIiab8zJyUFjY2NU/3A4jJaWFuTk5HS7P7PZDLPZ3BOlEhGRjsX8SOrrgKqursbbb7+NrKysqO2lpaVobW1FVVVVpG3Dhg3QNA0lJSWxLoeIiBLYaR9Jtbe3Y+/evZH3tbW12LFjBzIzM5Gbm4sbb7wR27dvx9q1a6GqauQ8U2ZmJkwmE4YNG4YpU6bgpz/9KVauXIlQKIS5c+dixowZp7Syj4iI+o7TXoL+zjvv4LLLLjuufdasWfj1r3+NoqKibj+3ceNGXHrppQC6LuadO3cu3njjDciyjGnTpmHFihWw2WynVAOXoBMRJbZTXYJ+VtdJxQtDiogosenmOikiIqIzxZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHSLIUVERLrFkCIiIt1iSBERkW4xpIiISLcYUkREpFs98mTenvb1jdu97VqcKyEiojPx9ff3yR7EkZAh1dbWBgAYeP6++BZCRERnpa2tDQ6H44TbE/J5Upqmob6+HkIIFBQUoK6u7jufR5LIvF4v8vPzk3qMAMeZbPrCOPvCGIGeG6cQAm1tbXC5XJDlE595SsgjKVmWkZeXB6/XCwBIS0tL6r8kQN8YI8BxJpu+MM6+MEagZ8b5XUdQX+PCCSIi0i2GFBER6VZCh5TZbMbSpUthNpvjXUqP6QtjBDjOZNMXxtkXxgjEf5wJuXCCiIj6hoQ+kiIiouTGkCIiIt1iSBERkW4xpIiISLcYUkREpFsJG1JPPfUUCgsLYbFYUFJSgq1bt8a7pLNSXl6OCy64AHa7HdnZ2Zg6dSp2794d1cfv92POnDnIysqCzWbDtGnT0NDQEKeKz97DDz8MSZIwb968SFuyjPHQoUO4+eabkZWVBavVilGjRuHDDz+MbBdCYMmSJcjNzYXVakVZWRmqq6vjWPHpU1UVixcvRlFREaxWKwYPHozf/OY3UTcMTcRxbt68Gddeey1cLhckScLrr78etf1UxtTS0oKZM2ciLS0N6enpuO2229De3t6Lo/hu3zXGUCiEBQsWYNSoUUhNTYXL5cKPf/xj1NfXR+2j18YoEtDq1auFyWQS//u//ys+/fRT8dOf/lSkp6eLhoaGeJd2xiZPniyee+45sWvXLrFjxw5x1VVXiYKCAtHe3h7p87Of/Uzk5+eLiooK8eGHH4qJEyeKCy+8MI5Vn7mtW7eKwsJCcd5554m777470p4MY2xpaREDBw4Ut9xyi9iyZYuoqakRb731lti7d2+kz8MPPywcDod4/fXXxc6dO8V1110nioqKRGdnZxwrPz3Lly8XWVlZYu3ataK2tla8+uqrwmaziccffzzSJxHH+c9//lP88pe/FK+99poAINasWRO1/VTGNGXKFDF69GjxwQcfiHfffVecc8454qabburlkZzYd42xtbVVlJWViZdffll88cUXorKyUkyYMEGMGzcuah+9NcaEDKkJEyaIOXPmRN6rqipcLpcoLy+PY1Wx1djYKACITZs2CSG6/uIYjUbx6quvRvp8/vnnAoCorKyMV5lnpK2tTRQXF4v169eLSy65JBJSyTLGBQsWiIsuuuiE2zVNEzk5OeJ3v/tdpK21tVWYzWbx0ksv9UaJMXH11VeLn/zkJ1FtN9xwg5g5c6YQIjnGeewX+KmM6bPPPhMAxLZt2yJ93nzzTSFJkjh06FCv1X6qugviY23dulUAEPv37xdC9O4YE266LxgMoqqqCmVlZZE2WZZRVlaGysrKOFYWWx6PBwCQmZkJAKiqqkIoFIoa99ChQ1FQUJBw454zZw6uvvrqqLEAyTPGv//97xg/fjx++MMfIjs7G2PHjsUf//jHyPba2lq43e6ocTocDpSUlCTUOC+88EJUVFRgz549AICdO3fivffew5VXXgkgecb5bacypsrKSqSnp2P8+PGRPmVlZZBlGVu2bOn1mmPB4/FAkiSkp6cD6N0xJtxd0Jubm6GqKpxOZ1S70+nEF198EaeqYkvTNMybNw+TJk3CyJEjAQButxsmkynyl+RrTqcTbrc7DlWemdWrV2P79u3Ytm3bcduSZYw1NTV4+umnMX/+fNx///3Ytm0bfv7zn8NkMmHWrFmRsXT3dziRxrlw4UJ4vV4MHToUiqJAVVUsX74cM2fOBICkGee3ncqY3G43srOzo7YbDAZkZmYm5Lj9fj8WLFiAm266KXIX9N4cY8KFVF8wZ84c7Nq1C++99168S4mpuro63H333Vi/fj0sFku8y+kxmqZh/PjxeOihhwAAY8eOxa5du7By5UrMmjUrztXFziuvvIJVq1bhxRdfxIgRI7Bjxw7MmzcPLpcrqcbZl4VCIUyfPh1CCDz99NNxqSHhpvv69esHRVGOW/HV0NCAnJycOFUVO3PnzsXatWuxceNG5OXlRdpzcnIQDAbR2toa1T+Rxl1VVYXGxkacf/75MBgMMBgM2LRpE1asWAGDwQCn05nwYwSA3NxcDB8+PKpt2LBhOHDgAABExpLof4fvvfdeLFy4EDNmzMCoUaPwX//1X7jnnntQXl4OIHnG+W2nMqacnBw0NjZGbQ+Hw2hpaUmocX8dUPv378f69eujniXVm2NMuJAymUwYN24cKioqIm2apqGiogKlpaVxrOzsCCEwd+5crFmzBhs2bEBRUVHU9nHjxsFoNEaNe/fu3Thw4EDCjPvyyy/HJ598gh07dkRe48ePx8yZMyP/nehjBIBJkyYdd/nAnj17MHDgQABAUVERcnJyosbp9XqxZcuWhBpnR0fHcU9UVRQFmqYBSJ5xftupjKm0tBStra2oqqqK9NmwYQM0TUNJSUmv13wmvg6o6upqvP3228jKyora3qtjjOkyjF6yevVqYTabxZ/+9Cfx2WefidmzZ4v09HThdrvjXdoZu+OOO4TD4RDvvPOOOHz4cOTV0dER6fOzn/1MFBQUiA0bNogPP/xQlJaWitLS0jhWffa+vbpPiOQY49atW4XBYBDLly8X1dXVYtWqVSIlJUW88MILkT4PP/ywSE9PF3/729/Exx9/LK6//nrdL80+1qxZs8SAAQMiS9Bfe+010a9fP3HfffdF+iTiONva2sRHH30kPvroIwFAPProo+Kjjz6KrGw7lTFNmTJFjB07VmzZskW89957ori4WFdL0L9rjMFgUFx33XUiLy9P7NixI+r7KBAIRPbRW2NMyJASQognnnhCFBQUCJPJJCZMmCA++OCDeJd0VgB0+3ruuecifTo7O8Wdd94pMjIyREpKivjBD34gDh8+HL+iY+DYkEqWMb7xxhti5MiRwmw2i6FDh4pnn302arumaWLx4sXC6XQKs9ksLr/8crF79+44VXtmvF6vuPvuu0VBQYGwWCxi0KBB4pe//GXUF1kijnPjxo3d/lucNWuWEOLUxnTkyBFx0003CZvNJtLS0sStt94q2tra4jCa7n3XGGtra0/4fbRx48bIPnprjHyeFBER6VbCnZMiIqK+gyFFRES6xZAiIiLdYkgREZFuMaSIiEi3GFJERKRbDCkiItIthhQREekWQ4qIiHSLIUVERLrFkCIiIt36/8X6SUE9DTyqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2,  0,  2], device='cuda:0')\n",
            "tensor([-2,  0,  2], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-594fd623b9f7>\u001b[0m in \u001b[0;36m<cell line: 140>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# Run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m mean_teacher_model, loss_total_list, loss_sup_list, loss_con_list, dice_train_list, dice_val_list, asd_val_list = train_mean_teacher(\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mmean_teacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fourier_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     consistency_criterion, supervised_criterion, device, epochs=epochs)\n",
            "\u001b[0;32m<ipython-input-69-594fd623b9f7>\u001b[0m in \u001b[0;36mtrain_mean_teacher\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, consistency_criterion, supervised_criterion, device, epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mp_ori\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_ori\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_aug\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mp_ori_tea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_aug_tea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_ori_tea\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_aug_tea\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mloss_ori_tea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsistency_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_ori_tea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batchmean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mloss_aug_tea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsistency_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_ori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_aug_tea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batchmean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss curve\n",
        "fig = plt.figure()\n",
        "plt.plot(loss_total_list, label='Total Loss')\n",
        "plt.plot(loss_sup_list, label='Supervised Loss')\n",
        "plt.plot(loss_con_list, label='Consistency Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot accuracy curve\n",
        "fig = plt.figure()\n",
        "plt.plot(dice_train_list, label='Training DICE')\n",
        "plt.plot(dice_val_list, label='Validation DICE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('DICE')\n",
        "plt.title('DICE Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot AUC curve\n",
        "fig = plt.figure()\n",
        "plt.plot(asd_val_list[0], label='OC Validation ASD')\n",
        "plt.plot(asd_val_list[1], label='OD Validation ASD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('ASD')\n",
        "plt.title('ASD Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AR-8B9aJH3gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_teacher_model.load_state_dict(torch.load(MODEL_PATH))\n",
        "mean_teacher_model.student_model.eval()\n",
        "for it, (batch) in enumerate(test_loader):\n",
        "    if it < 10:\n",
        "        data, labels = batch[0].to(device), batch[1].to(device)\n",
        "        outputs = mean_teacher_model.student_model(data)\n",
        "        outputs = F.softmax(outputs,dim=1)\n",
        "      \n",
        "        plt.figure()\n",
        "        plt.title(\"Prediction \"+str(it))\n",
        "        image = outputs.cpu().detach().numpy()[1]\n",
        "        # image = np.transpose(image, (1,2,0))\n",
        "        imgplot1 = plt.imshow(image[0])\n",
        "        plt.show(imgplot1)\n",
        "        plt.figure()\n",
        "        plt.title(\"Label \"+str(it))\n",
        "        image = labels.cpu().detach().numpy()[0]\n",
        "        image = np.transpose(image, (1,2,0))\n",
        "        imgplot2 = plt.imshow(image)\n",
        "        plt.show(imgplot2)\n",
        "    else:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "Sppl_mn5pF3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}